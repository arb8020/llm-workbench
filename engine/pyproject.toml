[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "engine"
version = "0.1.0"
description = "Multi-framework inference utilities for GPT-2 and other models"
authors = [{name = "Your Name", email = "your.email@example.com"}]
license = {text = "MIT"}
requires-python = ">=3.9"
dependencies = [
    "huggingface-hub>=0.34.4",
    "numpy<2.0.0",
    "jax[cuda12]",
    "pathlib",
    "requests>=2.28.0",
    "llama-stack",
    "torchinfo",
]

[project.optional-dependencies]
# Weight loading support
weights = [
    "safetensors>=0.3.0",
    "torch>=2.0.0",
    "accelerate>=0.20.0",
]

# JAX backend
jax = [
    "jax[cuda12]>=0.4.0",
    "jaxlib>=0.4.0",
]

# Development setup
dev = [
    "safetensors>=0.3.0",
    "pytest>=7.0.0",
]

# Inference server backends
backends = [
    "vllm>=0.5.0",
    "requests>=2.28.0",
]

# Minimal transformers testing (no vllm, no heavy deps)
transformers-test = [
    "nnsight>=0.4",
    "transformers @ git+https://github.com/huggingface/transformers.git",  # Bleeding-edge for gpt_oss and glm4_moe support
    "torch>=2.0.0",
    "accelerate>=0.20.0",
    "datasets>=4.0.0",  # For streaming dataset support - tested compatible with pyarrow >=21.0.0
    "pyarrow>=21.0.0",  # Required for datasets compatibility

]

# Outlier analysis (nnsight + triton, no vllm conflict)
outlier = [
    "nnsight>=0.4",
    "transformers @ git+https://github.com/huggingface/transformers.git",  # Bleeding-edge for gpt_oss and glm4_moe support
    "torch>=2.0.0",
    "accelerate>=0.20.0",
    "datasets>=4.0.0",  # For streaming dataset support - tested compatible with pyarrow >=21.0.0
    "pyarrow>=21.0.0",  # Required for datasets compatibility
    "triton>=3.4.0",  # Required for MXFP4 quantization support
]

# Interpretability support (nnsight integration)
interp = [
    "nnsight>=0.4",
    "vllm>=0.8.0",  # Allow resolver maximum flexibility
    "transformers @ git+https://github.com/huggingface/transformers.git",  # Bleeding-edge for gpt_oss and glm4_moe support
    "fastapi>=0.100.0",
    "uvicorn>=0.20.0",
    "datasets>=4.0.0",  # For streaming dataset support - tested compatible with pyarrow >=21.0.0
    "pyarrow>=21.0.0",  # Required for datasets compatibility
    "triton>=3.4.0",  # Required for MXFP4 quantization support
]

# All optional dependencies
all = [
    "safetensors>=0.3.0",
    "torch>=2.0.0",
    "accelerate>=0.20.0",
    "jax[cuda12]>=0.4.0",
    "jaxlib>=0.4.0",
    "vllm>=0.5.0",
    "requests>=2.28.0",
]

[project.urls]
Homepage = "https://github.com/yourusername/inference"
Repository = "https://github.com/yourusername/inference"

[tool.uv]
conflicts = [
    [
        { extra = "interp" },
        { extra = "outlier" },
        { extra = "all" },
        { extra = "backends" },
    ],
]

[tool.setuptools.packages.find]
where = ["."]
include = ["engine*"]

[tool.setuptools.package-data]
"*" = ["*.py"]
