#!/usr/bin/env python3
"""Push-button vLLM deployment script."""

import sys
import time
import json
from pathlib import Path

# Import broker and bifrost modules
# Note: Run with PYTHONPATH=/path/to/broker:/path/to/bifrost for clean imports

from broker.client import GPUClient
from bifrost.client import BifrostClient


def deploy_vllm(min_vram: int = 8, max_price: float = 0.40) -> dict:
    """Deploy vLLM server on GPU and return connection info."""
    
    print("üöÄ Starting vLLM deployment...")
    
    # 1. PROVISION GPU
    print(f"üì° Creating GPU instance (min {min_vram}GB VRAM, max ${max_price}/hr)...")
    gpu_client = GPUClient()
    
    # Build query for GPU with minimum VRAM and max price
    query = (gpu_client.vram_gb >= min_vram) & (gpu_client.price_per_hour <= max_price)
    
    gpu_instance = gpu_client.create(
        query=query,
        exposed_ports=[8000],  # Expose port 8000 for vLLM
        enable_http_proxy=True,  # Enable RunPod proxy
        name="vllm-server",
        cloud_type="secure"
    )
    
    print(f"‚úÖ GPU ready: {gpu_instance.id}")
    
    # Wait for SSH to be ready
    print("‚è≥ Waiting for SSH connection to be ready...")
    if not gpu_instance.wait_until_ssh_ready(timeout=300):  # 5 minutes
        print("‚ùå Failed to get SSH connection ready")
        sys.exit(1)
    
    ssh_connection = gpu_instance.ssh_connection_string()
    print(f"‚úÖ SSH ready: {ssh_connection}")
    
    # 2. DEPLOY CODE
    print("üì¶ Deploying codebase...")
    bifrost_client = BifrostClient(ssh_connection)
    
    # Deploy the codebase to remote workspace
    workspace_path = bifrost_client.push()
    bifrost_client.exec("echo 'Codebase deployed successfully'")
    print(f"‚úÖ Code deployed to: {workspace_path}")
    
    # 3. START SERVER IN TMUX
    print("üåü Starting vLLM server in tmux session...")
    
    # Create tmux session and start vLLM server with persistent logging
    tmux_cmd = "tmux new-session -d -s vllm 'python examples/deploy_inference_server/simple_vllm/start_server.py 2>&1 | tee ~/vllm_server.log'"
    bifrost_client.exec(tmux_cmd)
    
    print("‚úÖ vLLM server starting in tmux session 'vllm'")
    
    # 4. POLL UNTIL READY
    print("‚è≥ Waiting for server to be ready (this may take 2-3 minutes for model loading)...")
    
    max_wait_time = 600  # 10 minutes max
    start_time = time.time()
    server_ready = False
    
    while not server_ready and (time.time() - start_time) < max_wait_time:
        try:
            # Check if OpenAI-compatible server is responding
            models_check = bifrost_client.exec("curl -s --connect-timeout 5 http://localhost:8000/v1/models")
            if models_check and "gpt2" in models_check.lower():
                server_ready = True
                break
                
            # Fallback: try a simple completions request
            test_completion = bifrost_client.exec(
                'curl -s --connect-timeout 5 -X POST http://localhost:8000/v1/completions '
                '-H "Content-Type: application/json" '
                '-d \'{"model":"openai-community/gpt2","prompt":"test","max_tokens":1}\''
            )
            if test_completion and ("choices" in test_completion.lower() or "text" in test_completion.lower()):
                server_ready = True
                break
                
        except Exception as e:
            # Server not ready yet, continue waiting
            pass
        
        elapsed = int(time.time() - start_time)
        if elapsed % 30 == 0:  # Print update every 30 seconds
            print(f"   Still loading model... ({elapsed}s elapsed)")
            
        time.sleep(10)
    
    if not server_ready:
        print(f"‚ùå Server failed to start within timeout: {time.time() - start_time}s elapsed")
        print("   Check logs with:")
        print(f"   # Persistent log file: bifrost exec {ssh_connection} 'cat ~/vllm_server.log'")
        print(f"   # tmux session logs: bifrost exec {ssh_connection} 'tmux capture-pane -t vllm -p'")
        sys.exit(1)
    
    print("‚úÖ Server is ready and responding!")
    
    # 5. CONSTRUCT PROXY URL  
    proxy_url = gpu_instance.get_proxy_url(8000)
    
    if not proxy_url:
        print("‚ö†Ô∏è  No proxy URL available - instance may not be RunPod")
        proxy_url = f"http://{gpu_instance.public_ip}:8000"
    
    # 6. RETURN CONNECTION INFO
    connection_info = {
        "url": proxy_url,
        "instance_id": gpu_instance.id,
        "ssh": ssh_connection,
        "provider": gpu_instance.provider,
        "status": "ready"
    }
    
    print("\nüéâ vLLM deployment complete!")
    print(f"   Server URL: {proxy_url}")
    print(f"   Instance ID: {gpu_instance.id}")
    print(f"   SSH: {ssh_connection}")
    
    print("\nüß™ Test your server:")
    print(f"   curl -X POST {proxy_url}/v1/completions \\")
    print('     -H "Content-Type: application/json" \\')
    print('     -d \'{"model":"openai-community/gpt2","prompt":"Hello!","max_tokens":20}\'')
    
    print("\nüîß Management commands:")
    print(f"   # View tmux session: bifrost exec {ssh_connection} 'tmux attach -t vllm'")
    print(f"   # Check persistent logs: bifrost exec {ssh_connection} 'cat ~/vllm_server.log'")
    print(f"   # Check tmux session: bifrost exec {ssh_connection} 'tmux capture-pane -t vllm -p'")
    print(f"   # Stop server: bifrost exec {ssh_connection} 'tmux kill-session -t vllm'")
    print(f"   # Terminate GPU: broker terminate {gpu_instance.id}")
    
    return connection_info


def main():
    """Main deployment function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Deploy vLLM server on remote GPU")
    parser.add_argument("--min-vram", type=int, default=8, help="Minimum VRAM in GB")
    parser.add_argument("--max-price", type=float, default=0.40, help="Maximum price per hour")
    parser.add_argument("--json", action="store_true", help="Output connection info as JSON")
    
    args = parser.parse_args()
    
    try:
        connection_info = deploy_vllm(args.min_vram, args.max_price)
        
        if args.json:
            print(json.dumps(connection_info, indent=2))
        
        return connection_info
        
    except KeyboardInterrupt:
        print("\nüõë Deployment interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Deployment failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
