{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Comparison and Visualization\n",
    "\n",
    "This notebook demonstrates how to compare different optimizers and visualize their behavior on various loss surfaces.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup for inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Add paths for optimizer imports\n",
    "sys.path.append('../validation')\n",
    "\n",
    "# Import optimizers\n",
    "from adam.solution import adam_init, adam_update\n",
    "from adamw.solution import adamw_init, adamw_update\n",
    "from sgd.solution import sgd_init, sgd_update\n",
    "from muon.solution import muon_init, muon_update\n",
    "\n",
    "# Import visualization tools\n",
    "from contract import RosenbrockModel, SimpleQuadraticModel, OptimizerConfig\n",
    "from compare import compare_optimizers, print_comparison_table, plot_loss_curves, plot_convergence_summary\n",
    "from visualize import plot_2d_trajectories, plot_high_dimensional_trajectories, plot_parameter_evolution\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Optimizers\n",
    "\n",
    "We'll compare Adam, AdamW, SGD with momentum, and Muon across different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizers for comparison\n",
    "optimizers = {\n",
    "    'Adam': OptimizerConfig(\n",
    "        name='Adam',\n",
    "        init_fn=adam_init,\n",
    "        update_fn=adam_update,\n",
    "        kwargs={'lr': 0.01},\n",
    "        color='red'\n",
    "    ),\n",
    "    'AdamW': OptimizerConfig(\n",
    "        name='AdamW',\n",
    "        init_fn=adamw_init,\n",
    "        update_fn=adamw_update,\n",
    "        kwargs={'lr': 0.01, 'weight_decay': 0.001},\n",
    "        color='blue'\n",
    "    ),\n",
    "    'SGD+Momentum': OptimizerConfig(\n",
    "        name='SGD+Momentum',\n",
    "        init_fn=sgd_init,\n",
    "        update_fn=sgd_update,\n",
    "        kwargs={'lr': 0.002, 'momentum': 0.9},\n",
    "        color='green'\n",
    "    ),\n",
    "    'Muon': OptimizerConfig(\n",
    "        name='Muon',\n",
    "        init_fn=muon_init,\n",
    "        update_fn=muon_update,\n",
    "        kwargs={'lr': 0.005, 'momentum': 0.9, 'orthogonalize': True},\n",
    "        color='purple'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(optimizers)} optimizers: {list(optimizers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Rosenbrock Function (2D)\n",
    "\n",
    "The Rosenbrock function is a classic non-convex optimization challenge with a narrow valley leading to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 2D problem\n",
    "model_2d = RosenbrockModel(dim=2)\n",
    "initial_params_2d = jnp.array([-2.0, 2.0])\n",
    "batches = [None]  # No batches for deterministic functions\n",
    "\n",
    "print(f\"ðŸŽ¯ Rosenbrock function starting from {initial_params_2d}\")\n",
    "print(f\"ðŸŽ¯ Global minimum at {model_2d.optimal_params()}\")\n",
    "print(f\"ðŸŽ¯ Initial loss: {model_2d.loss(initial_params_2d):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization comparison\n",
    "results_2d = compare_optimizers(\n",
    "    model=model_2d,\n",
    "    optimizer_configs=optimizers,\n",
    "    initial_params=initial_params_2d,\n",
    "    batches=batches,\n",
    "    num_steps=1000,\n",
    "    verbose=False  # Keep notebook clean\n",
    ")\n",
    "\n",
    "print(\"âœ… Optimization complete!\")\n",
    "\n",
    "# Show comparison table\n",
    "print_comparison_table(results_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2D trajectories\n",
    "fig = plot_2d_trajectories(\n",
    "    results_2d, model_2d, \n",
    "    x_range=(-2.5, 2.5), y_range=(-1, 3),\n",
    "    figsize=(12, 10)\n",
    ")\n",
    "fig.suptitle(\"Rosenbrock Function - Optimizer Trajectories\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig = plot_loss_curves(results_2d, title=\"Rosenbrock Function - Loss Curves\", figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence analysis\n",
    "fig = plot_convergence_summary(results_2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: High-Dimensional Ill-Conditioned Problem\n",
    "\n",
    "Test optimizers on a challenging high-dimensional quadratic with poor conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenging high-dimensional problem\n",
    "model_hd = SimpleQuadraticModel(dim=20, condition_number=1000)\n",
    "initial_params_hd = jax.random.normal(jax.random.PRNGKey(42), (20,)) * 2.0\n",
    "\n",
    "print(f\"ðŸŽ¯ High-dimensional problem: {model_hd.dim}D\")\n",
    "print(f\"ðŸŽ¯ Condition number: {1000}\")\n",
    "print(f\"ðŸŽ¯ Initial loss: {model_hd.loss(initial_params_hd):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (fewer steps for high-dim)\n",
    "results_hd = compare_optimizers(\n",
    "    model=model_hd,\n",
    "    optimizer_configs=optimizers,\n",
    "    initial_params=initial_params_hd,\n",
    "    batches=batches,\n",
    "    num_steps=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"âœ… High-dimensional optimization complete!\")\n",
    "print_comparison_table(results_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA projection of high-dimensional trajectories\n",
    "fig = plot_high_dimensional_trajectories(\n",
    "    results_hd, method='pca', n_components=2, figsize=(12, 8)\n",
    ")\n",
    "fig.suptitle(\"High-Dimensional Trajectories (PCA Projection)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA projection\n",
    "fig = plot_high_dimensional_trajectories(\n",
    "    results_hd, method='pca', n_components=3, figsize=(12, 10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter evolution for first 6 parameters\n",
    "fig = plot_parameter_evolution(results_hd, max_params=6, figsize=(15, 10))\n",
    "fig.suptitle(\"Parameter Evolution (High-Dimensional Problem)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Effect of Problem Conditioning\n",
    "\n",
    "Compare how optimizers handle problems with different condition numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different condition numbers\n",
    "condition_numbers = [1, 10, 100, 1000]\n",
    "conditioning_results = {}\n",
    "\n",
    "# Use subset of optimizers for cleaner comparison\n",
    "optimizers_subset = {\n",
    "    'Adam': optimizers['Adam'],\n",
    "    'Muon': optimizers['Muon']\n",
    "}\n",
    "\n",
    "for cond_num in condition_numbers:\n",
    "    print(f\"Testing condition number: {cond_num}\")\n",
    "    \n",
    "    model_cond = SimpleQuadraticModel(dim=5, condition_number=cond_num)\n",
    "    initial_params_cond = jax.random.normal(jax.random.PRNGKey(123), (5,)) * 1.0\n",
    "    \n",
    "    results_cond = compare_optimizers(\n",
    "        model=model_cond,\n",
    "        optimizer_configs=optimizers_subset,\n",
    "        initial_params=initial_params_cond,\n",
    "        batches=batches,\n",
    "        num_steps=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    conditioning_results[cond_num] = results_cond\n",
    "\n",
    "print(\"âœ… Conditioning study complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conditioning analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, cond_num in enumerate(condition_numbers):\n",
    "    results = conditioning_results[cond_num]\n",
    "    \n",
    "    for name, trajectory in results.items():\n",
    "        losses = np.array(trajectory.losses)\n",
    "        steps = np.arange(len(losses))\n",
    "        axes[i].semilogy(steps, losses, label=name, linewidth=2)\n",
    "    \n",
    "    axes[i].set_title(f'Condition Number: {cond_num}')\n",
    "    axes[i].set_xlabel('Step')\n",
    "    axes[i].set_ylabel('Loss')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Effect of Problem Conditioning on Optimizer Performance', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "From these experiments, we can observe:\n",
    "\n",
    "1. **Adam/AdamW** adapt well to different scales and are generally robust\n",
    "2. **SGD+Momentum** needs careful learning rate tuning but can be very stable\n",
    "3. **Muon's orthogonalization** helps with ill-conditioned problems\n",
    "4. **High condition numbers** challenge all optimizers, but adaptive methods cope better\n",
    "5. **2D visualizations** reveal trajectory differences clearly\n",
    "6. **PCA projections** help understand high-dimensional behavior\n",
    "\n",
    "## Custom Experiments\n",
    "\n",
    "You can easily run your own experiments by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create your own model\n",
    "class CustomModel:\n",
    "    def __init__(self):\n",
    "        self.param_shape = (2,)\n",
    "    \n",
    "    def loss(self, params, batch=None):\n",
    "        # Your custom loss function\n",
    "        x, y = params[0], params[1]\n",
    "        return x**4 + y**4 - 2*x**2 - 2*y**2 + x*y  # Multi-modal function\n",
    "    \n",
    "    def grad(self, params, batch=None):\n",
    "        # Analytical or use jax.grad\n",
    "        return jax.grad(self.loss)(params)\n",
    "\n",
    "# Test your custom model\n",
    "custom_model = CustomModel()\n",
    "custom_initial = jnp.array([2.0, -1.0])\n",
    "\n",
    "custom_results = compare_optimizers(\n",
    "    model=custom_model,\n",
    "    optimizer_configs={'Adam': optimizers['Adam'], 'SGD': optimizers['SGD+Momentum']},\n",
    "    initial_params=custom_initial,\n",
    "    batches=[None],\n",
    "    num_steps=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plot_loss_curves(custom_results, title=\"Custom Multi-Modal Function\")\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Custom experiment complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}