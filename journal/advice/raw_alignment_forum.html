/* Raw content from https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher */

<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" type="text/css" href="https://use.typekit.net/jvr1gjm.css">
        <link rel="stylesheet" type="text/css" href="https://use.typekit.net/tqv5rhd.css">
        <style id="jss-insertion-start"></style>
        <style id="jss-insertion-end"></style>
        <script>
            _embedStyles = function(name, priority, css) {
                const styleNode = document.createElement("style");
                styleNode.append(document.createTextNode(css));
                styleNode.setAttribute("data-name", name);
                styleNode.setAttribute("data-priority", priority);

                const head = document.head;
                const startNode = document.getElementById('jss-insertion-start');
                const endNode = document.getElementById('jss-insertion-end');

                if (!startNode || !endNode) {
                    throw new Error('Insertion point markers not found');
                }

                styleNode.setAttribute('data-priority', priority.toString());
                styleNode.setAttribute('data-name', name);

                const styleNodes = Array.from(head.querySelectorAll('style[data-priority]'));
                let left = 0;
                let right = styleNodes.length - 1;

                while (left <= right) {
                    const mid = Math.floor((left + right) / 2);
                    const midNode = styleNodes[mid];
                    const midPriority = parseInt(midNode.getAttribute('data-priority') || '0', 10);
                    const midName = midNode.getAttribute('data-name') || '';

                    if (midPriority < priority || (midPriority === priority && midName < name)) {
                        left = mid + 1;
                    } else if (midPriority > priority || (midPriority === priority && midName > name)) {
                        right = mid - 1;
                    } else {
                        // Equal priority and name, insert after this node
                        midNode.insertAdjacentElement('afterend', styleNode);
                        return;
                    }
                }

                // If we didn't find an exact match, insert at the position determined by 'left'
                if (left === styleNodes.length) {
                    // Insert before the end marker
                    endNode.insertAdjacentElement('beforebegin', styleNode);
                } else if (left === 0) {
                    // Insert after the start marker
                    startNode.insertAdjacentElement('afterend', styleNode);
                } else {
                    // Insert before the node at the 'left' index
                    styleNodes[left].insertAdjacentElement('beforebegin', styleNode);
                }
            }
        </script>
        <script>
            window.publicInstanceSettings = {
                "forumType": "AlignmentForum",
                "title": "AI Alignment Forum",
                "siteNameWithArticle": "the AI Alignment Forum",
                "sentry": {
                    "url": "https://1ab1949fc8d04608b43132f37bb2a1b0@sentry.io/1301611",
                    "environment": "alignmentForum",
                    "release": "69f0f3c5d57b596e8249571383f8a280eff9bb23"
                },
                "debug": false,
                "aboutPostId": "Yp2vYb4zHXEeoTkJc",
                "faqPostId": "Yp2vYb4zHXEeoTkJc",
                "contactPostId": "ehcYkvyz7dh9L7Wt8",
                "expectedDatabaseId": "production",
                "tagline": "A community blog devoted to technical AI alignment research",
                "faviconUrl": "https://res.cloudinary.com/dq3pms5lt/image/upload/v1531267596/alignmentForum_favicon_o9bjnl.png",
                "forumSettings": {
                    "headerTitle": "AI ALIGNMENT FORUM",
                    "shortForumTitle": "AF",
                    "tabTitle": "AI Alignment Forum"
                },
                "analytics": {
                    "environment": "alignmentforum.org"
                },
                "testServer": false,
                "fmCrosspost": {
                    "siteName": "the EA Forum",
                    "baseUrl": "https://forum.effectivealtruism.org/"
                },
                "performanceMetricLogging": {
                    "enabled": true,
                    "batchSize": 100
                },
                "recombee": {
                    "databaseId": "lightcone-infrastructure-lesswrong-prod-2",
                    "publicApiToken": "sb95OJbQ7mKLQAm1abPog2m5vCPj7XqZlVYdHGyANcjzqaHT5fX6HEgB0vCfiLav"
                },
                "taggingName": "wikitag",
                "taggingUrlCustomBase": "w"
            }
        </script>
        <link rel="shortcut icon" href="https://res.cloudinary.com/dq3pms5lt/image/upload/v1531267596/alignmentForum_favicon_o9bjnl.png"/>
        <script>
            window.publicSettings = {
                "forum": {
                    "numberOfDays": 10,
                    "postInterval": 30,
                    "numberOfWeeks": 4,
                    "numberOfYears": 4,
                    "maxPostsPerDay": 5,
                    "numberOfMonths": 4
                },
                "type3": {
                    "cutoffDate": "2023-07-01",
                    "explicitlyAllowedPostIds": ["SvKSwT6xYfYahH4XN", "2weRdcvqANDq3zdPH", "Zm7WAJMTaFvuh2Wc7", "HcjL8ydHxPezj6wrt", "pgGiqLQg2KWsaz5RE", "jFzovY2CERF5bd2EW", "sm6npdgZArSn4afeZ", "CfX6pGepdjQYELSpK", "NyFuuKQ8uCEDtd2du", "LCjtqsQWapoSfDHqK", "MxyRNd6qJsYAcXKuw", "reG3g4wwzwJcKnFfh", "zfeWGvTrS6wKQeeoF", "oHsMeXehPy4jHcmwy", "ofL22R6KZsfrvdmwg", "655TmdcwAgryPGPWS", "hhrv8aAcmkzJxvP58", "iqQJiKcephtMgzJgN", "mnpkM57R6ZbjnwrYw", "6mRv7Cr57AJAtRFHv", "ak9wY2t9K3K4GxCXv", "Ay6GBGNcCgP55dRQ7", "aH4mjhgqNPyYvJT85", "JpoLCHytYiCm7fwNA", "efMgZujzfjP9B9H4R", "BjLxPLsev54LFCS3A", "HTGCGASf9xfB6edAh", "H6LnGwjKiGvDyR5yo", "qL8Z9TBCNWQyN6yLq", "F4xwRTrFQyazHufjD", "LY7Nca846X8kcT8Jk", "K9aLcuxAPyf5jGyFX", "2AuvBPw6Rb7yxkvKc", "muhtBvbh4etjkKXd9", "ALEYMFAuFSCz8v5YE", "CJxSgaqG6y7z6Rbij", "k5TpDCEHeK4qwnJt8", "4Y2J7NtuweW2B8JvB", "BpYDqQNZ2NZNCqPp6", "oMiogKLkK8L59WzDe", "TyQSMmoJpRG3HBv5S", "8KHR3tfa4SJjMSkXd", "g4pi2jfQHFF6mPdjw", "znEhB9hJtwXica5s3", "Sd2r7H8bCmd9ChGbX", "P2nYKqwmHdYKARTG8", "FW3DEYbKPZJh5A8Bj", "K3hFLRn7MvYacL466", "ouSpHCCPgsXkwxAGb", "w9SuQtRJLbDpeir6L", "yPQGYn9rSme9RRpiQ", "BD6WYC4GT6dnWaJRN", "c8khnHoRTSGjmHLLf", "TaPr4YSBbiakeKdwX", "pyNPXST7feDX45ygt", "ERPL3v2Y976W7XG3j", "XpXQ4KNzLa9ZHYw8p", "PBhrHw5X8sDmHDWkX", "8KhThQXzsAEZ59iko", "iYJo382hY28K7eCrP", "KrEwDMN4YXp5YWD45", "rNJ39yQmzTnseh8nL", "hMQPyLDbg3bA7P6aN", "3Jqz6JE8K6vyQ9hJ5", "SQAfPKZBAAKYMjx25", "Y345zuBetHqGnotwm", "pZerSnxv6FPqvgoYu", "3bPH2az479gzxDMbf", "QXShCBvPydkwafekn", "iLMkKDKmfbMkDuQBm", "iNCg6mjw584r9BWZK", "9oqF382ASmjaGBo7z", "DdNB42JgBzbbvmAum", "JP7eZYHB7aY6fA4TR", "snwX7hXgLFikqDBr6", "CsKrQdQJJCFPjfKjF", "vhxywjnBH6ioRnnt3", "A4MK9RQqSAJZjanQD", "PCpzG9NJeviXM5YSq", "KCcdhZK7omEMwBdju", "kdmCm5NQTpqhJmGm6", "2p8BWvcJvKkXGMsch", "FLnDFnXyWrKr6eiT6", "2gWs8SScqeDFidqyv", "2HafkDSNdtMzptzcN", "cTQRGJTQ2eGKm5G9g", "qaHHJ3kkCQS4nsoGJ", "gS8Jmcfoa9FAh92YK", "eRhFaibbTeGbjdaaf", "xij43oLTBRnEQv2bT", "BZMc9Xzqw5WcCMHrr", "2jZykdLg9fBGqKd46", "gBChm3THPGFcrq5eH", "9HSwh2mE3tX6xvZ2W", "tEHJXNhw6t87foqJL", "T5McDuWDeCvDZKeSj", "PeTL97v92LxRJBsrM", "Cq45AuedYnzekp3LX", "pfmZ5cYQCahABGZzi", "3wBj8BPquskZAbXu9", "xPJKZyPCvap4Fven8", "BPKvZuLRyiJBjfNbg", "um7w5RogAHhxGy8Ti", "CcyGR3pp3FCDuW6Pf", "BfaAADSQ88cuxLQoD", "ckuuDa8DmJ4pdFeD8", "pczHfyxmnFhtKthqR", "dymK5c7BkpgXH4acw", "B4AyJXYPpGbBmxQzd", "xNBRkPNHAGQ6EQaLS", "88TN6y9M5xxAHHNwW", "Lt8Rn4rkYwqiTXGPy", "QdXrkWoK2Pp6XhNuQ", "NjzBrtvDS4jXi5Krp", "ZWC3n9c6v4s35rrZ3", "Fy2b55mLtghd4fQpx", "eaczwARbFnrisFx8E", "KLjQedNYNEP4tW73W", "DSnamjnW7Ad8vEEKd", "7iDtkfyn322nPzTP4", "eaSJtg8Kvc56bFBdt", "AmaWMMWPzuQ62Ernf", "jkf2YjuH8Z2E7hKBA", "BroeiXGh9PrKZEkJ5", "9Tw5RqnEzqEtaoEkq", "EMJ3egz48BtZS8Pws", "MkKcnPdTZ3pQ9F5yC", "kjArXFinD3deRZNRu", "Q8zqoBWBBHD2RjDuS", "ePA4NDzZkunz98tLx", "4xKeNKFXFB458f5N8", "irbREZtZzPi7WEYex", "QxZs5Za4qXBegXCgu", "ZmQv4DFx6y4jFbhLy", "M7rwT264CSYY6EdR3", "z3cTkXbA7jgwGWPcv", "9thqSN8HDLM3LTxK5", "MtNnFg4uN32YPoKNa", "Ep2Z42hYqj68QZz6w", "ibk7q8msSYxZXmfCf", "EgDpZS4HHeh5vqJPe", "5dhWhjfxn4tPfFQdi", "Wh8HAK6LR5CAoPCCC", "Yy7mgec8tsbTAuTqb", "azoP7WeKYYfgCozoh", "Zh9AiXNjQaYXjmNaC", "bJiyYJeCyh4HcKHub", "aPrCzeFfbBmRsvzby", "vXCK3kptLLggEfojX", "M2LWXsJxKS626QNEA", "LQp9cZPzJncFKh5c8", "CZnBQtvDw33rmWpBD", "miHttwTgajY2sjY3L", "K2JBqDeETX2yEgyyZ", "r8aAqSBeeeMNRtiYK", "Gh2qQHrCg3teQen3c", "mja6jZ6k9gAwki9Nu", "qjSHfbjmSyMnGR9DS", "Sx26Aj3xuMzmnKE4A", "P3uavjFmZD5RopJKk", "pJJdcZgB6mPNWoSWr", "oGezscrQvPDgGvrbt", "AYbhqi65SWzHzy7Xx", "E4cKD9iTWHaE7f3AJ", "x9FNKTEt68Rz6wQ6P", "HAEPbGaMygJq8L59k", "znBJwbuT3f5eWgM4E", "yJfBzcDL9fBHJfZ6P", "YAkpzvjC768Jm2TYb", "LTtNXM9shNM9AC2mp", "9hR2RmpJmxT8dyPo4", "WQWhXzALcrzrJtqRh", "p7WXmG6Fbo3eaSwm3", "KheBaeW8Pi7LwewoF", "A2Qam9Bd9xpbb2wLQ", "asmZvCPHcB4SkSCMW", "euJm4RwkAptZnP89i", "r8stxYL29NF9w53am", "6yTShbTdtATxKonY5", "yDRX2fdkm3HqfTpav", "EhEZoTFzys9EDmEXn", "YSWa8rYeD3aDaofSP", "rwkkcgSpnAyE8oNo3", "HmfxSWnqnK265GEFM", "Ltey8BS83qSkd9M3u", "atcJqdhCxTZiJSxo2", "pC47ZTsPNAkjavkXs", "wJnm5cBiZGmKn595f", "GrtbTAPfkJa4D6jjH", "LgavAYtzFQZKg95WC", "reitXJgJXFzKpdKyd", "ZiQqsgGX6a42Sfpii", "neQ7eXuaXpiYw7SBy", "hQHuXuRGZxxWXaPgg", "9kcTNWopvXFncXgPy", "baTWMegR42PAsH9qJ", "Kbm6QnJv9dgWsPHQP", "gFMH3Cqw4XxwL69iy", "R6M4vmShiowDn56of", "6Fpvch8RR29qLEWNH", "N6WM6hs7RQMKDhYjB", "pdaGN6pQyQarFHXF4", "SA9hDewwsYgnuscae", "i9xyZBS3qzA8nFXNQ", "bx3gkHJehRCYZAF3r", "Jk9yMXpBLMWNTFLzh", "JvZhhzycHu2Yd57RN", "vzfz4AS6wbooaTeQk", "gHefoxiznGfsbiAu9", "sbcmACvB6DqYXYidL", "kipMvuaK3NALvFHc9", "xdwbX9pFEr7Pomaxv", "XvN2QQpKTuEzgkZHY", "uFNgRumrDTpBfQGrs", "ii4xtogen7AyYmN6B", "kpPnReyBC54KESiSn", "FRv7ryoqtvSuqBxuT", "u8GMcpEN9Z6aQiCvp", "B2CfMNfay2P8f2yyc", "JD7fwtRQ27yc8NoqS", "mRwJce3npmzbKfxws", "3rxMBRCYEmHCNDLhu", "FWvzwCDRgcjb9sigb", "KrJfoZzpSDpnrv9va", "LpM3EAakwYdS6aRKf", "Cf2xxC3Yx9g6w7yXN", "qHCDysDnvhteW7kRd", "mELQFMi9egPn5EAjK", "qDmnyEMtJkE9Wrpau", "4ZvJab25tDebB8FGE", "4QemtxDFaGXyGSrGD", "Psr9tnQFuEXiuqGcR", "qmXqHKpgRfg83Nif9", "ximou2kyQorm6MPjX", "eccTPEonRe4BAvNpD", "2cYebKxNp47PapHTL", "pv7Qpu8WSge8NRbpB", "PqMT9zGrNsGJNfiFR", "B9kP6x5rpmuCzpfWb", "zB4f7QqKhBHa5b37a", "qc7P2NwfxQMC3hdgm", "RcifQCKkRc9XTjxC2", "YABJKJ3v97k9sbxwg", "bNXdnRTpSXk9p4zmi", "fRsjBseRuvRhMPPE5", "MzKKi7niyEqkBPnyu", "NQgWL7tvAPgN2LTLn", "cujpciCqNbawBihhQ", "wEebEiPpEwjYvnyqq", "AqbWna2S85pFTsHH4", "Nwgdq6kHke5LY692J", "8xLtE3BwgegJ7WBbf", "SWxnP5LZeJzuT3ccd", "Tr7tAyt5zZpdTwTQK", "ax695frGJEzGxFBK4", "FkgsxrGf3QxhfLWHG", "vJ7ggyjuP4u2yHNcP", "X5RyaEDHNq5qutSHK", "xhD6SHAAE9ghKZ9HS", "AyNHoTWWAJ5eb99ji", "F5ktR95qqpmGXXmLq", "znfkdCoHMANwqc2WE", "jbE85wCkRr9z7tqmD", "4K5pJnKBGkqqTbyxx", "yeADMcScw8EW9yxpH", "9QxnfMYccz9QRgZ5z", "X2i9dQQK3gETCyqh2", "4XRjPocTprL4L8tmB", "D6trAzh6DApKPhbv4", "BcYfsi7vmhDvzQGiF", "i42Dfoh4HtsCAfXxL", "zp5AEENssb8ZDnoZR", "KwdcMts8P8hacqwrX", "RQpNHSiWaXTvDxt6R", "nSjavaKcBrtNktzGa", "hNqte2p48nqKux3wS", "7im8at9PmhbT4JHsW", "SwcyMEgLyd4C3Dern", "AHhCrJ2KpTjsCSwbt", "rz73eva3jv267Hy7B", "E4zGWYzh6ZiG85b2z", "hvGoYXi2kgnS3vxqb", "D4hHASaZuLCW92gMy", "v7c47vjta3mavY3QC", "G5TwJ9BGxcgh5DsmQ", "YRgMCXMbkKBZgMz4M", "ham9i5wf4JCexXnkN", "a4jRN9nbD79PAhWTB", "xJyY5QkQvNJpZLJRo", "ivpKSjM4D6FbqF4pZ", "p7x32SEt43ZMC9r7r", "f886riNJcArmpFahm", "xhE4TriBSPywGuhqi", "ThvvCE2HsLohJYd7b", "diruo47z32eprenTg", "JJFphYfMsdFMuprBy", "ZDZmopKquzHYPRNxq", "KkwtLtroaNToWs2H6", "vKErZy7TFhjxtyBuG", "3L46WGauGpr7nYubu", "CSZnj2YNMKGfsMbZA", "G2Lne2Fi7Qra5Lbuf", "x6hpkYyzMG6Bf8T3W", "aFaKhG86tTrKvtAnT", "PrCmeuBPC4XLDQz8C", "dYspinGtiba5oDCcv", "9cbEPEuCa9E7uHMXT", "N5Jm6Nj4HkNKySA5Z", "asmZvCPHcB4SkSCMW", "duxy4Hby5qMsv42i8", "Djs38EWYZG8o7JMWY", "A8iGaZ3uHNNGgJeaD", "XYYyzgyuRH5rFN64K", "2jfiMgKkh7qw9z8Do", "JPan54R525D68NoEt", "o4cgvYmNZnfS4xhxL", "CeZXDmp8Z363XaM6b", "DQKgYhEYP86PLW7tZ", "niQ3heWwF6SydhS7R", "gvK5QWRLk3H8iqcNy", "fnkbdwckdfHS2H22Q", "YicoiQurNBxSp7a65", "JBFHzfPkXHB2XfDGj", "tj8QP2EFdP8p54z6i", "9fB4gvoooNYa4t56S", "zTfSXQracE7TW8x4w", "YcdArE79SDxwWAuyF", "8xRSjC76HasLnMGSf", "CvKnhXTu9BPcdKE4W", "DtcbfwSrcewFubjxp", "NxF5G6CJiof6cemTw", "4ZwGqkMTyAvANYEDw", "EF5M6CmKRd6qZk27Z", "cCMihiwtZx7kdcKgt", "Qz6w4GYZpgeDp6ATB", "TPjbTXntR54XSZ3F2", "x3fNwSe5aWZb5yXEG", "bnY3L48TtDrKTzGRb", "ZFtesgbY9XwtqqyZ5", "S7csET9CgBtpi7sCh", "tTWL6rkfEuQN9ivxj", "L6Ktf952cwdMJnzWm", "P6fSj3t4oApQQTB7E", "4s2gbwMHSdh2SByyZ", "sTwW3QLptTQKuyRXx", "EYd63hYSzadcNnZTD", "tF8z9HBoBn783Cirz", "hyShz2ABiKX56j5tJ", "YN6daWakNnkXEeznB", "6DuJxY8X45Sco4bS2", "TMFNQoRZxM4CuRCY6", "q3JY4iRzjq56FyjGF", "diutNaWF669WgEt3v", "5okDRahtDewnWfFmz", "r3NHPD3dLFNk9QE2Y", "ALkH4o53ofm862vxc", "N9oKuQKuf7yvCCtfq", "WjsyEBHgSstgfXTvm", "2G8j8D5auZKKAjSfY", "rBkZvbGDQZhEymReM", "nNqXfnjiezYukiMJi", "36Dhz325MZNq3Cs6B", "f2GF3q6fgyx8TqZcn", "byewoxJiAfwE6zpep", "nEBbw2Bc2CnN2RMxy", "w4aeAFzSAguvqA5qu", "xFotXGEotcKouifky", "rzqACeBGycZtqCfaX", "DoPo4PDjgSySquHX8", "o3RLHYviTE4zMb9T9", "5gfqG3Xcopscta3st", "GNhMPAWcfBCASy8e6", "uXH4r6MmKPedk8rMA", "Gg9a4y8reWKtLe3Tn", "bBdfbWfWxHN9Chjcq", "sT6NxFxso6Z9xjS7o", "k9dsbn8LZ6tTesDS3", "exa5kmvopeRyfJgCy", "YTJp5WBcktBimdxBG", "X79Rc5cA5mSWBexnd", "SvKpaPbZ2tibeDpgh", "rQKstXH8ZMAdN5iqD", "vQKbgEKjGZcpbCqDs", "Z9cbwuevS9cqaR96h", "pHHaNkG8xDcaq5DJF", "sjRG35aq5fosJ6mdG", "pPWiLGsWCtN92vLwu", "D5BP9CxKHkcjA7gLv", "57sq9qA3wurjres4K", "t2LGSDwT7zSnAGybG", "7Pq9KwZhG6vejmYpo", "g3PwPgcdcWiP33pYn", "zcriHTKgKNehSSdyG", "kvLPC5YWgSujcHSkY", "HnC29723hm6kJT7KP", "CRiJuJxgArjBMJLvK", "dyJfGeWo5GX2u6NGi", "QLmSFeFexgTLsNeeA", "kmT47aLQmqzcw329Y", "givHhuPu6G43g8kWN", "83DimRqppcaoyYAsy", "vvzfFcbmKgEsDBRHh", "FfNEt8mpi6qanNmXg", "MrAfiomDNWCzxjei5", "73kwTFKgi4AagxFHJ", "iBBK4j6RWC7znEiDv", "W8vSrHAM9qoWdzFoP", "Rx9GLepCxctXDqCPc", "4X9JLr2SpB6v68twG", "yxTP9FckrwoMjxPc4", "FuZ7MoR3dJEJuoRbN", "xRyLxfytmLFZ6qz5s", "mwGAyWmsSqzMz4WMd", "xxC3Ka7axphW8kJ9E", "KT8Mf3ey6uwQAkWek", "GDT6tKH5ajphXHGny", "ZXaRHHLsxaTTQQsZb", "CHdsSaQGAvtkXBzmJ", "HAEPbGaMygJq8L59k", "SmDziGM9hBjW9DKmf", "8NKu9WES7KeKRWEKK", "NfdHG6oHBJ8Qxc26s", "LTtNXM9shNM9AC2mp", "uKp6tBFStnsvrot5t", "baTWMegR42PAsH9qJ", "Xqcorq5EyJBpZcCrN", "7cAsBPGh98pGyrhz9", "ZbgCx2ntD5eu8Cno9", "9kcTNWopvXFncXgPy", "HxWdXMqoQtjDhhNGA", "xwBuoE9p8GE7RAuhd", "inedT6KkbLSDwZvfd", "sWLLdG6DWJEy3CH7n", "dhj9dhiwhq3DX6W8z", "yLLkWMDbC9ZNKbjDG", "P3Yt66Wh5g7SbkKuT", "brXr7PJ2W4Na2EW2q", "45mNHCMaZgsvfDXbw", "7izSBpNJSEXSAbaFh", "pfoZSkZ389gnz5nZm", "jfG6vdJZCwTQmG7kb", "sGnPTfjE5JthAStqg", "gvA4j8pGYG4xtaTkw", "PZtsoaoSLpKjjbMqM", "jnDibtfvWNHLucf4D", "GrtbTAPfkJa4D6jjH", "zEWJBFFMvQ835nq6h", "64FdKLwmea8MCLWkE", "Dx9LoqsEh3gHNJMDk", "FMkQtPvzsriQAow5q", "XuLG6M7sHuenYWbfC", "PGv9THs68ArPur7yP", "NcGBmDEe5qXB7dFBF", "tEDXpFgsHsm5T8sWz", "7gsehrZnvXo2YGiT7", "x4n4jcoDP7xh5LWLq", "boBZkTqPdboX5u7g9", "CJw2tNHaEimx6nwNy", "CcC8MocynqKPmMPwL", "Rrt7uPJ8r3sYuLrXo", "rwjv8bZfSuE9ZAigH", "khYYedgupgrHonWNc", "wrkEnGrTTrM2mnmGa", "f9s7pHub6hbsX7YKT", "YduZEfz8usGbJXN4x", "55SHk8kh9dDvaDTCC", "SFG9Cm7mf5eP4juKs", "eLRSCC7r4KinuxqZX", "oW6mbA3XHzcfJTwNq", "kWMkDoy3izRTobZFe", "LtsJLfnP4YwhGdaCf", "w9kwayt5SWqBQe8Nx", "h5CGM5qwivGk2f5T9", "iPGpENE4ARKbzzQmt", "PQ3nutgxfTgvq69Xt", "3zZjF3YKJ257x79mu", "9Qwignbzu4ddXLTsT", "aiCtrN9EF2FjKz5sv", "JcpwEKbmNHdwhpq5n", "idipkijjz5PoxAwju", "F7RgpHHDpZYBjZGia", "xWTSHJASRaLABgHWc", "Fg8dtE8HHkDoiGcwt", "zPJE7MDtL25RpN7Cc", "qqhdj3W3vSfB5E9ss", "9SE67uz98kh6x2CxR", "gR6H3egpRPNYnoTrA", "qPoaA5ZSedivA4xJa", "H6L7fuEN9qXDanQ6W", "gfexKxsBDM6v2sCMo", "7uJnA3XDpTgemRH2c", "stb3Jjumzhv49zCEb", "XjMkPyaPYTf7LrKiT", "XuyRMxky6G8gq7a69", "huRxRzwcvwTzvtEPY", "8bWbNwiSGbGi9jXPS", "sq3WkpyqGANT7hGRP", "AyfDnnAdjG7HHeD3d", "WmfapdnpFfHWzkdXY", "8rYxw9xZfwy86jkpG", "zFhhDCxz87yKwqYQf", "doiMq8aH2yiZaCJsT", "MQzbaHoiQutiHkx2M", "ra9Pt2JkEDnKW4jsc", "9YDk52NPrfq7nqLvd", "KTEciTeFwL2tTujZk", "6bSjRezJDxR2omHKE", "r5H6YCmnn8DMtBtxt", "JbcWQCxKWn3y49bNB", "R4FX6wDmppvZ2JqpB", "9vnWFwng8QzEnBT8z", "XCtFBWoMeFwG8myYh", "6uwLq8kofo4Tzxfe2", "G993PFTwqqdQv4eTg", "DWgWbXRfXLGHPgZJM", "K7wtTqTEoKXC9Kb24", "hmai5Lru5kWXpH7Ju", "w4jjwDPa853m9P4ag", "xvAkpCSdqgtYhEceo", "6vMBpZtoRw4ia2JrK", "Wzjjynmp8gMmdX6dt", "CsN6WxwDnPzxAFhps", "CLXkgEerPi9MpJCem", "BKjJJH2cRpJcAnP7T", "qXtbBAxmFkAQLQEJE", "jES7mcPvKpfmzMTgC", "D7epkkJb3CqDTYgX9", "FpcgSoJDNNEZ4BQfj", "mF8dkhZF9hAuLHXaD", "camG6t6SxzfasF42i", "HALKHS4pMbfghxsjD", "HDXLTFnSndhpLj2XZ", "fgYQjTktBmNZvMqce", "fwNskn4dosKng9BCB", "B5auLtDfQrvwEkw4Q", "z7YvA5osMotdL5F4w", "Hoh6umyMWSqzPGMJZ", "vHSrtmr3EBohcw6t8", "nsCwdYJEpmW5Hw5Xm", "LKAXgTen4Xbqb8eZY", "22GrdspteQc8EonMn", "TSaJ9Zcvc3KWh3bjX", "sJK6HN5vTPPnuuNgQ", "mh3xapTix6fFtd3xM", "JBnaLpsrYXLXjFocu", "uR8c2NPp4bWHQ5u45", "d4YGxMpzmvxknHfbe", "wcNEXDHowiWkRxDNv", "scNCmwaduCgJmCBYh", "LsXtcLyzyfGg3gT5R", "McN9BNtNcbYNfdCB5", "4tzEAgdbNTwB6nKyL", "sCFGEhwcB8MX3FQf5", "G4uMdBzgDsxMsTNmr", "34Gkqus9vusXRevR8", "7MCqRnZzvszsxgtJi", "HXxHcRCxR4oHrAsEr", "cmrtpfG7hGEL9Zh9f", "oHk9T3jbx2J5zJ39P", "sYt3ZCrBq2QAf3rak", "r8stxYL29NF9w53am", "zymnWfGwf6BdDt64c", "yyDrMYBfvYtKbmPmm", "4gevjbK77NQS6hybY", "jnjjzkH8Fdzg4D6EK", "XKfQF73YnyMRiRf9a", "gYfgWSxCpFdk2cZfE", "CQsEwAyJP6NYvKZw6", "JiLcxpWzCrnwkndsT", "gpk8dARHBi7Mkmzt9", "GrbeyZzp6NwzSWpds", "9MZdwQ7u53oaRiBYX", "gFyJgnu5vAbzELBM8", "ouQNu3hhfKLBRuwR7", "m5AH78nscsGjMbBwv", "oKYWbXioKaANATxKY", "cq5x4XDnLcBrYbb66", "KjdP2WjWng6skwbY7", "wfpdejMWog4vEDLDg", "7F5jo5LD9FD7DpxCX", "kDjKF2yFhFEWe4hgC", "pWi5WmvDcN4Hn7Bo6", "NGc3Yjecg9pDMznWq", "xxvKhjpcTAJwvtbWM", "DJnvFsZ2maKxPi7v7", "zo9zKcz47JxDErFzQ", "fyZBtNB3Ki3fM4a6Y", "H4kadKrC2xLK24udn", "BxersHYN2qcFoonwg", "Ck5cgNS2Eozc8mBeJ", "wr9dH2GjztvCz6pYX", "EzAt4SbtQcXtDNhHK", "syeBtmGoKvjJTH6sH", "eWqFy8wESHbxNod7i", "8cWMX6L8St8k9pPRC", "jP583FwKepjiWbeoQ", "rMfpnorsMoRwyn4iP", "TKk7rShf9d5ePN7vR", "fNJvYD6XqnX82i4jA", "r8aAqSBeeeMNRtiYK", "Gh2qQHrCg3teQen3c", "3GAnfeG9KmsbsWeTj", "JKgGvJCzNoBQss2bq", "JjGs6mDZxeCWkg3ii", "AzKx6EjaoaMuk595v", "duAkuSqJhGDcfMaTA", "pXLqpguHJzxSjDdx7", "FbJYEn6eWA5JnGeGP", "8GiTowD6XqTNzgCz7", "qfDgEreMoSEtmLTws", "96N8BT9tJvybLbn5z", "SCs4KpcShb23hcTni", "bDMoMvw2PYgijqZCC", "nqwzrpkPvviLHWXaE", "YuZXRxWSqaCoZHEXr", "6YYmkpumigAmh3efu", "SgszmZwrDHwG3qurr", "9EyzaH3jzH3PyQtM5", "eR7Su77N2nK3e5YRZ", "GSBCw94DsxLgDat6r", "cpdsMuAHSWhWnKdog", "avvXAvGhhGgkJDDso", "KnPN7ett8RszE79PH", "ptmmK9PWgYTuWToaZ", "XNhfw5Bqsi4SGNNBk", "PKy8NuNPknenkDY74", "3yqf6zJSwBF34Zbys", "YpyW97jRbtvBAncAr", "LwcKYR8bykM6vDHyo", "H6hMugfY3tDQGfqYL", "iyRpsScBa6y4rduEt", "mLuQfS7gmfr4nwTdv", "TrvkWBwYvvJjSqSCj", "yXHcqrCpiHC5tDuEc", "HKfBeWN8ufNdFgzG6", "P8yeoeJ2bwmnD93mZ", "kxW6q5YdTGWh5sWby", "ksatPnddyZjHwZWwG", "st7DiQP23YQSxumCt", "tE7y8FZe7wSSzoRaS", "L4HQ3gnSrBETRdcGu", "eqxqgFxymP8hXDTt5", "uKWXktrR7KpbgZAs4", "h4vWsBBjASgiQ2pn6", "DXBziiT2RFLcmLY9J", "k42G2aaNhRNB7hdCJ", "XSKQLeQnBupFo7GGC", "BnDF5kejzQLqd5cjH", "AMmqk74zWmvP8tXEJ", "NQQzXpahhkb6f6ZCe", "Tk5ovpucaqweCu4tu", "9WX59u7g2sdKqnjDm", "Xht9swezkGZLAxBrd", "8c8AZq5hgifmnHKSN", "nMNi86hgNjaNnh8iu", "s3rAKTkdSHb6Hwwoz", "rqnbrJhDKCoZvNGEZ", "Ea8pt2dsrS6D4P54F", "uN3wjp2K6TEQ2oAML", "DAc4iuy4D3EiNBt9B", "jqCz2X49FRn5Bgb5b", "8hxvfZiqH24oqyr6y", "puYfAEJJomeodeSsi", "S54HKhxQyttNLATKu", "igSPcmvTigCHxWt8x", "4esQ684vtR9zcjHgW", "yGaw4NqRha8hgx5ny", "eHnupDgggBqDqT5eg", "k7oxdbNaGATZbtEg3", "bbGEiSmNiTpPrFhcQ", "Z6dmoLyfBdmo6HEss", "QcXuwQvvPkqcKZmXS", "7FJRnxbRtT7Sbzizs", "oBTkthd7h8sDpkiu2", "cmiRk9XtT9Psnd3Yr", "G6npMHwgRGSQDKavX", "hwxj4gieR7FWNwYfa", "yGycR8tFA3JJbvApp", "jxy7rBcQink8a7C9b", "vQNJrJqebXEWjJfnz", "kjmpq33kHg7YpeRYW", "FwYMuD2sNcaEpE5on", "4rwABGAd9kZG8nf2P", "GkXKvkLAcTm5ackCq", "TrmMcujGZt5JAtMGg", "gBpYo7mt2zNBmtBJd", "aNRYQFnMQbA7uu99u", "YMokuZdoY9tEDHjzv", "MG8Yhsxqu9JY4xRPr", "zEvqFtT4AtTztfYC4", "fzeoYhKoYPR3tDYFT", "8npC4KRcAJtGdErTq", "AYbhqi65SWzHzy7Xx", "N99KgncSXewWqkzMA", "2KacvW34BbXFmDBtQ", "tSgcorrgBnrCH8nL3", "NHuLAS3oKZWr2X9hP", "9hR2RmpJmxT8dyPo4", "fwSDKTZvraSdmwFsj", "Cf2zBkoocqcjnrNFD", "MPj7t2w3nk4s9EYYh", "TTPux7QFBpKxZtMKE", "shcSdHGPhnLQkpSbX", "M4w2rdYgCKctbADMn", "hMjFMSQZb4swKugfv", "mkrvsNi8cYGSjGqkh", "DXcezGmnBcAYL2Y2u", "Aq8BQMXRZX3BoFd4c", "FoJSa8mgLPT83g9e8", "Xt85tj6GQJCuuXT68", "JAAHjm4iZ2j5Exfo2", "sAiHxHkQrsYsRpKFP", "6phFYpNQH9SmWL9Jt", "Rkxj7TFxhbm59AKJh", "rNFzvii8LtCL5joJo", "Hw26MrLuhGWH7kBLm", "Zvu6ZP47dMLHXMiG3", "HByDKLLdaWEcA2QQD", "7qhtuQLCCvmwCPfXK", "FgjcHiWvADgsocE34", "Lp4Q9kSGsJHLfoHX3", "3xF66BNSC5caZuKyC", "BseaxjsiDPKvGtDrm", "Q924oPJzK92FifuFg", "oJwJzeZ6ar2Hr7KAX", "H7Rs8HqrwBDque8Ru", "gEKHX8WKrXGM4roRC", "FKB7iEergZaC7PvQf", "suxvE2ddnYMPJN9HD", "iETtCZcfmRyHp69w4", "mz3hwS4c9bc9EHAm9", "KFLdfuw35qkgjzWer", "RApxEu3A4GnvGoEe2", "XLbDQL2qYi9FDozvL", "p4XpZWcQksSiCPG72", "mB95aqTSJLNR9YyjH", "2NaAhMPGub8F2Pbr7", "BbM47qBPzdSRruY4z", "dYnHLWMXCYdm9xu5j", "qHpazCw3ryvBojGSa", "wyYubb3eC5FS365nk", "wmjPGE8TZKNLSKzm4", "CBWSDdzjqfnexBurB", "gBnSRErajRtvhMnDr", "BfBF6T6HA82zBxPrv", "dbDHEQyKqnMDDqq2G", "doPejjd84w8BmERqj", "PT8vSxsusqWuN7JXp", "dKxX76SCfCvceJXHv", "DSzpr8Y9299jdDLc9", "hnLutdvjC8kPScPAj", "vit9oWGj6WgXpRhce", "CsKboswS3z5iaiutC", "kjQXzkTGuixoJtQnq", "RgJicDmXHDxcJ9Fsw", "L6iFpR9ZyTmzHvYci", "Z5ZBPEgufmDsm7LAv", "PRAyQaiMWg2La7XQy", "x6Kv7nxKHfLGtPJej", "3pjv6uDvY9sqmsnvY", "Aet2mbnK7GDDfrEQu", "scL68JtnSr3iakuc6", "3SG4WbNPoP8fsuZgs", "XfpJ6WQBDcEcC8Mu4", "iprqfLaDLCGoJFeiZ", "frApEhpyKQAcFvbXJ", "znBJwbuT3f5eWgM4E", "cR7Zfrc4BtnFes46y", "hbmsW2k9DxED5Z4eJ", "SzecSPYxqRa5GCaSF", "hxaq9MCaSrwWPmooZ", "FSmPtu7foXwNYpWiB", "WQWhXzALcrzrJtqRh", "jYNT3Qihn2aAYaaPb", "gebzzEwn2TaA6rGkc", "WhHFvzFsYfMxgYCdo", "tjxgbovwc5Ft7wrtc", "2brqzQWfmNx5Agdrx", "QaDwBio8MLqRvTREH", "Jko7pt7MwwTBrfG3A", "A9tJFJY7DsGTFKKkh", "Wnqua6eQkewL3bqsF", "DJB82jKwgJE5NsWgT", "5b6YcFbEBCZbX6YSK", "zk6RK3xFaDeJHsoym", "FQqcejhNWGG8vHDch", "srge9MCLHSiwzaX6r", "DJRe5obJd7kqCkvRr", "D8ds9idKWbwzCseCh", "hTMFt3h7QqA2qecn7", "9LXxgXySTFsnookkw", "CHtwDXy63BsLkQx4n", "u5RLu5F3zKTB3Qjnu", "4tke3ibK9zfnvh9sE", "2WngsveoLhFubuLMH", "ADwayvunaJqBLzawa", "NG6FrXgmqPd5Wn3mh", "Ww5xKq5brC4xAJY7o", "HL6x8zHo9BkuK3tic", "PKBXczqhry7iK3Ruw", "oBBzqkZwkxDvsKBGB", "HuFZJkGptWDtRbkWs", "iQWk5jYeDg5ACCmpx", "RdpqsQ6xbHzyckW9m", "sizjfDgCgAsuLJQmm", "X3p8mxE5dHYDZNxCm", "wZGpoZgDANdkwTrwt", "uAc7bWgpEhrGwFcv7", "3nDR23ksSQJ98WNDm", "sMsvcdxbK2Xqx8EHr", "evYFijNMdjfbPaCho", "Psp8ZpYLCDJjshpRb", "Zupr296Zy74wpihXT", "68dHanLWsS6SEyZp9", "x9FNKTEt68Rz6wQ6P", "DWHkxqX4t79aThDkg", "xLm9mgJRPvmPGpo7Q", "6LzKRP88mhL9NKNrS", "XYDsYSbBjqgPAgcoQ", "eRohP4gbxuBuhqTbe", "Wpf3Gsa8A89mmjkk8", "PfcQguFpT8CDHcozj", "XPwEptSSFRCnfHqFk", "pohTfSGsNQZYbGpCy", "zcPLNNw4wgBX5k8kQ", "2meuc3kPRkBcRpj3R", "bzhGBHrGrFfQss4Df", "2269iGRnWruLHsZ5r", "kj37Hzb2MsALwLqWt", "Qz9GvoPbnFwGrHHQB", "pJJdcZgB6mPNWoSWr", "dtmmP4YdJEfK9y4Rc", "QPqm5aj2meRmE7kR8", "2oybbEw697CQgcRE5", "TYTEJxzeK3jBMq2TZ", "K4eDzqS2rbcBDsCLZ", "FcRt3xAF4ynojfj6G", "gMXsyhPiEJbGerF6F", "9sguwESkteCgqFMbj", "mvPfao35Moah8py46", "kuDKtwwbsksAW4BG2", "pL56xPoniLvtMDQ4J", "ENBzEkoyvdakz4w5d", "wM4bcDxEh75NDkhjo", "YAkpzvjC768Jm2TYb", "ExssKjAaXEEYcnzPd", "n3LAgnHg6ashQK3fF", "GMCs73dCPTL8dWYGq", "8gapy2nLy4wysXSGL", "dgFcJtHaYfaoByAK9", "HhWhaSzQr6xmBki8F", "CpvyhFy9WvCNsifkY", "aan3jPEEwPhrcGZjj", "mhA4vkeaRn9cpxkag", "iA25AvZqAr6G8mAXR", "C4tR3BEpuWviT7Sje", "FghubkDy6Dp6mnxk7", "RKz7pc6snBttndxXz", "jiJquD34sa9Lyo5wc", "c8EeJtqnsKyXdLtc5", "ZGGGBR9sDgtLgMDaA", "uM6mENiJi2pNPpdnC", "o9dnstYoc7cwpgdhg", "YSWa8rYeD3aDaofSP", "pC47ZTsPNAkjavkXs", "QtyKq4BDyuJ3tysoK", "bYrF8rXFYwPqnfxTp", "KbyRPCAsWv5GtfrbG", "c2RzFadrxkzyRAFXa", "9ZodFr54FtpLThHZh", "xmoYza9vgcRvWD5PA", "sbb9bZgojmEa7Yjrc", "6yTShbTdtATxKonY5", "BHYBdijDcAKQ6e45Z", "qGEqpy7J78bZh3awf", "KJbQyFbXiiYDDWbaS", "PYtus925Gcg7cqTEq", "yTvBSFrXhZfL8vr5a", "Aud7CL7uhz55KL8jG", "bXTNKjsD4y3fabhwR", "AmNjHo8xXMKnZEWRS", "CHD5m9fnosr7L3dto", "MN4NRkMw7ggt9587K", "CDXDnruBJe23rpdfC", "y5GftLezdozEHdXkL", "d6yNW5T6J9rtnGizc", "pT48swb8LoPowiAzR", "27AWRKbKyXuzQoaSk", "vNHf7dx5QZA4SLSZb", "KwbJFexa4MEdhJbs4", "mja6jZ6k9gAwki9Nu", "fW9n8bEuMpLwkxCx6", "muXfZr5EYCfZqLmsb", "5PBWgHiCiiJHjPRSn", "PAYMMgPi2L3MPP967", "RaxaXBNmStYe289gC", "DMxe4XKXnjyMEAAGw", "xF7gBJYsy6qenmmCS", "gMszBSAX23uqYhytR", "HbXXd2givHBBLxr3d", "Z5wF8mdonsM2AuGgt", "utySCY9nJt9xGYGGQ", "gCz7cB6JG66EhweSS", "krHDNc7cDvfEL8z9a", "aNAFrGbzXddQBMDqh", "sksP9Lkv9wqaAhXsA", "p3s8RvkcyTwzu27ps", "8ccTZ9ZxpJrvnxt4F", "p7WXmG6Fbo3eaSwm3", "CPBmbgYZpsGqkiz2R", "yDRX2fdkm3HqfTpav", "WbLAA8qZQNdbRgKte", "75dnjiD8kv2khe9eQ", "JZZENevaLzLLeC3zn", "MgFDzAfCku9MSDLuw", "PQtEqmyqHWDa2vf5H", "zbqLuTgTCu365MNu9", "P3uavjFmZD5RopJKk", "8gqrbnW758qjHFTrH", "pZaPhGg2hmmPwByHc", "4hLcbXaqudM9wSeor", "WxW6Gc6f2z3mzmqKs", "j9HoG56Y6KuopSzdn", "GhFoAxG49RXFzze5Y", "rD57ysqawarsbry6v", "LCfaLXcWnk8pujnX4", "tAXrD8Y6hcJ8dt6Nt", "af9MjBqF2hgu3EN6r", "FRRb6Gqem8k69ocbi", "LbyxFk8JmPKPAQBvL", "PHmYhE4sKnwzYgvkh", "fZJRxYLtNNzpbWZAA", "kgmkdf3C7EkDX7dnT", "Gs29k3beHiqWFZqnn", "MMAK6eeMCH3JGuqeZ", "cdB5f2adKoLGW8Ytc", "5e49dHLDJoDpeXGnh", "Ccsx339LE9Jhoii9K", "PHnMDhfiadQt6Gj23", "Jo89KvfAs9z7owoZp", "fri4HdDkwhayCYFaE", "tD9zEiHfkvakpnNam", "xggxWfyzZmnz7hydm", "JgBBuDf5uZHmpEMDs", "vbcjYg6h3XzuqaaN8", "hRohhttbtpY3SHmmD", "6KzFwcDy7hsCkzJKY", "F2DZXsMdhGyX4FPAd", "esRZaPXSHgWzyB2NL", "AqsjZwxHNqH64C2b6", "4psQW7vRwt7PE5Pnj", "voLHQgNncnjjgAPH7", "aaHDA4X6cTzFrvuSX", "LHtMNz7ua8zu4rSZr", "zjMKpSB2Xccn9qi5t", "BAzCGCys4BkzGDCWR", "goC9qv4PWf2cjfnbm", "Z2CuyKtkCmWGQtAEh", "c3iQryHA4tnAvPZEv", "vwLxd6hhFvPbvKmBH", "Js34Ez9nrDeJCTYQL", "fJvjin8ETkzhFdadC", "W59Nb72sYJhMJKGB8", "xiPMaYGTm2xfsB8WF", "oPEWyxJjRo4oKHzMu", "PjfsbKrK5MnJDDoFr", "sBBGxdvhKcppQWZZE", "vwM7hnT9ysE3suwfk", "BzYmJYECAc3xyCTt6", "uiyWHaTrz3ML7JqDX", "vZssZr2wq7YrG3FMa", "73QyjLymEak4L8RDC", "6vcxuRHzeM99jYcYd", "bG4PR9uSsZqHg2gYY", "HoQ5Rp7Gs6rebusNP", "9iA87EfNKnREgdTJN", "QEYWkRoCn4fZxXQAY", "kAgJJa3HLSZxsuSrf", "FZaDFYbnRoHmde7F6", "BNfL58ijGawgpkh9b", "4gDbqL3Tods8kHDqs", "DwqgLXn5qYC7GqExF", "atcJqdhCxTZiJSxo2", "zRn6cLtxyNodudzhw", "P32AuYu9MqM2ejKKY", "K2JBqDeETX2yEgyyZ", "3FoMuCLqZggTxoC3S", "LcEzxX2FNTKbB6KXS", "o5F2p3krzT4JgzqQc", "cy3BhHrGinZCp3LXE", "zsG9yKcriht2doRhM", "WYmmC3W6ZNhEgAmWG", "EL4HNa92Z95FKL9R2", "EKu66pFKDHFYPaZ6q", "Pa5NqtxHBkGuCh98G", "JKj5Krff5oKMb8TjT", "vwt3wKXWaCvqZyF74", "4basF9w9jaPZpoC8R", "Bfq6ncLfYdtCb6sat", "jDQm7YJxLnMnSNHFu", "FDJnZt8Ks2djouQTZ", "f3o9ydY7iPjFF2fyk", "KnQs55tjxWopCzKsk", "Ww2dxwWpSfkQB4NZb", "ZawRiFR8ytvpqfBPX", "ZGzDNfNCXzfx6hYAH", "rFjhz5Ks685xHbMXW", "Mrz2srZWc7EzbADSo", "B4DuwmtqF3HhNwvua", "zQKgKjecvR4W7oJw5", "BSpdshJWGAW6TuNzZ", "JHcTP4Ad8QAmRTCZm", "GGn8MBiY8Xz6NdNdH", "hQysqfSEzciRazx8k", "AtfQFj8umeyBBkkxa", "r99tazGiLgzqFX7ka", "uFYQaGCRwt3wKtyZP", "BFamedwSgRdGGKXQQ", "teaxCFgtmCQ3E9fy8", "ka8eveZpT7hXLhRTM", "euJm4RwkAptZnP89i", "LLRtjkvh9AackwuNB", "yPLr2tnXbiFXkMWvk", "ervaGwJ2ZcwqfCcLx", "4AHXDwcGab5PhKhHT", "NuueGqPZdotjMQKLu", "qjSHfbjmSyMnGR9DS", "xtzvtJBNofk4FPAtt", "SkcM4hwgH3AP6iqjs", "Br4xDbYu4Frwrb64a", "HvcZmKS43SLCbJvRb", "BEtzRE2M5m9YEAQpX", "EhEZoTFzys9EDmEXn", "bmoQ2wy7Nd7EiJdpg", "pYcFPMBtQveAjcSfH", "zb3hWt99i9Fm93KPq", "W9rJv26sxs4g2B9bL", "Dod9AWz8Rp4Svdpof", "hQHuXuRGZxxWXaPgg", "zB3ukZJqt3pQDw9jz", "KheBaeW8Pi7LwewoF", "Ek7M3xGAoXDdQkPZQ", "guDcrPqLsnhEjrPZj", "7XbcDaeigMaxW43EB", "ttGbpJQ8shBi8hDhh", "wJnm5cBiZGmKn595f", "puhPJimawPuNZ5wAR", "eoHbneGvqDu25Hasc", "gHgs2e2J5azvGFatb", "x5ASTMPKPowLKpLpZ", "EhAbh2pQoAXkm9yor", "jfq2BH5kfQqu2vYv3", "Mf2MCkYgSZSJRz5nM", "mXgsd5o9uuYaQKHMz", "YM6Qgiz9RT7EmeFpp", "PcfHSSAMNFMgdqFyB", "uX3HjXo6BWos3Zgy5", "nzmCvRvPm4xJuqztv", "CMt3ijXYuCynhPWXa", "Ndtb22KYBxpBsagpj", "yFJ7vCjefBxnTchmG", "SQ9cZtfrzDJmw9A2m", "PJLABqQ962hZEqhdB", "HmfxSWnqnK265GEFM", "i3BTagvt3HbPMx6PN", "ZEgQGAjQm5rTAnGuM", "ctpkTaqTKbmm6uRgC", "qEweugBipR5P2cMyK", "xnPFYBuaGhpq869mY", "YtvZxRpZjcFNwJecS", "ido3qfidfDJbigTEQ", "85J8hjEn48FicYfvp", "N6vZEnCn6A95Xn39p", "tJQsxD34maYw2g5E4", "96TBXaHwLbFyeAxrg", "ixZLTmFfnKRbaStA5", "2x7fwbwb35sG8QmEt", "oaqKjHbgsoqEXBMZ2", "t2NN6JwMFaqANuLqH", "J9pNx22bj5RuiRjAj", "AN2cBr6xKWCB8dRQG", "G5eMM3Wp3hbCuKKPE", "y5jAuKqkShdjMNZab", "vADtvr9iDeYsCDfxd", "x4GmqcwjFTnWeRiud", "5ntgky9ShzKKWu7us", "z8usYeKX7dtTWsEnk", "3S4nyoNEEuvNsbXt8", "EEv9JeuY5xfuDDSgF", "ASpGaS3HGEQCbJbjS", "AXXaXJvf7WcTessog", "QL7J9wmS6W2fWpofd", "osYFcQtxnRKB4F4HA", "MajyZJrsf8fAywWgY", "bvqC4Ci7rXq4sN9df", "GctJD5oCDRxCspEaZ", "A9NxPTwbw6r6Awuwt", "dKTh9Td3KaJ8QW6gw", "oTX2LXHqXqYg2u4g6", "LuXb6CZG4x7pDRBP8", "hamma4XgeNrsvAJv5", "BfTW9jmDzujYkhjAb", "DoHcgTvyxdorAMquE", "EbFABnst8LsidYs5Y", "Sdx6A6yLByRRs8iLY", "qbHLGo5vu8HD3JqEM", "48WeP7oTec3kBEada", "LgavAYtzFQZKg95WC", "5QpufhoH2ASnppsjs", "Kz9zMgWB5C27Pmdkh", "qy5dF7bQcFjSKaW58", "wkuDgmpxwbu2M2k3w", "JcpzFpPBSmzuksmWM", "zMxrkFrB6ka4Lb7fM", "PX7AdEkpuChKqrNoj", "ui6mDLdqXkaXiDMJ5", "uXn3LyA8eNqpvdoZw", "FwiPfF8Woe5JrzqEu", "hzuSDMx7pd2uxFc5w", "mHqQxwKuzZS69CXX5", "yKXKcyoBzWtECzXrE", "zHS4FJhByRjqsuH4o", "a5JAiTdytou3Jg749", "HEn2qiMxk5BggN83J", "tYAvXXgSwHCzNTK8f", "WXvt8bxYnwBYpy9oT", "kLR5H4pbaBjzZxLv6", "CtXaFo3hikGMWW4C9", "4DBBQkEQvNEWafkek", "qwdupkFd6kmeZHYXy", "EHbJ69JDs4suovpLw", "w5F4w8tNZc6LcBKRP", "xqkGmfikqapbJ2YMj", "yRAo2KEGWenKYZG9K", "scwoBEju75C45W5n3", "qJgz2YapqpFEDTLKn", "aSQy7yHj6nPD44RNo", "Ltey8BS83qSkd9M3u", "9Yc7Pp7szcjPgPsjf", "hN2aRnu798yas5b2k", "ERWeEA8op6s6tYCKy", "yJfBzcDL9fBHJfZ6P", "BZ6XaCwN4QGgH9CxF", "3nMpdmt8LrzxQnkGp", "TNHQLZK5pHbxdnz4e", "F6ZTtBXn2cFLmWPdM", "neQ7eXuaXpiYw7SBy", "k2SNji3jXaLGhBeYP", "WsSybGTqpBoHpXJyQ", "jtMXj24Masrnq3SpS", "jqTeghCJ2anMHPPjG", "B7P97C27rvHPz3s9B", "uK6sQCNMw8WKzJeCQ", "hurF9uFGkJYXzpHEE", "xEHy9oivifjgFbnvc", "33KewgYhNSxFpbpXg", "c5GHf2kMGhA4Tsj4g", "dC7mP5nSwvpL65Qu5", "hpjou9ZnLZkSJR7sd", "bshZiaLefDejvPKuS", "AvjbBjAAbKBk73v5F", "XqvnWFtRD2keJdwjX", "KJ9MFBPwXGwNpadf2", "37sHjeisS9uJufi4u", "5iZTwGHv2tNfFmeDa", "gziZACDg6EBpGZbJe", "RYcoJdvmoBbi5Nax7", "9o3QBg2xJXcRCxGjS", "vs3kzjLhbdKsndnBy", "bZ2w99pEAeAbKnKqo", "bjjbp5i5G8bekJuxv", "vwqLfDfsHmiavFAGP", "Yp2vYb4zHXEeoTkJc", "z6QQJbtpkEAX3Aojj", "ubPAo3zGeJNqtZDqT", "pfibDHFZ3waBo6pAc", "cumc876woKaZLmQs5", "Ty2tjPwv8uyPK9vrz", "ZiQqsgGX6a42Sfpii", "ybYBCK9D7MZCcdArB", "pNcFYZnPdXyL2RfgA", "rEBXN3x6kXgD4pLxs", "no5jDTut5Byjqb4j5", "qCsxiojX7BSLuuBgQ", "uyBeAN5jPEATMqKkX", "aHaqgTNnFzD7NGLMx", "bQ6zpf6buWgP939ov", "mkbGjzxD8d8XqKHzA", "CKpByWmsZ8WmpHtYa", "midXmMb2Xg37F2Kgn", "reitXJgJXFzKpdKyd", "LFNXiQuGrar3duBzJ", "KcvJXhKqx4itFNWty", "RWu8eZqbwgB9zaerh", "EFQ3F6kmt4WHXRqik", "FfPukic3Qskd9ZAkk", "A2Qam9Bd9xpbb2wLQ", "t9svvNPNmFf5Qa3TA", "n5TqCuizyJDfAPjkr", "Kbm6QnJv9dgWsPHQP", "gFMH3Cqw4XxwL69iy", "Kyc5dFDzBg4WccrbK", "RWo4LwFzpHNQCTcYt", "vbWBJGWyWyKyoxLBe", "PsEppdvgRisz5xAHG", "tscc3e5eujrsEeFN4", "GG2rtBReAm6o3mrtn", "E4cKD9iTWHaE7f3AJ", "yCWPkLi8wJvewPbEp", "AcKRB8wDpdaN6v6ru", "LbrPTJ4fmABEdEnLf", "rtM3jFaoQn3eoAiPh", "eDicGjD9yte6FLSie", "xg3hXCYQPJkwHyik2", "bJ2haLkcGeLtTWaD5", "PBRWb2Em5SNeWYwwB", "7hFeMWC6Y5eaSixbD", "aMHq4mA2PHSM2TMoH", "wpZJvgQ4HvJE2bysy", "2brqzQWfmNx5Agdrx", "GLMFmFvXGyAcG25ni", "NLBbCQeNLFvBJJkrt", "bYrF8rXFYwPqnfxTp", "v7c47vjta3mavY3QC", "3MvaoZbGPxtRFCijw", "TappK5n3kZmQzWEWD", "tSemJckYr29Gnxod2", "WdkLDpBGMCWhfByAY", "EctieqKwDQcQHhqZy", "hNqte2p48nqKux3wS", "qw3Z79HELMsmLkL9F", "zwDz9pgT43fRczkB4", "Fafzj3wMvoCW4WjeF", "kxW6q5YdTGWh5sWby", "G5eMM3Wp3hbCuKKPE", "kSiT2XjfTnDHKx44W", "DSzpr8Y9299jdDLc9", "wZGpoZgDANdkwTrwt", "qajfiXo5qRThZQG7s", "rRzZzBBQ36CrqhZTY", "aP36QcAsxyuEispq6", "TxcRbCYHaeL59aY7E", "MFNJ7kQttCuCXHp8P", "PQ3nutgxfTgvq69Xt", "JJFphYfMsdFMuprBy", "ythFNoiAotjvuEGkg", "GZSzMqr8hAB2dR8pk", "BBQ5HEnL3ShefQxEj", "fzeoYhKoYPR3tDYFT", "bXuAXCbzw9hsJSuEN", "mbCccXJuuRBZdXdpH", "m7THsgXyxxiEXgyHv", "xtHd6sfdr2bZHa6Pb", "pfaTqpWFghfrbvzaD", "u8GMcpEN9Z6aQiCvp", "gBpYo7mt2zNBmtBJd", "rkpDX7j7va6c8Q7cZ", "NGkBfd8LTqcpbQn5Z", "GEPX7jgLMB8vR2qaK"]
                },
                "locale": "en-US",
                "mapbox": {
                    "apiKey": "pk.eyJ1IjoiaGFicnlrYSIsImEiOiJjaWxvcnhidzgwOGlodHJrbmJ2bmVmdjRtIn0.inr-_5rWOOslGQxY8iDFOA"
                },
                "petrov": {
                    "afterTime": 1727400080403,
                    "beforeTime": 1727376805595,
                    "petrovPostId": "6LJ6xcHEjKF9zWKzs",
                    "petrovServerUrl": "https://forum.effectivealtruism.org/graphql",
                    "petrovGamePostId": "KTEciTeFwL2tTujZk"
                },
                "reacts": {
                    "addNewReactKarmaThreshold": 10,
                    "downvoteExistingReactKarmaThreshold": 20,
                    "addNameToExistingReactKarmaThreshold": 5
                },
                "stripe": {
                    "publicKey": "pk_live_51HtKAwA2QvoATZCZiy9f2nc6hA52YS1BE81cFu9FEV1IKar0Bwx6hIpxxxYHnhaxO9KM7kRYofZId3sUUI7Q0NeO00tGni3Wza"
                },
                "algolia": {
                    "appId": "fakeAppId",
                    "searchKey": "fakeSearchKey",
                    "indexPrefix": "test_"
                },
                "llmChat": {
                    "userIds": ["McgHKH6MMYSnPwQcm", "6Fx2vQtkYSZkaCvAg", "MEu8MdhruX5jfGsFQ", "YaNNYeR5HjKLDBefQ", "hBEAsEpoNHaZfefxR", "NFmcwmaFeTWfgrvBN", "ZnpELPxzzD2CiigNy", "Q7NW4XaWQmfPfdcFj", "NXeHNNSFHGESrYkPv", "QDNJ93vrjoaRBesk2", "iMBN2523tmh4Yicc3", "5iPRfSnjako6iM6LG", "aBHfQ4C5fSM4TPyTn", "n4M37rPXGyL6p8ivK", "e9ToWWzhwWp5GSE7P", "TCjNiBLBPyhZq5BuM", "XLwKyCK7JmC292ZCC", "S3ydcLKdejjkodNut", "ENgxBL95Sc7MRwYty", "KCExMGwS2ETzN3Ksr", "XGEcH5rmq4yGvD82A", "YFiFbXgjBpDKZT93g", "dZMo8p7fGCgPMfdfD", "Pdca6FNZBrXj9z28n", "LHbu27FubhwFv8ZJt", "gYxdDBQ3AZbde8HgZ", "5JqkvjdNcxwN8D86a", "6c2KCEXTGogBZ9KoE", "haTrhurXNmNN8EiXc", "cJnvyeYrotgZgfG8W", "3ryuHfhqp8BsLPWg9"]
                },
                "logoUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1498011194/LessWrong_Logo_skglnw.svg",
                "ckEditor": {
                    "uploadUrl": "https://39669.cke-cs.com/easyimage/upload/",
                    "webSocketUrl": "39669.cke-cs.com/ws"
                },
                "recombee": {
                    "enabled": true
                },
                "hasEvents": true,
                "logRocket": {
                    "apiKey": "mtnxzn/lesswrong",
                    "sampleDensity": 5
                },
                "reCaptcha": {
                    "apiKey": "6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"
                },
                "siteImage": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg",
                "cloudinary": {
                    "cloudName": "lesswrong-2-0",
                    "uploadPresetBanner": "navcjwf7",
                    "uploadPresetGridImage": "tz0mgw2s",
                    "uploadPresetSocialPreview": "nn5tppry"
                },
                "googleMaps": {
                    "apiKey": "AIzaSyA3C48rl26gynG3qIuNuS-3Bh_Zz9jFXkY"
                },
                "adminAccount": {
                    "email": "team@lesswrong.com",
                    "username": "LessWrong"
                },
                "annualReview": {
                    "end": "2024-02-01T08:00:00Z",
                    "start": "2023-12-04T00:10:00Z",
                    "reviewPhaseEnd": "2024-01-15T08:00:00Z",
                    "votingPhaseEnd": "2024-02-01T08:00:00Z",
                    "nominationPhaseEnd": "2023-12-17T08:00:00Z",
                    "votingResultsPostId": "TSaJ9Zcvc3KWh3bjX",
                    "announcementPostPath": "/posts/B6CxEApaatATzown6/the-lesswrong-2022-review",
                    "reviewWinnerSectionsInfo": {
                        "modeling": {
                            "tag": "World Modeling",
                            "order": 2,
                            "title": "World",
                            "coords": {
                                "leftXPct": 0.05,
                                "leftYPct": 0,
                                "rightXPct": 0.57,
                                "rightYPct": 0,
                                "middleXPct": 0.31,
                                "middleYPct": 0,
                                "leftFlipped": true,
                                "leftWidthPct": 0.26,
                                "rightWidthPct": 0.26,
                                "middleWidthPct": 0.26
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753450/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_15ba02c3-b268-45f1-a780-322bbaa6fc22_eu9l0l.png"
                        },
                        "ai safety": {
                            "tag": "AI",
                            "order": 5,
                            "title": "Technical AI Safety",
                            "coords": {
                                "leftXPct": 0.2,
                                "leftYPct": 0.3,
                                "rightXPct": 0.554,
                                "rightYPct": 0.3,
                                "middleXPct": 0.467,
                                "middleYPct": 0.3,
                                "leftFlipped": false,
                                "leftWidthPct": 0.267,
                                "rightFlipped": true,
                                "middleFlipped": false,
                                "rightWidthPct": 0.267,
                                "middleWidthPct": 0.267
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,fl_progressive,q_auto/v1708570131/lwbot_topographic_watercolor_artwork_of_a_giant_robot_hand_gent_e4e9f305-9611-4787-8768-d7af3d702ed4_ta2ii9.png"
                        },
                        "practical": {
                            "tag": "Practical",
                            "order": 3,
                            "title": "Practical",
                            "coords": {
                                "leftXPct": 0.2,
                                "leftYPct": 0.05,
                                "rightXPct": 0.634,
                                "rightYPct": 0.05,
                                "middleXPct": 0.417,
                                "middleYPct": 0.05,
                                "leftFlipped": false,
                                "leftWidthPct": 0.217,
                                "rightWidthPct": 0.217,
                                "middleWidthPct": 0.217
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708974564/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_4f6449e2-569b-48a3-b878-a400315b3ef0_hqutxe.png"
                        },
                        "ai strategy": {
                            "tag": "AI",
                            "order": 4,
                            "title": "AI Strategy",
                            "coords": {
                                "leftXPct": 0,
                                "leftYPct": 0,
                                "rightXPct": 0.66,
                                "rightYPct": 0,
                                "middleXPct": 0.33,
                                "middleYPct": 0,
                                "leftFlipped": false,
                                "leftWidthPct": 0.33,
                                "rightFlipped": true,
                                "middleFlipped": false,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753570/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_8dda30ee-71d6-4b24-80c7-a8499a5b25c6_uacvgk.png"
                        },
                        "rationality": {
                            "tag": "Rationality",
                            "order": 0,
                            "title": "Rationality",
                            "coords": {
                                "leftXPct": 0.12,
                                "leftYPct": 0,
                                "rightXPct": 0.72,
                                "rightYPct": 0,
                                "middleXPct": 0.42,
                                "middleYPct": 0,
                                "leftFlipped": false,
                                "leftWidthPct": 0.3,
                                "rightFlipped": true,
                                "rightWidthPct": 0.3,
                                "middleWidthPct": 0.3
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753260/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_09275054-eb84-43c4-9cfa-4a05e1818c9e_rmov5i.png"
                        },
                        "optimization": {
                            "tag": "World Optimization",
                            "order": 1,
                            "title": "Optimization",
                            "coords": {
                                "leftXPct": 0.1,
                                "leftYPct": 0.2,
                                "rightXPct": 0.7,
                                "rightYPct": 0.2,
                                "middleXPct": 0.4,
                                "middleYPct": 0.2,
                                "leftWidthPct": 0.33,
                                "rightFlipped": true,
                                "middleFlipped": false,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753382/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_242eda7f-95a9-4c3b-8090-991a1b11286f_xcjhxq.png"
                        }
                    },
                    "reviewWinnerYearGroupsInfo": {
                        "2018": {
                            "tag": null,
                            "coords": {
                                "leftXPct": 0.01,
                                "leftYPct": 0.1,
                                "rightXPct": 0.72,
                                "rightYPct": 0.1,
                                "middleXPct": 0.34,
                                "middleYPct": 0.1,
                                "leftFlipped": false,
                                "leftWidthPct": 0.33,
                                "rightFlipped": false,
                                "middleFlipped": false,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008323/ruby37_green_on_white_aquarelle_sketch_by_thomas_schaller_of_ri_7a3fa89a-ac7a-466f-929f-b396cb4d9bd5_p8rh9t.png"
                        },
                        "2019": {
                            "tag": null,
                            "coords": {
                                "leftXPct": 0.01,
                                "leftYPct": 0.1,
                                "rightXPct": 0.72,
                                "rightYPct": 0.1,
                                "middleXPct": 0.34,
                                "middleYPct": 0.1,
                                "leftFlipped": false,
                                "leftWidthPct": 0.33,
                                "rightFlipped": false,
                                "middleFlipped": false,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008331/ruby37_blue_on_white_aquarelle_sketch_by_thomas_schaller_of_gre_f421cc99-2bb5-4357-b164-d05c2f4fe84e_aib1co.png"
                        },
                        "2020": {
                            "tag": null,
                            "coords": {
                                "leftXPct": 0.01,
                                "leftYPct": 0.01,
                                "rightXPct": 0.72,
                                "rightYPct": 0.01,
                                "middleXPct": 0.34,
                                "middleYPct": 0.01,
                                "leftFlipped": false,
                                "leftWidthPct": 0.33,
                                "rightFlipped": false,
                                "middleFlipped": false,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008346/ruby37_aquarelle_sketch_of_futuristic_landscape_by_thomas_schal_f07d5805-9fb0-4dcc-9295-7f063624e28c_slcokh.png"
                        },
                        "2021": {
                            "tag": null,
                            "coords": {
                                "leftXPct": 0.01,
                                "leftYPct": 0.1,
                                "rightXPct": 0.545,
                                "rightYPct": 0.1,
                                "middleXPct": 0.278,
                                "middleYPct": 0.1,
                                "leftFlipped": false,
                                "leftWidthPct": 0.267,
                                "rightFlipped": false,
                                "middleFlipped": false,
                                "rightWidthPct": 0.267,
                                "middleWidthPct": 0.267
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/a_270/q_auto,f_auto/ohabryka_Topographic_aquarelle_book_cover_by_Thomas_W._Schaller_f9c9dbbe-4880-4f12-8ebb-b8f0b900abc1_m4k6dy_734413"
                        },
                        "2022": {
                            "tag": null,
                            "coords": {
                                "leftXPct": 0,
                                "leftYPct": 0.1,
                                "rightXPct": 0.79,
                                "rightYPct": 0.1,
                                "middleXPct": 0.43,
                                "middleYPct": 0.1,
                                "leftFlipped": false,
                                "leftWidthPct": 0.33,
                                "rightFlipped": true,
                                "rightWidthPct": 0.33,
                                "middleWidthPct": 0.33
                            },
                            "imgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008351/ruby37_aquarelle_sketch_of_a_woman_focusing_hard_studying_in_an_2ac568ef-408e-4561-acc8-84c76bb42fba_gwt8uq.png"
                        }
                    },
                    "showReviewOnFrontPageIfActive": true
                },
                "googleVertex": {
                    "enabled": false
                },
                "intercomAppId": "wtb8z7sj",
                "commentInterval": 15,
                "googleDocImport": {
                    "enabled": true
                },
                "moderationEmail": "team@lesswrong.com",
                "timeDecayFactor": 1.15,
                "googleTagManager": {
                    "apiKey": "GTM-TRC765W"
                },
                "textReplacements": {
                    "Less Wrong": "Down Bad",
                    "Alignment Forum": "Standards Committee",
                    "Artificial Intelligence": "Fake News"
                },
                "ultraFeedEnabled": true,
                "alternateHomePage": false,
                "gatherTownMessage": "Schelling social hours on Tues 1pm and Thurs 6pm PT",
                "bookDisplaySetting": false,
                "gardenOpenToPublic": false,
                "karmaRewarderId100": "iqWr6C3oEB4yWpzn5",
                "legacyRouteAcronym": "lw",
                "maxRenderQueueSize": 3,
                "recommendationsTab": {
                    "manuallyStickiedPostIds": []
                },
                "frontpageScoreBonus": 0,
                "karmaRewarderId1000": "mBBmKWkmw8bgJmGiG",
                "lightconeFundraiser": {
                    "active": false,
                    "postId": "5n2ZQcbc7r4R8mvqc",
                    "paymentLinkId": "plink_1QPdGLBlb9vL5IMTvkJ3LZ6v",
                    "unsyncedAmount": 2082623.2,
                    "thermometerBgUrl": "https://res.cloudinary.com/lesswrong-2-0/image/upload/q_auto,f_auto,h_400/v1732869999/Group_1_b4ap4h.png",
                    "thermometerGoalAmount": 1000000,
                    "thermometerGoal2Amount": 2000000
                },
                "defaultVisibilityTags": [{
                    "tagId": "Ng8Gice9KNkncxqcj",
                    "tagName": "Rationality",
                    "filterMode": 10
                }, {
                    "tagId": "3uE2pXvbcnS9nnZRE",
                    "tagName": "World Modeling",
                    "filterMode": 10
                }],
                "enableGoodHeartProject": false,
                "maxDocumentsPerRequest": 5000,
                "defaultSequenceBannerId": "sequences/vnyzzznenju0hzdv6pqb.jpg",
                "defaultModeratorComments": [{
                    "id": "FfMok764BCY6ScqWm",
                    "label": "Option A"
                }, {
                    "id": "yMHoNoYZdk5cKa3wQ",
                    "label": "Option B"
                }],
                "newUserIconKarmaThreshold": 50,
                "dialogueMatchmakingEnabled": true,
                "hideUnreviewedAuthorComments": "2023-04-04T18:54:35.895Z",
                "gatherTownUserTrackingIsBroken": true,
                "postModerationWarningCommentId": "sLay9Tv65zeXaQzR4",
                "commentModerationWarningCommentId": "LbGNE5Ssnvs6MYnLu",
                "performanceMetricLoggingEnax5bled": true,
                "firstCommentAcknowledgeMessageCommentId": "QgwD7PkQHFp3nfhjj"
            }
        </script>
        <script>
            window.tabId = "AcTbbtfNj9Z8WQiJk"
        </script>
        <script>
            window.isReturningVisitor = false
        </script>
        <script async src="/js/bundle.js?hash=2ee05431b6f13a0bd9e89c1c42eb1787b61e249f881028abd1872ea3e7733cbb"></script>
        <script>
            (function() {
                const q = [];
                const types = ['click'];
                const cap = e => q.push(e);
                types.forEach(t => document.addEventListener(t, cap, true));
                window.__replayEvents = () => {
                    types.forEach(t => document.removeEventListener(t, cap, true));
                    q.forEach(e => {
                        const evt = new e.constructor(e.type,e);
                        e.target.dispatchEvent(evt)
                    }
                    )
                }
            }
            )()
        </script>
        <script>
            window.themeOptions = {
                "name": "default"
            }
        </script>
        <meta data-rh="true" charSet="utf-8"/>
        <meta data-rh="true" name="viewport" content="width=device-width, initial-scale=1"/>
        <title data-rh="true">How To Become A Mechanistic Interpretability Researcher — AI Alignment Forum</title>
        <meta data-rh="true" name="twitter:image:src" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"/>
        <meta data-rh="true" property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"/>
        <meta data-rh="true" http-equiv="Accept-CH" content="DPR, Viewport-Width, Width"/>
        <meta data-rh="true" property="og:title" content="How To Become A Mechanistic Interpretability Researcher — AI Alignment Forum"/>
        <meta data-rh="true" name="description" content="Note: If you’ll forgive the shameless self-promotion, applications for my MATS stream are open until Sept 12. I help people write a mech interp paper…"/>
        <meta data-rh="true" name="twitter:description" content="Note: If you’ll forgive the shameless self-promotion, applications for my MATS stream are open until Sept 12. I help people write a mech interp paper…"/>
        <meta data-rh="true" property="og:type" content="article"/>
        <meta data-rh="true" property="og:url" content="https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher"/>
        <meta data-rh="true" property="og:description" content="Note: If you’ll forgive the shameless self-promotion, applications for my MATS stream are open until Sept 12. I help people write a mech interp paper…"/>
        <meta data-rh="true" http-equiv="delegate-ch" content="sec-ch-dpr https://res.cloudinary.com;"/>
        <meta data-rh="true" name="citation_title" content="How To Become A Mechanistic Interpretability Researcher"/>
        <meta data-rh="true" name="citation_author" content="Neel Nanda"/>
        <link data-rh="true" rel="canonical" href="https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher"/>
        <link data-rh="true" rel="alternate" type="application/rss+xml" href="https://www.alignmentforum.org/feed.xml"/>
        <meta name="twitter:card" content="summary"/>
    </head>
    <script>
        _embedStyles("Layout", 0, ".Layout-main{grid-area:main;min-height:calc(100vh - 64px);margin-left:auto;padding-top:50px;margin-right:auto;padding-bottom:15px}@media (max-width:1279.95px){.Layout-main{padding-top:50px}}@media (max-width:959.95px){.Layout-main{padding-top:10px;padding-left:8px;padding-right:8px}}.Layout-wrapper{position:relative;overflow-x:clip}.Layout-mainNoFooter{padding-bottom:0}.Layout-mainFullscreen{height:100%;padding:0}@media (max-width:959.95px){.Layout-mainUnspacedGrid{padding-top:0;padding-left:0;padding-right:0}}.Layout-fullscreen{height:max(100vh,600px);display:flex;flex-direction:column}.Layout-fullscreenBodyWrapper{overflow:auto;flex-grow:1;flex-basis:0}@media (max-width:599.95px){.Layout-fullscreenBodyWrapper{overflow:visible}}@supports (grid-template-areas:\"title\"){.Layout-spacedGridActivated{display:grid;grid-template-areas:\"navSidebar ... main imageGap sunshine\";grid-template-columns:minmax(0,min-content) minmax(0,1fr) minmax(0,min-content) minmax(0,7fr) minmax(0,min-content)}}@media (max-width:1279.95px){.Layout-spacedGridActivated{display:block}}@supports (grid-template-areas:\"title\"){.Layout-unspacedGridActivated{display:grid;grid-template-areas:\"navSidebar main sunshine\";grid-template-columns:0 minmax(0,1fr) minmax(0,min-content)}}.Layout-unspacedGridActivated .Layout-main{width:100%;padding-top:0}@media (max-width:1279.95px){.Layout-unspacedGridActivated{display:block}}.Layout-eaHomeLayout{display:flex;align-items:start}@media (max-width:1279.95px){.Layout-eaHomeLayout{display:block}}.Layout-navSidebar{grid-area:navSidebar}.Layout-sunshine{grid-area:sunshine}.Layout-languageModelLauncher{top:-57px;right:-334px;position:absolute}@media (max-width:1919.95px){.Layout-languageModelLauncher{display:none}}.Layout-whiteBackground{background:#fff}html{color:#000;font-size:13px;box-sizing:border-box;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;--ck-z-modal:10000000002!important;--ck-z-panel:10000000002!important}*,::after,::before{box-sizing:inherit}body{margin:0;background:#f8f8f8}@media print{body{background:#fff}}h1,h2,h3,h4{font-weight:500}input,input:focus,textarea,textarea:focus{color:#000;border:0;outline:0}button{border:0;cursor:pointer;box-shadow:none}figure{margin:1em 0}.message.error{color:#e04e4b}.ais-InstantSearch__root{font-family:inherit!important}.noscript-warning{padding:20px;font-size:17px}a,a:active,a:hover{text-decoration:none}a{color:inherit;cursor:pointer}a:active,a:hover{opacity:.5}.grecaptcha-badge,figure:after{visibility:hidden}.reCaptcha-text{color:rgba(0,0,0,.5);font-size:11px;font-family:sans-serif}.thoughtSaverFrame{width:100%;border:0;height:500px;border-radius:5px}.ck-mentions-balloon{--ck-color-text:rgba(0,0,0,1);--ck-color-panel-border:#c4c4c4;--ck-color-list-background:#fff;--ck-color-panel-background:#fff;--ck-color-list-button-on-background:#198cf0;--ck-color-list-button-hover-background:#e6e6e6;--ck-color-list-button-on-background-focus:#0e7fe1}.ck-comment-marker{border-top:none!important;border-bottom:none!important}.ck-powered-by,.ck-powered-by-balloon{display:none!important}.comments-node-even{background-color:#f2f2f2}.comments-node-odd{background-color:#fcfcfc}.comments-node-root{background:#fff;margin-bottom:17px;backdrop-filter:none}@media screen and (max-width:715px){.comments-node-root{margin-bottom:10px}}@media screen and (max-width:400px){.comments-node-root{padding-top:5px;margin-bottom:8px}.comments-node .comments-node{margin-left:5px;margin-bottom:5px}}.comments-edit-form{position:relative;padding-bottom:12px}.comments-edit-form .form-submit{text-align:right;margin-right:10px}.comments-load-more{margin-left:10px}.comments-node-its-getting-nested-here{margin-left:7px!important;margin-bottom:7px!important}.comments-node-so-take-off-all-your-margins{margin-left:6px!important;margin-bottom:6px!important}.comments-node-im-getting-so-nested,.comments-node-im-gonna-drop-my-margins{margin-left:5px!important;margin-bottom:5px!important}.comments-node-what-are-you-even-arguing-about{margin-left:4px!important;margin-bottom:4px!important}.comments-node-are-you-sure-this-is-a-good-idea{margin-left:3px!important;margin-bottom:3px!important}.comments-node-seriously-what-the-fuck{transform:rotate(.5deg);margin-left:2px!important;margin-bottom:2px!important}.comments-node-are-you-curi-and-lumifer-specifically{transform:rotate(1deg);margin-left:1px!important;margin-bottom:1px!important}.comments-node-cuz-i-guess-that-makes-sense-but-like-really-tho{transform:rotate(-1deg);margin-left:1px!important;margin-bottom:1px!important}.recent-comments-node.loading{padding:35px;min-height:80px;background-color:#f2f2f2}.editor blockquote div,.editor blockquote span,.recent-comments-node .comments-node{margin:0}.recent-comments-node.comments-node-root{position:inherit;margin-bottom:.8em;background-color:none}.dialogue-message-input-wrapper{display:flex;margin-top:12px;flex-direction:column}.dialogue-message-input{order:2;border:2px solid!important;margin:12px 0;padding:26px 16px 40px;position:relative;border-radius:3px}.dialogue-message-input[user-order=\"1\"] .dialogue-message-input-header{color:#1192e8!important}.dialogue-message-input[user-order=\"2\"] .dialogue-message-input-header{color:#198038!important}.dialogue-message-input[user-order=\"3\"] .dialogue-message-input-header{color:#b28600!important}.dialogue-message-input[user-order=\"4\"] .dialogue-message-input-header{color:#9f1853!important}.dialogue-message-input[user-order=\"5\"] .dialogue-message-input-header{color:#a56eff!important}.dialogue-message-input[user-order=\"6\"] .dialogue-message-input-header{color:#6c7bff!important}.dialogue-message-input-header{top:-14px;padding:4px;position:absolute;background-color:#fff}.dialogue-message-input button{right:16px;bottom:12px;display:block;padding:0;position:absolute;min-height:unset;margin-left:auto;margin-right:-8px;margin-bottom:-4px}.dialogue-message{padding:22px 8px 8px 0;position:relative;margin-top:6px}.dialogue-message[user-order=\"1\"] .dialogue-message-header{color:#1192e8!important}.dialogue-message[user-order=\"2\"] .dialogue-message-header{color:#198038!important}.dialogue-message[user-order=\"3\"] .dialogue-message-header{color:#b28600!important}.dialogue-message[user-order=\"4\"] .dialogue-message-header{color:#9f1853!important}.dialogue-message[user-order=\"5\"] .dialogue-message-header{color:#a56eff!important}.dialogue-message[user-order=\"6\"] .dialogue-message-header{color:#6c7bff!important}.dialogue-message p,.dialogue-message-input p{margin-bottom:0!important}.dialogue-message-header{top:0;position:absolute}.dialogue-message-header b{font-weight:600}body.t3a-sticky-player-visible .StickyDigestAd-root{bottom:78px}.t3a-heading-play-button{display:none}@media screen and (min-width:850px){.t3a-heading-play-button{top:0;left:0;color:#fff;width:1.5rem;border:0;cursor:pointer;height:1.5rem;display:block;outline:0;z-index:10;position:absolute;transform:translate(0,0);margin-left:-34px;margin-right:10px;border-radius:9999px;background-color:#ddd}.t3a-heading-play-button:hover{background-color:#bdbdbd}.t3a-heading-play-button:focus{outline:0}.t3a-heading-play-icon{display:flex;align-items:center;margin-right:-1px;justify-content:center}h1 .t3a-heading-play-button{margin-top:10px}h2 .t3a-heading-play-button{margin-top:7px}h3 .t3a-heading-play-button{margin-top:3px}}.editor{min-height:220px;border-radius:2px;margin-bottom:2em}figure:after{clear:both;height:0;content:Foo}.draft-image,figure:after{display:block}.draft-image.center{margin-left:auto;margin-right:auto}.draft-image.right{float:right}.form-component-CommentEditor,.form-component-EditorFormComponent{position:static!important}.dividerBlock{width:100%;border:0;height:100%;margin:32px 0;display:flex;text-align:center;align-items:center;justify-content:center}.dividerBlock::after{color:rgba(0,0,0,.26);content:•••;font-size:1rem;margin-left:12px;letter-spacing:12px}.form-input{margin:16px 0;position:relative}.form-component-EditTitle{margin:0;flex-grow:1}.form-cancel{margin-left:25px}.form-component-MuiInput{margin-bottom:0}.form-component-AlignmentCheckbox{position:relative}@media screen and (max-width:715px){.primary-form-submit-button{float:left}}h3,style~p{margin-top:0}.ais-Breadcrumb-list,.ais-CurrentRefinements-list,.ais-HierarchicalMenu-list,.ais-Hits-list,.ais-InfiniteHits-list,.ais-InfiniteResults-list,.ais-Menu-list,.ais-NumericMenu-list,.ais-Pagination-list,.ais-RatingMenu-list,.ais-RefinementList-list,.ais-Results-list,.ais-ToggleRefinement-list{margin:0;padding:0;list-style:none}.ais-ClearRefinements-button,.ais-CurrentRefinements-delete,.ais-CurrentRefinements-reset,.ais-HierarchicalMenu-showMore,.ais-InfiniteHits-loadMore,.ais-InfiniteResults-loadMore,.ais-Menu-showMore,.ais-RangeInput-submit,.ais-RefinementList-showMore,.ais-SearchBox-reset,.ais-SearchBox-submit{font:inherit;color:inherit;border:0;cursor:pointer;padding:0;overflow:visible;background:0 0;line-height:normal;user-select:none;-ms-user-select:none;-moz-user-select:none;-webkit-user-select:none}.ais-ClearRefinements-button::-moz-focus-inner,.ais-CurrentRefinements-delete::-moz-focus-inner,.ais-CurrentRefinements-reset::-moz-focus-inner,.ais-HierarchicalMenu-showMore::-moz-focus-inner,.ais-InfiniteHits-loadMore::-moz-focus-inner,.ais-InfiniteResults-loadMore::-moz-focus-inner,.ais-Menu-showMore::-moz-focus-inner,.ais-RangeInput-submit::-moz-focus-inner,.ais-RefinementList-showMore::-moz-focus-inner,.ais-SearchBox-reset::-moz-focus-inner,.ais-SearchBox-submit::-moz-focus-inner{border:0;padding:0}.ais-ClearRefinements-button[disabled],.ais-CurrentRefinements-delete[disabled],.ais-CurrentRefinements-reset[disabled],.ais-HierarchicalMenu-showMore[disabled],.ais-InfiniteHits-loadMore[disabled],.ais-InfiniteResults-loadMore[disabled],.ais-Menu-showMore[disabled],.ais-RangeInput-submit[disabled],.ais-RefinementList-showMore[disabled],.ais-SearchBox-reset[disabled],.ais-SearchBox-submit[disabled]{cursor:default}.ais-Breadcrumb-item,.ais-Breadcrumb-list,.ais-Pagination-list,.ais-PoweredBy,.ais-RangeInput-form,.ais-RatingMenu-link{display:flex;align-items:center;-ms-flex-align:center;-webkit-box-align:center}.ais-HierarchicalMenu-list .ais-HierarchicalMenu-list{margin-left:1em}.ais-PoweredBy-logo{width:70px;height:auto;display:block}.ais-RatingMenu-starIcon{width:20px;height:20px;display:block}.ais-SearchBox-input::-ms-clear,.ais-SearchBox-input::-ms-reveal{width:0;height:0;display:none}.ais-SearchBox-input::-webkit-search-cancel-button,.ais-SearchBox-input::-webkit-search-decoration,.ais-SearchBox-input::-webkit-search-results-button,.ais-SearchBox-input::-webkit-search-results-decoration{display:none}.ais-RangeSlider .rheostat{overflow:visible;margin-top:40px;margin-bottom:40px}.ais-RangeSlider .rheostat-background{top:0;width:100%;border:1px solid #aaa;height:6px;position:relative;background-color:#fff}.ais-RangeSlider .rheostat-handle{top:-7px;margin-left:-12px}.ais-RangeSlider .rheostat-progress{top:1px;height:4px;position:absolute;background-color:#303030}.rheostat-handle{width:20px;border:1px solid #303030;cursor:grab;height:20px;z-index:1;position:relative;border-radius:50%;background-color:#fff}.rheostat-marker{width:1px;height:5px;position:absolute;margin-left:-1px;background-color:#aaa}.rheostat-marker--large{height:9px}.rheostat-value{padding-top:15px}.rheostat-tooltip,.rheostat-value{position:absolute;transform:translateX(-50%);text-align:center;margin-left:50%;-webkit-transform:translateX(-50%)}.rheostat-tooltip{top:-22px}p{margin-top:1em;margin-bottom:1em}p:first-child{margin-top:0}p:last-child{margin-bottom:0}.mapboxgl-popup{z-index:6;will-change:auto!important}@font-face{font-family:GreekFallback;src:local(\"Arial\");unicode-range:U+0370-03FF,U+1F00-1FFF}@font-face{font-family:ETBookRoman;src:url(https://res.cloudinary.com/lesswrong-2-0/raw/upload/v1723063815/et-book-roman-line-figures_tvofzs.woff)format(\"woff\")}.ck-table-properties-form__alignment-row{display:none!important}.Layout-searchResultsArea{top:0;width:100%;z-index:1100;position:absolute}@media (max-width:959.95px){.Layout-hideHomepageMapOnMobile{display:none}}.Layout-stickyWrapper{transform:translateY(20px);transition:transform 200ms ease-in-out;margin-bottom:20px}.Layout-stickyWrapperHeaderVisible{transform:translateY(84px)}");
        _embedStyles("Header", 0, ".Header-appBar,.Header-appBar.Header-blackBackgroundAppBar{box-shadow:0 1px 1px rgba(0,0,0,.05),0 1px 1px rgba(0,0,0,.05)}.Header-appBar{color:#000;width:100%;display:flex;z-index:1100;position:static;box-sizing:border-box;flex-shrink:0;flex-direction:column;backdrop-filter:blur(4px);background-color:#fff}.Header-appBar.Header-blackBackgroundAppBar{background:#fff}.Header-appBarDarkBackground{box-shadow:none;--header-text-color:#fff;--header-contrast-color:#000}.Header-appBarDarkBackground,.Header-appBarDarkBackground .Header-titleLink,.Header-appBarDarkBackground .HeaderSubtitle-subtitle,.Header-appBarDarkBackground .KarmaChangeNotifier-gainedPoints,.Header-appBarDarkBackground .KarmaChangeNotifier-starIcon,.Header-appBarDarkBackground .MessagesMenuButton-buttonClosed,.Header-appBarDarkBackground .NotificationsMenuButton-badge,.Header-appBarDarkBackground .NotificationsMenuButton-buttonClosed,.Header-appBarDarkBackground .SearchBar-searchIcon,.Header-appBarDarkBackground .UsersMenu-arrowIcon,.Header-appBarDarkBackground .ais-SearchBox-input{color:var(--header-text-color)}.Header-appBarDarkBackground .ais-SearchBox-input::placeholder{color:var(--header-text-color)}.Header-appBarDarkBackground .EAButton-variantContained{color:var(--header-contrast-color);background-color:var(--header-text-color)}.Header-appBarDarkBackground .EAButton-greyContained{color:var(--header-text-color);background-color:color-mix(in oklab,var(--header-text-color) 15%,var(--header-contrast-color))}.Header-appBarDarkBackground .EAButton-greyContained:hover{background-color:color-mix(in oklab,var(--header-text-color) 10%,transparent)!important}.Header-appBarDarkBackground .EAButton-variantContained:hover{background-color:color-mix(in oklab,var(--header-text-color) 90%,var(--header-contrast-color))}.Header-root{height:64px}@media (max-width:599.95px){.Header-root{height:56px}}@media print{.Header-root{display:none}}.Header-titleSubtitleContainer{display:flex;align-items:center}.Header-titleFundraiserContainer{display:flex;align-items:center;flex-direction:row}.Header-title{top:3px;flex:1;color:rgba(0,0,0,.54);position:relative;padding-right:8px}.Header-titleLink{top:0;color:#000;height:19px;display:flex;font-size:19px;align-items:center}.Header-titleLink:active,.Header-titleLink:hover{opacity:.7;text-decoration:none}.Header-menuButton{margin-left:-8px;margin-right:8px}.Header-icon{width:24px;height:24px}.Header-siteLogo{margin-left:-7px;margin-right:6px}@media (max-width:959.95px){.Header-siteLogo{margin-left:-12px;margin-right:3px}}@media (min-width:1280px){.Header-hideLgUp{display:none!important}}@media (max-width:1279.95px){.Header-hideMdDown{display:none!important}}@media (max-width:959.95px){.Header-hideSmDown{display:none!important}}@media (max-width:599.95px){.Header-hideXsDown{display:none!important}}@media (min-width:960px){.Header-hideMdUp{display:none!important}}.Header-rightHeaderItems{display:flex;margin-left:auto;margin-right:-8px}.Header-searchSSRStandin{min-width:48px}.Header-headroom .headroom{top:unset;left:0;right:0;z-index:1300}.Header-headroom .headroom--unfixed{position:relative;transform:translateY(0)}.Header-headroom .headroom--scrolled{transition:transform 200ms ease-in-out}.Header-headroom .headroom--unpinned{position:fixed;transform:translateY(-100%)}.Header-headroom .headroom--pinned{position:fixed;transform:translateY(0)}.Header-headroomPinnedOpen .headroom--unpinned{transform:none!important}.Header-headroomPinnedOpen .headroom--unfixed{position:fixed!important}.Header-lightconeFundraiserHeaderItem,.Header-lightconeFundraiserHeaderItemSmall{color:#b3884f;font-size:1.4rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-left:8px}.Header-lightconeFundraiserHeaderItemSmall{font-weight:600;margin-bottom:1.5px}");
        _embedStyles("MuiToolbar", -10, ".MuiToolbar-root{display:flex;position:relative;align-items:center}.MuiToolbar-gutters{padding-left:16px;padding-right:16px}@media (min-width:600px){.MuiToolbar-gutters{padding-left:24px;padding-right:24px}}.MuiToolbar-regular{min-height:56px}@media (min-width:0px) and (orientation:landscape){.MuiToolbar-regular{min-height:48px}}@media (min-width:600px){.MuiToolbar-regular{min-height:64px}}.MuiToolbar-dense{min-height:48px}");
        _embedStyles("MuiIconButton", -10, ".MuiIconButton-root{flex:0 0 auto;color:rgba(0,0,0,.54);padding:12px;overflow:visible;font-size:1.5rem;text-align:center;transition:background-color 150ms cubic-bezier(.4,0,.2,1) 0ms;border-radius:50%}.MuiIconButton-root:hover{background-color:rgba(0,0,0,.08)}.MuiIconButton-root.MuiIconButton-disabled{color:rgba(0,0,0,.26)}@media (hover:none){.MuiIconButton-root:hover{background-color:transparent}}.MuiIconButton-root:hover.MuiIconButton-disabled{background-color:transparent}.MuiIconButton-colorInherit{color:inherit}.MuiIconButton-colorPrimary,.MuiIconButton-colorSecondary{color:#3f51b5}.MuiIconButton-colorPrimary:hover{background-color:rgba(63,81,181,.08)}@media (hover:none){.MuiIconButton-colorPrimary:hover{background-color:transparent}}.MuiIconButton-colorSecondary:hover{background-color:rgba(63,81,181,.08)}@media (hover:none){.MuiIconButton-colorSecondary:hover{background-color:transparent}}.MuiIconButton-label{width:100%;display:flex;align-items:inherit;justify-content:inherit}");
        _embedStyles("MuiButtonBase", -11, ".MuiButtonBase-root{color:inherit;border:0;cursor:pointer;margin:0;display:inline-flex;outline:0;padding:0;position:relative;align-items:center;user-select:none;border-radius:0;vertical-align:middle;-moz-appearance:none;justify-content:center;text-decoration:none;background-color:transparent;-webkit-appearance:none;-webkit-tap-highlight-color:transparent}.MuiButtonBase-root::-moz-focus-inner{border-style:none}.MuiButtonBase-root.MuiButtonBase-disabled{cursor:default;pointer-events:none}");
        _embedStyles("ForumIcon", -2, ".ForumIcon-root{width:var(--icon-size, 1em);height:var(--icon-size, 1em);display:inline-block;font-size:var(--icon-size, 24px);flex-shrink:0;user-select:none}.ForumIcon-linkRotation{transform:rotate(-45deg)}.ForumIcon-linkRotation.MuiListItemIcon-root{margin-right:12px}");
        _embedStyles("MuiSvgIcon", -10, ".MuiSvgIcon-root{fill:currentColor;width:1em;height:1em;display:inline-block;font-size:24px;transition:fill 200ms cubic-bezier(.4,0,.2,1) 0ms;flex-shrink:0;user-select:none}");
        _embedStyles("MuiTouchRipple", -10, "@keyframes mui-ripple-enter{0%{opacity:.1;transform:scale(0)}to{opacity:.3;transform:scale(1)}}@keyframes mui-ripple-exit{0%{opacity:1}to{opacity:0}}@keyframes mui-ripple-pulsate{0%,to{transform:scale(1)}50%{transform:scale(.92)}}.MuiTouchRipple-root{top:0;left:0;width:100%;height:100%;display:block;z-index:0;overflow:hidden;position:absolute;border-radius:inherit;pointer-events:none!important}.MuiTouchRipple-ripple{top:0;left:0;width:50px;height:50px;opacity:0;position:absolute}.MuiTouchRipple-rippleVisible{opacity:.3;animation:mui-ripple-enter 550ms cubic-bezier(.4,0,.2,1);transform:scale(1)}.MuiTouchRipple-ripplePulsate{animation-duration:200ms}.MuiTouchRipple-child{width:100%;height:100%;display:block;opacity:1;border-radius:50%;background-color:currentColor}.MuiTouchRipple-childLeaving{opacity:0;animation:mui-ripple-exit 550ms cubic-bezier(.4,0,.2,1)}.MuiTouchRipple-childPulsate{top:0;left:0;position:absolute;animation:mui-ripple-pulsate 2500ms cubic-bezier(.4,0,.2,1) 200ms infinite}");
        _embedStyles("Typography", -2, ".Typography-root{margin:0;display:block}.Typography-display4{font-size:7rem;font-weight:300;line-height:1.14286em;margin-left:-.04em;letter-spacing:-.04em}.Typography-display1,.Typography-display2,.Typography-display3,.Typography-display4{color:#424242;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif}.Typography-display3{font-size:39px;margin-top:1.2em;font-weight:500;line-height:$1.30357em;margin-left:-.02em;letter-spacing:-.02em}.Typography-display1,.Typography-display2{margin-top:1em}.Typography-display2{font-size:36.4px;font-weight:500;line-height:1.13333em;margin-left:-.02em}.Typography-display1{font-size:26px;font-weight:400;line-height:1.20588em}.Typography-headline{color:rgba(0,0,0,.87);font-size:1.5rem;font-weight:400;line-height:1.35417em}.Typography-body1,.Typography-body2,.Typography-headline,.Typography-subheading,.Typography-title{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif}.Typography-title{color:rgba(0,0,0,.87);font-size:18px;font-weight:500;line-height:1.16667em;margin-bottom:3px}.Typography-body1,.Typography-body2,.Typography-subheading{font-weight:400}.Typography-subheading{color:#757575;font-size:15px;line-height:1.5em}.Typography-body1,.Typography-body2{color:rgba(0,0,0,.87);font-size:14.3px;line-height:19.5px}.Typography-body1{font-size:18.2px;line-height:26px}.Typography-gutterBottom{margin-bottom:.35em}");
        _embedStyles("HeaderSubtitle", 0, ".HeaderSubtitle-subtitle{color:rgba(0,0,0,.87);border-left:1px solid #bdbdbd;margin-left:1em;padding-left:1em;text-transform:uppercase}");
        _embedStyles("HeaderEventSubtitle", 0, ".HeaderEventSubtitle-root{margin-left:1em;background-clip:text!important;-webkit-background-clip:text!important;-webkit-text-fill-color:transparent}.HeaderEventSubtitle-root:hover{opacity:.8}");
        _embedStyles("SearchBar", 0, ".SearchBar-root{display:flex;align-items:center}.SearchBar-rootChild{height:fit-content}.SearchBar-searchInputArea{height:48px;display:block;position:relative;min-width:48px}.SearchBar-searchInputArea .ais-SearchBox{width:100%;height:46px;display:inline-block;position:relative;font-size:14px;max-width:300px;box-sizing:border-box;white-space:nowrap}.SearchBar-searchInputArea .ais-SearchBox-form{height:100%}.SearchBar-searchInputArea .ais-SearchBox-submit{display:none}.SearchBar-searchInputArea .ais-SearchBox-input{width:100%;cursor:text;height:100%;display:none;font-size:inherit;box-shadow:none;border-style:none;padding-left:48px;border-radius:5px;padding-right:0;vertical-align:bottom;background-color:transparent;-webkit-appearance:none}.SearchBar-searchInputArea.open .ais-SearchBox-input{display:inline-block}.SearchBar-searchInputArea.open .SearchBar-searchIconButton{position:fixed}.SearchBar-searchIcon{--icon-size:24px}.SearchBar-searchIconButton{color:rgba(0,0,0,.87)}.SearchBar-closeSearchIcon{font-size:14px}.SearchBar-searchBarClose{top:15px;right:5px;cursor:pointer;display:inline-block;position:absolute}.SearchBar-alignmentForum .ais-SearchBox-input{color:#fff}.SearchBar-alignmentForum .ais-SearchBox-input::placeholder{color:rgba(255,255,255,.5)}");
        _embedStyles("UsersAccountMenu", 0, ".UsersAccountMenu-root{margin-top:5px}.UsersAccountMenu-userButton{color:rgba(0,0,0,.87);opacity:.8;font-size:14px;font-weight:400}.UsersAccountMenu-login{margin-left:12px;margin-right:8px}.UsersAccountMenu-signUp{display:inline-block;margin-right:8px}@media (max-width:540px){.UsersAccountMenu-signUp{display:none}}");
        _embedStyles("MuiButton", -10, ".MuiButton-root{color:rgba(0,0,0,.87);padding:8px 16px;font-size:.875rem;min-width:64px;box-sizing:border-box;min-height:36px;transition:background-color 250ms cubic-bezier(.4,0,.2,1) 0ms,box-shadow 250ms cubic-bezier(.4,0,.2,1) 0ms,border 250ms cubic-bezier(.4,0,.2,1) 0ms;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:500;line-height:1.4em;border-radius:4px;text-transform:uppercase}.MuiButton-root:hover{text-decoration:none;background-color:rgba(0,0,0,.08)}.MuiButton-root.MuiButton-disabled{color:rgba(0,0,0,.26)}@media (hover:none){.MuiButton-root:hover{background-color:transparent}}.MuiButton-root:hover.MuiButton-disabled{background-color:transparent}.MuiButton-label{width:100%;display:inherit;align-items:inherit;justify-content:inherit}.MuiButton-textPrimary,.MuiButton-textSecondary{color:#3f51b5}.MuiButton-textPrimary:hover{background-color:rgba(63,81,181,.08)}@media (hover:none){.MuiButton-textPrimary:hover{background-color:transparent}}.MuiButton-textSecondary:hover{background-color:rgba(63,81,181,.08)}@media (hover:none){.MuiButton-textSecondary:hover{background-color:transparent}}.MuiButton-outlined{border:1px solid rgba(0,0,0,.23)}.MuiButton-outlinedPrimary,.MuiButton-outlinedSecondary{border:1px solid rgba(63,81,181,.5)}.MuiButton-outlinedPrimary:hover{border:1px solid #3f51b5}.MuiButton-outlinedPrimary.MuiButton-disabled{border:1px solid rgba(0,0,0,.26)}.MuiButton-outlinedSecondary:hover{border:1px solid #3f51b5}.MuiButton-outlinedSecondary.MuiButton-disabled{border:1px solid rgba(0,0,0,.26)}.MuiButton-contained{color:rgba(0,0,0,.87);box-shadow:0 1px 5px 0 rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 3px 1px -2px rgba(0,0,0,.12);background-color:#e0e0e0}.MuiButton-contained.MuiButton-focusVisible{box-shadow:0 3px 5px -1px rgba(0,0,0,.2),0 6px 10px 0 rgba(0,0,0,.14),0 1px 18px 0 rgba(0,0,0,.12)}.MuiButton-contained:active{box-shadow:0 5px 5px -3px rgba(0,0,0,.2),0 8px 10px 1px rgba(0,0,0,.14),0 3px 14px 2px rgba(0,0,0,.12)}.MuiButton-contained.MuiButton-disabled{color:rgba(0,0,0,.26);box-shadow:none;background-color:rgba(0,0,0,.12)}.MuiButton-contained:hover{background-color:#d5d5d5}@media (hover:none){.MuiButton-contained:hover{background-color:#e0e0e0}}.MuiButton-contained:hover.MuiButton-disabled{background-color:rgba(0,0,0,.12)}.MuiButton-containedPrimary,.MuiButton-containedSecondary{color:#fff;background-color:#3f51b5}.MuiButton-containedPrimary:hover{background-color:#303f9f}@media (hover:none){.MuiButton-containedPrimary:hover{background-color:#3f51b5}}.MuiButton-containedSecondary:hover{background-color:#303f9f}@media (hover:none){.MuiButton-containedSecondary:hover{background-color:#3f51b5}}.MuiButton-fab{width:56px;height:56px;padding:0;min-width:0;box-shadow:0 3px 5px -1px rgba(0,0,0,.2),0 6px 10px 0 rgba(0,0,0,.14),0 1px 18px 0 rgba(0,0,0,.12);border-radius:50%}.MuiButton-fab:active{box-shadow:0 7px 8px -4px rgba(0,0,0,.2),0 12px 17px 2px rgba(0,0,0,.14),0 5px 22px 4px rgba(0,0,0,.12)}.MuiButton-extendedFab{width:auto;height:48px;padding:0 16px;min-width:48px;border-radius:24px}.MuiButton-colorInherit{color:inherit}.MuiButton-mini{width:40px;height:40px}.MuiButton-sizeSmall{padding:7px 8px;font-size:.8125rem;min-width:64px;min-height:32px}.MuiButton-sizeLarge{padding:8px 24px;font-size:.9375rem;min-width:112px;min-height:40px}.MuiButton-fullWidth{width:100%}");
        _embedStyles("LWPopper", 0, ".LWPopper-popper{z-index:10000;position:absolute}.LWPopper-default,.LWPopper-tooltip{z-index:10001;position:relative}.LWPopper-tooltip{color:#fff;padding:8px;font-size:1rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;border-radius:3px;background-color:rgba(75,75,75,.94);font-variant-numeric:lining-nums}.LWPopper-noMouseEvents{pointer-events:none}@media (pointer:coarse){.LWPopper-hideOnTouchScreens{display:none}}");
        _embedStyles("NavigationDrawer", 0, ".NavigationDrawer-paperWithoutToC{width:280px;overflow-y:auto}.NavigationDrawer-paperWithToC{width:280px;overflow:hidden}@media (max-width:959.95px){.NavigationDrawer-paperWithToC{width:300px}}.NavigationDrawer-drawerNavigationMenuUncompressed{left:0;width:260px;padding-bottom:20px}@media (max-width:959.95px){.NavigationDrawer-hideOnMobile{display:none}}.NavigationDrawer-drawerNavigationMenuCompressed{color:#757575;width:55px;height:100%;padding-top:16px;border-right:1px solid rgba(0,0,0,.1);padding-bottom:8px;background-color:#f5f5f5}@media (min-width:960px){.NavigationDrawer-drawerNavigationMenuCompressed{display:none}}.NavigationDrawer-tableOfContents{left:55px;height:100%;padding:16px 0 16px 16px;position:absolute;max-width:247px;overflow-y:auto}@media (min-width:960px){.NavigationDrawer-tableOfContents{display:none}}");
        _embedStyles("Drawer", 0, ".Drawer-containerLeft{top:0;left:0;width:0;bottom:0;position:absolute}.Drawer-containerRight{top:0;right:0;width:0;bottom:0;position:absolute}.Drawer-docked{flex:0 0 auto}.Drawer-container{top:0;height:100%;z-index:1400;position:fixed}.Drawer-paper{flex:1 0 auto;height:100%;display:flex;outline:0;position:absolute;overflow-y:auto;transition:transform .3s ease-out;flex-direction:column;-webkit-overflow-scrolling:touch}.Drawer-anchorLeft{right:100%;transform:translateX(0)}.Drawer-anchorRight{left:100%;transform:translateX(0)}.Drawer-open{margin-left:0!important;margin-right:0!important}.Drawer-paperAnchorDockedLeft{border-right:1px solid rgba(0,0,0,.12)}.Drawer-paperAnchorDockedRight{border-left:1px solid rgba(0,0,0,.12)}");
        _embedStyles("FlashMessages", 0, ".FlashMessages-paper{display:flex;padding:6px 24px;flex-wrap:wrap;align-items:center;background-color:#fff}@media (min-width:960px){.FlashMessages-paper{max-width:568px;min-width:288px;border-radius:4px}}@media (max-width:959.95px){.FlashMessages-paper{flex-grow:1}}.FlashMessages-message{color:#000;padding:8px 0}.FlashMessages-action{display:flex;align-items:center;margin-left:auto;margin-right:-8px;padding-left:24px}");
        _embedStyles("OnboardingFlow", 0, ".OnboardingFlow-root{padding:0}");
        _embedStyles("Footer", 0, ".Footer-root{height:150px}");
        _embedStyles("DelayedLoading", 0, "@keyframes sk-bouncedelay{0%,80%,to{transform:scale(0)}40%{transform:scale(1)}}.DelayedLoading-spinner{height:10px;display:block;max-width:100px;text-align:center;margin-left:auto;margin-right:auto}.DelayedLoading-spinner div{width:10px;height:10px;display:inline-block;animation:sk-bouncedelay 1.4s infinite ease-in-out both;border-radius:100%;background-color:rgba(0,0,0,.55);-webkit-animation:sk-bouncedelay 1.4s infinite ease-in-out both}.DelayedLoading-whiteSpinner div{background-color:#fff}.DelayedLoading-bounce1{margin-right:5px;animation-delay:.68s!important}.DelayedLoading-bounce2{margin-right:5px;animation-delay:.84s!important}.DelayedLoading-bounce3{margin-right:0;animation-delay:1s!important}")
    </script>
    <body>
        <div id="react-app">
            <!--$?-->
            <template id="B:0"></template>
            <!--/$-->
            <div style="color-scheme:only light">
                <div class="wrapper alignment-forum Layout-wrapper Layout-whiteBackground" id="wrapper">
                    <noscript class="noscript-warning">This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. </noscript>
                    <noscript>
                        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRC765W" height="0" width="0" style="display:none;visibility:hidden"></iframe>
                    </noscript>
                    <!--$-->
                    <div class="Header-root">
                        <div style="height:64px" class="Header-headroom headroom-wrapper">
                            <div class="headroom headroom--unfixed">
                                <header class="Header-appBar Header-blackBackgroundAppBar">
                                    <div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters">
                                        <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu">
                                            <span class="MuiIconButton-label">
                                                <svg class="MuiSvgIcon-root Header-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation">
                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                    <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path>
                                                </svg>
                                            </span>
                                            <span class="MuiTouchRipple-root"></span>
                                        </button>
                                        <h2 class="Typography-root Typography-title Header-title">
                                            <div class="Header-hideSmDown">
                                                <div class="Header-titleSubtitleContainer">
                                                    <div class="Header-titleFundraiserContainer">
                                                        <a class="Header-titleLink" href="/">AI ALIGNMENT FORUM</a>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="Header-hideMdUp Header-titleFundraiserContainer">
                                                <a class="Header-titleLink" href="/">AF</a>
                                            </div>
                                        </h2>
                                        <div class="Header-rightHeaderItems">
                                            <div class="SearchBar-root">
                                                <div class="SearchBar-rootChild">
                                                    <div class="SearchBar-searchInputArea SearchBar-alignmentForum SearchBar-searchInputAreaSmall">
                                                        <div>
                                                            <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SearchBar-searchIconButton SearchBar-searchIconButtonSmall" type="button">
                                                                <span class="MuiIconButton-label">
                                                                    <svg class="MuiSvgIcon-root SearchBar-searchIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                        <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path>
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    </svg>
                                                                </span>
                                                                <span class="MuiTouchRipple-root"></span>
                                                            </button>
                                                        </div>
                                                        <div></div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="UsersAccountMenu-root">
                                                <button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat" type="button">
                                                    <span class="MuiButton-label">
                                                        <span class="UsersAccountMenu-userButton">Login</span>
                                                    </span>
                                                    <span class="MuiTouchRipple-root"></span>
                                                </button>
                                            </div>
                                        </div>
                                    </div>
                                </header>
                            </div>
                        </div>
                    </div>
                    <!--/$-->
                    <!--$-->
                    <!--/$-->
                    <div class="">
                        <div class="Layout-searchResultsArea"></div>
                        <div class="Layout-main">
                            <!--$?-->
                            <template id="B:1"></template>
                            <div class="DelayedLoading-spinner">
                                <div class="DelayedLoading-bounce1"></div>
                                <div class="DelayedLoading-bounce2"></div>
                                <div class="DelayedLoading-bounce3"></div>
                            </div>
                            <!--/$-->
                            <!--$-->
                            <!--/$-->
                            <div class="Footer-root"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div hidden id="S:0"></div>
        <script>
            $RC = function(b, c, e) {
                c = document.getElementById(c);
                c.parentNode.removeChild(c);
                var a = document.getElementById(b);
                if (a) {
                    b = a.previousSibling;
                    if (e)
                        b.data = "$!",
                        a.setAttribute("data-dgst", e);
                    else {
                        e = b.parentNode;
                        a = b.nextSibling;
                        var f = 0;
                        do {
                            if (a && 8 === a.nodeType) {
                                var d = a.data;
                                if ("/$" === d)
                                    if (0 === f)
                                        break;
                                    else
                                        f--;
                                else
                                    "$" !== d && "$?" !== d && "$!" !== d || f++
                            }
                            d = a.nextSibling;
                            e.removeChild(a);
                            a = d
                        } while (a);
                        for (; c.firstChild; )
                            e.insertBefore(c.firstChild, a);
                        b.data = "$"
                    }
                    b._reactRetry && b._reactRetry()
                }
            }
            ;
            $RC("B:0", "S:0")
        </script>
        <script>
            _embedStyles("PostsPage", 0, ".PostsPage-title{margin-bottom:32px}@media (max-width:959.95px){.PostsPage-title{margin-bottom:20px}.PostsPage-titleWithMarket{margin-bottom:35px}}.PostsPage-centralColumn{max-width:682px;margin-left:auto;margin-right:auto}.PostsPage-audioPlayerHidden .t3a-heading-play-button{display:none!important}.PostsPage-betweenPostAndComments{min-height:24px}.PostsPage-recommendations{margin:0 auto 40px;max-width:720px}.PostsPage-commentsSection{position:relative;background:#fff;min-height:calc(70vh - 100px)}@media (max-width:959.95px){.PostsPage-commentsSection{margin-left:0;padding-right:0}}.PostsPage-noCommentsPlaceholder{color:#757575;font-size:14px;margin-top:60px;text-align:center;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:500;line-height:1.6em}.PostsPage-headerImageContainer{padding-bottom:15px}@media (min-width:960px){.PostsPage-headerImageContainer{margin-top:-50px}}@media (max-width:959.95px){.PostsPage-headerImageContainer{margin-top:-12px;margin-left:-8px;margin-right:-8px}}@media (max-width:599.95px){.PostsPage-headerImageContainer{margin-top:-10px}}@media (min-width:960px){.PostsPage-headerImageContainerWithComment{margin-top:10px}}@media (max-width:959.95px){.PostsPage-headerImageContainerWithComment{margin-top:10px}}@media (max-width:599.95px){.PostsPage-headerImageContainerWithComment{margin-top:10px}}.PostsPage-headerImage{width:100vw;max-width:682px}.PostsPage-embeddedPlayer{margin-bottom:30px}.PostsPage-hideEmbeddedPlayer{display:none}.PostsPage-welcomeBox{max-width:220px;margin-top:110px}@media (max-width:1279.95px){.PostsPage-welcomeBox{display:none}}.PostsPage-bottomOfPostSubscribe{border:1px solid rgba(72,94,144,.16);display:flex;margin-top:40px;border-radius:5px;margin-bottom:40px;justify-content:center}.PostsPage-splashHeaderImage{top:0;left:0;right:0;width:100vw;bottom:0;height:100vh;position:absolute}.PostsPage-reserveSpaceForSidenotes{width:300px}@media (max-width:1279.95px){.PostsPage-reserveSpaceForSidenotes{width:50px}@media (max-width:599.95px){.PostsPage-reserveSpaceForSidenotes{width:5px}}}.PostsPage-reserveSpaceForIcons{width:0}@media (min-width:0px){.PostsPage-reserveSpaceForIcons{width:5px}@media (min-width:600px){.PostsPage-reserveSpaceForIcons{width:50px}}}.PostsPage-subscribeToGroup{color:rgba(0,0,0,.87);padding:8px 16px;font-size:14.3px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px}.PostsPage-dateAtBottom{color:#757575;cursor:default;font-size:14.3px}.PostsPage-reviewVoting{margin-top:60px;margin-bottom:-20px}");
            _embedStyles("SideItems", 0, ".SideItems-sideItem{width:100%;position:absolute}.SideItems-sidebar{height:100%;position:relative}");
            _embedStyles("ReadingProgressBar", 0, ".ReadingProgressBar-readingProgressBar{top:0;width:var(--scrollAmount);height:4px;z-index:10000000001;position:fixed;background:#3f51b5;--scrollAmount:0%}@media (max-width:959.95px){.ReadingProgressBar-readingProgressBar{margin-left:-8px;margin-right:-8px}}");
            _embedStyles("MultiToCLayout", 0, ".MultiToCLayout-root:has(.MultiToCLayout-gap1:hover) .ToCRowHover,.MultiToCLayout-root:has(.MultiToCLayout-toc:hover) .ToCRowHover,.MultiToCLayout-root:has(.MultiToCLayout-tocFooter:hover) .ToCRowHover{opacity:1}.MultiToCLayout-tableOfContents{display:grid;position:relative;grid-template-columns:0 minmax(200px,270px) minmax(35px,.5fr) minmax(min-content,720px) minmax(10px,30px) 50px minmax(0,.5fr)}@media (max-width:959.95px){.MultiToCLayout-tableOfContents{grid-template-columns:0 0 1fr minmax(0,720px) minmax(5px,1fr) min-content 0}}@media (min-width:1280px){.MultiToCLayout-tableOfContents{grid-template-columns:0 minmax(200px,270px) minmax(35px,.5fr) minmax(min-content,720px) minmax(10px,30px) min-content minmax(0,.5fr)}}.MultiToCLayout-gap1{grid-area:gap1}.MultiToCLayout-toc{width:unset;position:unset;margin-top:-50px}@media (max-width:959.95px){.MultiToCLayout-toc{display:none;margin-top:0;margin-bottom:0}}.MultiToCLayout-splashPageHeaderToc{margin-top:calc(-100vh - 64px)}.MultiToCLayout-commentToCMargin{margin-top:unset}.MultiToCLayout-commentToCIntersection{top:-1px!important}body:has(.headroom--pinned) .MultiToCLayoutStickyBlockScroller,body:has(.headroom--unfixed) .MultiToCLayoutStickyBlockScroller{top:64px;height:calc(100vh - 64px - var(--fixed-toc-footer-height, 50px))}.MultiToCLayout-stickyBlockScroller{top:0;height:calc(100vh - var(--fixed-toc-footer-height, 50px));position:sticky;font-size:12px;max-height:calc(100vh - var(--fixed-toc-footer-height, 50px));overflow-y:auto;text-align:left;transition:top .2s ease-in-out,height .2s ease-in-out;line-height:1;margin-left:1px;padding-left:16px;scrollbar-width:none}.MultiToCLayout-stickyBlockScroller::-webkit-scrollbar{width:0}@media (max-width:959.95px){.MultiToCLayout-stickyBlockScroller{display:none}}.MultiToCLayout-stickyBlock{height:100%;direction:ltr}.MultiToCLayout-hideTocButton{gap:10px;top:0;left:0;color:#757575;cursor:pointer;margin:18px;display:flex;position:fixed;font-size:14px;align-items:center;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:500;user-select:none}@media (max-width:959.95px){.MultiToCLayout-hideTocButton{display:none}}.MultiToCLayout-hideTocButtonHidden{display:none}.MultiToCLayout-tocFooter{left:0;width:240px;bottom:0;height:var(--fixed-toc-footer-height, 50px);z-index:1000;position:fixed;padding-top:12px;padding-left:12px;padding-bottom:20px;background-color:#fff}@media (max-width:959.95px){.MultiToCLayout-tocFooter{display:none}}");
            _embedStyles("TableOfContents", 0, "");
            _embedStyles("FixedPositionToC", 0, ".FixedPositionToC-root{top:0;left:0;height:100%;display:flex;max-height:100vh;transition:opacity .25s ease-in-out .5s,transform .5s ease-in-out .5s,height .4s ease-in-out,max-height .4s ease-in-out;word-break:break-word;padding-top:22px;flex-direction:column}.FixedPositionToC-root .TableOfContentsRow-title{border-bottom:none}.FixedPositionToC-hover .FixedPositionToC-headingOpacity,.FixedPositionToC-hover .FixedPositionToC-rowOpacity,.FixedPositionToC-hover .FixedPositionToC-tocTitle{opacity:1}.FixedPositionToC-rowWrapper{display:flex;z-index:1;position:relative;flex-direction:column}.FixedPositionToC-headingOpacity,.FixedPositionToC-rowOpacity{opacity:0;transition:.25s}.FixedPositionToC-rowDotContainer{display:flex;align-items:center}.FixedPositionToC-rowDot{color:#616161;height:9px;z-index:1;font-size:9px;background:#fff;line-height:9px;margin-left:2px;margin-right:8px}.FixedPositionToC-tocWrapper{margin-left:13px}.FixedPositionToC-tocTitle{font-size:1.3rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-variant:small-caps;font-variant-numeric:lining-nums}.FixedPositionToC-wrapper{display:flex;flex-grow:1;flex-direction:row;justify-content:space-evenly}.FixedPositionToC-progressBarContainer{width:1px;display:flex;background:#bdbdbd;overflow-y:clip;margin-right:-4px;margin-bottom:0;--scrollAmount:0%;flex-direction:column;justify-content:space-evenly}.FixedPositionToC-progressBar{flex:var(--scrollAmount);display:flex}@media (max-width:959.95px){.FixedPositionToC-progressBar{margin-left:-8px;margin-right:-8px}}.FixedPositionToC-progressBar:after{height:var(--windowHeight);content:\"\";align-self:end;background:#757575;margin-left:-.5px;padding-left:2px}.FixedPositionToC-unfilledProgressBar{flex:calc(100% - var(--scrollAmount));width:1px}@media (max-width:959.95px){.FixedPositionToC-unfilledProgressBar{margin-left:-8px;margin-right:-8px}}.FixedPositionToC-rows{display:flex;flex-grow:1;flex-direction:column}.FixedPositionToC-rows .TableOfContentsDivider-divider{margin-left:4px}");
            _embedStyles("LWPostsPageHeader", 0, ".LWPostsPageHeader-root{padding-top:110px;margin-bottom:96px}@media (max-width:599.95px){.LWPostsPageHeader-root{padding-top:16px;margin-bottom:38px}}@media (max-width:959.95px){.LWPostsPageHeader-rootWithAudioPlayer{padding-top:62px}}@media (max-width:599.95px){.LWPostsPageHeader-rootWithAudioPlayer{padding-top:16px}}.LWPostsPageHeader-eventHeader{margin-bottom:0}.LWPostsPageHeader-authorAndSecondaryInfo{color:#757575;display:flex;flex-wrap:wrap;column-gap:20px;margin-top:12px;align-items:baseline;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-variant-numeric:lining-nums}.LWPostsPageHeader-authorInfo{max-width:calc(100% - 60px)}@media (max-width:599.95px){.LWPostsPageHeader-authorInfo{width:100%;font-size:14.3px;margin-bottom:8px}}.LWPostsPageHeader-feedName{font-size:14.3px}@media (max-width:959.95px){.LWPostsPageHeader-feedName{display:none}}.LWPostsPageHeader-topRight{top:-48px;right:8px;display:flex;position:absolute}@media (max-width:959.95px){.LWPostsPageHeader-topRight{top:-16px;margin-top:8px;margin-bottom:8px}}@media (max-width:599.95px){.LWPostsPageHeader-topRight{display:none}}.LWPostsPageHeader-audioPlayerWrapper{top:15px;right:8px;display:flex;position:absolute}@media (max-width:959.95px){.LWPostsPageHeader-audioPlayerWrapper{position:relative;margin-left:8px;margin-right:-64px;justify-content:flex-end}}@media (max-width:599.95px){.LWPostsPageHeader-audioPlayerWrapper{justify-content:flex-start}}.LWPostsPageHeader-sequenceNav{margin-top:-22px;margin-bottom:8px}.LWPostsPageHeader-eventData{margin-top:48px}.LWPostsPageHeader-titleSection{display:flex;align-items:center;justify-content:space-between}@media (max-width:959.95px){.LWPostsPageHeader-title{margin-bottom:10px}}.LWPostsPageHeader-mobileHeaderVote{font-size:42px;margin-top:-55px;text-align:center;margin-left:12px}@media (min-width:600px){.LWPostsPageHeader-mobileHeaderVote{display:none}}.LWPostsPageHeader-date{margin-top:8px;margin-bottom:8px}.LWPostsPageHeader-mobileButtons{display:flex;align-items:center}@media (min-width:600px){.LWPostsPageHeader-mobileButtons{display:none}}.LWPostsPageHeader-audioToggle{display:flex;opacity:.75;margin-right:12px}.LWPostsPageHeader-readTime{margin-right:20px}.LWPostsPageHeader-bestOfLessWrong{color:rgba(0,0,0,.7);display:inline-block;font-size:22px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;line-height:24px;font-variant:small-caps;margin-bottom:12px;vertical-align:-webkit-baseline-middle}.LWPostsPageHeader-splashPageTitle.LWPostsPageHeader-splashPageTitle{font-size:6rem;margin-right:-100px}@media (max-width:1919.95px){.LWPostsPageHeader-splashPageTitle.LWPostsPageHeader-splashPageTitle{font-size:5rem;margin-right:-50px}}@media (max-width:1279.95px){.LWPostsPageHeader-splashPageTitle.LWPostsPageHeader-splashPageTitle{font-size:4rem;margin-right:-25px}}@media (max-width:599.95px){.LWPostsPageHeader-splashPageTitle.LWPostsPageHeader-splashPageTitle{font-size:3.75rem;margin-right:0}}.LWPostsPageHeader-splashPageTitleLong.LWPostsPageHeader-splashPageTitleLong{font-size:5rem;margin-right:-100px}@media (max-width:1919.95px){.LWPostsPageHeader-splashPageTitleLong.LWPostsPageHeader-splashPageTitleLong{font-size:4.5rem;margin-right:-50px}}@media (max-width:1279.95px){.LWPostsPageHeader-splashPageTitleLong.LWPostsPageHeader-splashPageTitleLong{font-size:4rem;margin-right:-25px}}@media (max-width:599.95px){.LWPostsPageHeader-splashPageTitleLong.LWPostsPageHeader-splashPageTitleLong{font-size:3.5rem;margin-right:0}}.LWPostsPageHeader-splashPageTitleLonger.LWPostsPageHeader-splashPageTitleLonger{font-size:4.5rem;margin-right:-100px}@media (max-width:1919.95px){.LWPostsPageHeader-splashPageTitleLonger.LWPostsPageHeader-splashPageTitleLonger{font-size:4rem;margin-right:-50px}}@media (max-width:1279.95px){.LWPostsPageHeader-splashPageTitleLonger.LWPostsPageHeader-splashPageTitleLonger{font-size:3.5rem;margin-right:-25px}}@media (max-width:599.95px){.LWPostsPageHeader-splashPageTitleLonger.LWPostsPageHeader-splashPageTitleLonger{font-size:3.25rem;margin-right:0}}.LWPostsPageHeader-titleSectionWithSplashPageHeader{margin-bottom:0}.LWPostsPageHeader-rootWithSplashPageHeader{padding-top:44vh}@media (max-width:599.95px){.LWPostsPageHeader-rootWithSplashPageHeader{margin-top:44vh;padding-top:0}}");
            _embedStyles("LWPostsPageHeaderTopRight", 0, ".LWPostsPageHeaderTopRight-root{display:flex;z-index:100;flex-wrap:nowrap;column-gap:8px;align-items:center}@media (max-width:959.95px){.LWPostsPageHeaderTopRight-root{top:8px;right:8px}}.LWPostsPageHeaderTopRight-vote{display:flex;flex-direction:row-reverse}@media (max-width:599.95px){.LWPostsPageHeaderTopRight-vote{display:none}}.LWPostsPageHeaderTopRight-postActionsButton{display:flex;opacity:.3;align-items:center}.LWPostsPageHeaderTopRight-postActionsButtonShortform{margin-top:12px;margin-right:8px}.LWPostsPageHeaderTopRight-tagList{opacity:.5;margin-top:12px;margin-bottom:12px}@media (max-width:599.95px){.LWPostsPageHeaderTopRight-tagList{display:none}}.LWPostsPageHeaderTopRight-audioToggle{display:flex;opacity:.55}.LWPostsPageHeaderTopRight-darkerOpacity{opacity:.7}");
            _embedStyles("FooterTagList", 0, ".FooterTagList-root{gap:4px;display:flex;flex-wrap:wrap;align-items:center;justify-content:flex-start}.FooterTagList-alignRight{justify-content:flex-end}.FooterTagList-allowTruncate{display:inline-flex;overflow:hidden;max-height:33px}.FooterTagList-overrideMargins{margin-top:0;margin-bottom:0}.FooterTagList-frontpageOrPersonal{color:#757575;border:1px solid rgba(0,0,0,.15);cursor:pointer;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;border-radius:3px;padding:4.5px 6px;background-color:#fff;font-variant-numeric:lining-nums}.FooterTagList-neverCoreStyling{color:rgba(0,0,0,.9)}.FooterTagList-noBackground.FooterTagList-noBackground.FooterTagList-noBackground{border:0;background-color:transparent}.FooterTagList-card{width:450px;padding:8px 16px 16px}.FooterTagList-smallText{font-size:12px;font-weight:400;padding-top:1px;margin-bottom:0;padding-bottom:2px}.FooterTagList-showAll{color:#9e9e9e;width:fit-content;cursor:pointer;font-size:14px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:600}.FooterTagList-altAddTagButton{gap:4px;border:solid 1px #eee;cursor:pointer;display:inline-block;font-weight:700;border-radius:3px;padding:4.5px 8px;background-color:#fff}");
            _embedStyles("FooterTag", -1, ".FooterTag-root{color:rgba(0,0,0,.9);border:solid 1px #eee;cursor:pointer;display:inline-block;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;border-radius:3px;padding:5px 6px;background-color:#eee;font-variant-numeric:lining-nums}.FooterTag-root:hover{opacity:1}.FooterTag-core{color:#757575;border:1px solid rgba(0,0,0,.15);background-color:#fff}.FooterTag-coreIcon{display:inline-block;position:relative;min-width:20px}.FooterTag-coreIcon svg{top:-13px;fill:#757575;left:-4px;width:20px;height:18px;position:absolute}.FooterTag-score{color:rgba(0,0,0,.7);padding-left:5px}.FooterTag-name{display:inline-block}.FooterTag-smallText{font-size:12px;font-weight:400;padding-top:1px;margin-bottom:0;padding-bottom:2px}.FooterTag-robotIcon svg{height:12px;opacity:.7;margin-left:4px}.FooterTag-noBackground{border:0;padding-top:3px;padding-bottom:3px;background-color:transparent}");
            _embedStyles("TagsTooltip", 0, ".TagsTooltip-tooltip{padding:0;background:#fff;box-shadow:0 1px 3px 0 rgba(0,0,0,.2),0 1px 1px 0 rgba(0,0,0,.14),0 2px 1px -1px rgba(0,0,0,.12)}.TagsTooltip-tooltipTitle{max-width:unset}.TagsTooltip-loading{padding-left:16px;padding-right:32px;padding-bottom:24px}.TagsTooltip-noTagOrPlaceholderMessage{padding:0 16px 8px}.TagsTooltip-noTagOrPlaceholderMessage .ContentStyles-base.ContentStyles-tagBody,.TagsTooltip-redLinkTooltip .ContentStyles-base.ContentStyles-tagBody{margin-bottom:0!important}.TagsTooltip-redLinkTooltip{width:500px;padding:8px 16px 6px}@media (max-width:599.95px){.TagsTooltip-redLinkTooltip{width:100%}}.TagsTooltip-redLinkTooltipTitle{color:rgba(0,0,0,.54);font-size:13px;margin-top:8px;margin-bottom:4px}");
            _embedStyles("LWTooltip", -1, ".LWTooltip-root{display:inline-block}.LWTooltip-tooltip{max-width:300px}.LWTooltip-tooltip img{max-width:100%}");
            _embedStyles("LWPostsPageTopHeaderVote", 0, ".LWPostsPageTopHeaderVote-voteBlockHorizontal{display:flex;opacity:.76;align-items:center;border-radius:4px;flex-direction:column;justify-content:flex-end}.LWPostsPageTopHeaderVote-upvoteHorizontal{margin-bottom:2px}.LWPostsPageTopHeaderVote-downvoteHorizontal .VoteArrowIconSolid-root,.LWPostsPageTopHeaderVote-upvoteHorizontal .VoteArrowIconSolid-root{color:#424242}.LWPostsPageTopHeaderVote-voteScoresHorizontal{margin:-4px 8px}.LWPostsPageTopHeaderVote-voteScore{color:#757575;z-index:2;position:relative;font-size:1rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-variant-numeric:lining-nums}.LWPostsPageTopHeaderVote-tooltip{color:#9e9e9e;font-size:1rem;transition:opacity 150ms cubic-bezier(.4,0,1,1) 0ms;margin-left:0;background-color:#fff}");
            _embedStyles("TooltipRef", -1, ".TooltipRef-tooltip{color:#fff;padding:9.1px;z-index:10000000;font-size:13px;max-width:300px;background-color:rgba(75,75,75,.94)}.TooltipRef-tooltip img{max-width:100%}");
            _embedStyles("VoteArrowIconSolid", 0, ".VoteArrowIconSolid-root{position:relative}.VoteArrowIconSolid-disabled{cursor:not-allowed}.VoteArrowIconSolid-smallArrow,.VoteArrowIconSolid-smallArrowLarge{opacity:.6;pointer-events:none}.VoteArrowIconSolid-down,.VoteArrowIconSolid-downLarge{top:1px;transform:rotate(-180deg)}.VoteArrowIconSolid-right,.VoteArrowIconSolid-rightLarge{transform:rotate(-270deg)}.VoteArrowIconSolid-left,.VoteArrowIconSolid-leftLarge{transform:rotate(-90deg)}.VoteArrowIconSolid-bigArrow,.VoteArrowIconSolid-bigArrowLarge{top:-70%;width:10px;height:10px;opacity:0;position:absolute;font-size:82%;pointer-events:none}.VoteArrowIconSolid-bigArrowLarge{top:-90%;width:14px;height:14px;font-size:100%}.VoteArrowIconSolid-bigArrowSolid{top:-35%;transform:scale(1.4)}.VoteArrowIconSolid-bigArrowSolidLarge{top:-30%;transform:scale(1.6)}.VoteArrowIconSolid-bigArrowCompleted{top:-35%;opacity:1;font-size:90%}.VoteArrowIconSolid-bigArrowCompletedLarge{top:-35%;opacity:1;font-size:110%}.VoteArrowIconSolid-entering{transition:opacity 1000ms cubic-bezier(.74,-.01,1,1) 0ms}");
            _embedStyles("VoteButton", 0, ".VoteButton-root{flex:0 0 auto;color:#bdbdbd;width:initial;border:0;cursor:pointer;height:initial;margin:0;display:inline-flex;outline:0;padding:0;overflow:visible;position:relative;font-size:inherit;text-align:center;transition:background-color 150ms cubic-bezier(.4,0,.2,1) 0ms;align-items:center;user-select:none;border-radius:50%;vertical-align:middle;-moz-appearance:none;justify-content:center;text-decoration:none;-webkit-appearance:none;-webkit-tap-highlight-color:transparent}.VoteButton-root,.VoteButton-root:hover{background-color:transparent}.VoteButton-root::-moz-focus-inner{border-style:none}.VoteButton-inner{width:100%;display:flex;align-items:inherit;justify-content:inherit}.VoteButton-colorMainPrimary,.VoteButton-colorMainSecondary{color:#3f51b5}.VoteButton-colorMainError{color:#bf360c}.VoteButton-colorLightPrimary,.VoteButton-colorLightSecondary{color:#7986cb}.VoteButton-colorLightError{color:#cb5e3c}");
            _embedStyles("PostActionsButton", 0, ".PostActionsButton-root{cursor:pointer}.PostActionsButton-icon{cursor:pointer;vertical-align:middle}.PostActionsButton-popper{z-index:1050;position:relative}");
            _embedStyles("PostsAudioPlayerWrapper", 0, ".PostsAudioPlayerWrapper-embeddedPlayer{margin-bottom:30px}.PostsAudioPlayerWrapper-hideEmbeddedPlayer{display:none}");
            _embedStyles("PostsPageTitle", 0, ".PostsPageTitle-root{color:rgba(0,0,0,.87);font-size:3.75rem;text-wrap:balance;margin-top:0;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:500;line-height:1.1;margin-left:0;margin-bottom:0;letter-spacing:-.02em;font-variant-numeric:lining-nums}@media (max-width:959.95px){.PostsPageTitle-root{font-size:3.5rem}}@media (max-width:599.95px){.PostsPageTitle-root{font-size:2.5rem}}.PostsPageTitle-draft{color:#9e9e9e}.PostsPageTitle-question{color:#757575;display:block}.PostsPageTitle-link:hover{opacity:unset}.PostsPageTitle-lastWord{display:inline-block}.PostsPageTitle-dialogueIcon,.PostsPageTitle-linkIcon{color:#9e9e9e;font-size:.8em;margin-left:14px}.PostsPageTitle-dialogueIcon{font-size:1em;transform:translateY(5px)}");
            _embedStyles("PostsAuthors", 0, ".PostsAuthors-root{display:inline;text-align:left}.PostsAuthors-authorName{font-weight:600;margin-left:0}.PostsAuthors-authorMarkers{display:inline-block;margin-left:3px;margin-right:-3px}");
            _embedStyles("UsersNameDisplay", 0, ".UsersNameDisplay-color{color:#3f51b5}.UsersNameDisplay-noColor{color:inherit!important}.UsersNameDisplay-noKibitz{min-width:55px}.UsersNameDisplay-nowrap{white-space:nowrap}");
            _embedStyles("UserTooltip", 0, ".UserTooltip-root{padding:0;background:unset}.UserTooltip-overrideTooltip{padding:0;max-width:none}");
            _embedStyles("UserCommentMarkers", 0, ".UserCommentMarkers-iconWrapper{margin:0 3px}.UserCommentMarkers-postAuthorIcon{color:#9e9e9e;font-size:16px;vertical-align:text-bottom}.UserCommentMarkers-sproutIcon{color:#69886e;bottom:-2px;position:relative;font-size:16px}");
            _embedStyles("PostsPageDate", 0, ".PostsPageDate-date{color:#757575;cursor:default;font-size:14.3px}@media (min-width:960px){.PostsPageDate-mobileDate{display:none}}");
            _embedStyles("ReadTime", 0, ".ReadTime-root{cursor:default;font-size:14.3px}@media print{.ReadTime-root{display:none}}");
            _embedStyles("LWCommentCount", 0, ".LWCommentCount-comments,.LWCommentCount-root{color:#757575;display:flex;align-items:center}.LWCommentCount-comments{color:#9e9e9e;font-size:14.3px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;margin-right:16px;font-variant-numeric:lining-nums}.LWCommentCount-commentsIcon{top:1px;color:#9e9e9e;position:relative;font-size:20px;margin-right:8px}@font-face{font-family:ETBookBold;src:url(https://res.cloudinary.com/lesswrong-2-0/raw/upload/v1504470690/et-book-bold-line-figures_piiabg.woff)format(\"woff\")}.LWCommentCount-answerIcon{color:#757575;font-size:24px;font-family:ETBookBold;font-weight:900;margin-right:7px}.LWCommentCount-commentsLabel{margin-left:6px}.LWCommentCount-wideClickTarget{width:100%}.LWCommentCount-rowOpacity{opacity:0;transition:opacity .3s ease-in-out}");
            _embedStyles("AudioToggle", 0, ".AudioToggle-togglePodcastContainer{color:#757575;height:(theme) => theme.isFriendlyUI ? 22 : 24;align-self:center}.AudioToggle-audioIcon{width:28px;height:28px;padding:2px;transform:translateY(-(theme) => theme.isFriendlyUI ? 4 : 2px)}.AudioToggle-audioIconOn{background:rgba(0,0,0,.08);border-radius:3px}.AudioToggle-audioIconDisabled{cursor:not-allowed;opacity:.5}");
            _embedStyles("PostsVoteDefault", 0, ".PostsVoteDefault-voteBlock{width:50px}.PostsVoteDefault-voteBlockHorizontal{display:flex;align-items:center;flex-direction:row-reverse;justify-content:flex-end}.PostsVoteDefault-upvote{margin-bottom:-21px}.PostsVoteDefault-upvoteHorizontal{margin-top:-8px}.PostsVoteDefault-downvote{margin-top:-28px}.PostsVoteDefault-downvoteHorizontal{margin-top:-6px}.PostsVoteDefault-voteScores{margin:15%;font-variant-numeric:lining-nums}.PostsVoteDefault-voteScoresHorizontal{margin:0 12px}.PostsVoteDefault-voteScore{color:#9e9e9e;z-index:2;position:relative;font-size:55%}.PostsVoteDefault-voteScoreFooter{font-size:18px;font-weight:500}.PostsVoteDefault-voteScoreGoodHeart{color:#616161;font-size:45%;text-align:center;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-variant-numeric:lining-nums}.PostsVoteDefault-secondaryVoteScore{font-size:35%;margin-bottom:2px}.PostsVoteDefault-tooltip{color:#9e9e9e;font-size:1rem;transition:opacity 150ms cubic-bezier(.4,0,1,1) 0ms;margin-left:0;padding-top:0;background-color:#fff}");
            _embedStyles("VoteArrowIconHollow", 0, ".VoteArrowIconHollow-disabled{cursor:not-allowed}.VoteArrowIconHollow-smallArrow{opacity:.6;font-size:50%}.VoteArrowIconHollow-right{transform:rotate(-270deg)}.VoteArrowIconHollow-down{transform:rotate(-180deg)}.VoteArrowIconHollow-left{transform:rotate(-90deg)}.VoteArrowIconHollow-bigArrow{top:-70%;opacity:0;position:absolute;font-size:82%;transition:opacity 1000ms cubic-bezier(.74,-.01,1,1) 0ms}.VoteArrowIconHollow-bigArrowCompleted{top:-75%;font-size:90%}.VoteArrowIconHollow-entered,.VoteArrowIconHollow-entering{opacity:1}.VoteArrowIconHollow-exiting{transition:opacity 150ms cubic-bezier(.74,-.01,1,1) 0ms}");
            _embedStyles("ReviewPillContainer", 0, ".ReviewPillContainer-root{gap:8px;color:rgba(0,0,0,.87);display:flex;flex-wrap:wrap;font-size:1.1rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;padding-top:24px}.ReviewPillContainer-review{gap:7px;color:#fff;cursor:pointer;display:flex;padding:3px 10px 4px;z-index:1;overflow:hidden;position:relative;white-space:nowrap;border-radius:3px;background-color:lch(68 34.48 85.39/76%)}.ReviewPillContainer-reviewPreviewContainer{min-height:0;padding-top:8px}.ReviewPillContainer-reviewPreview{width:650px;border:1px solid lch(68 34.48 85.39/76%);padding:8px;max-width:70vw;max-height:100%;overflow-y:scroll;text-align:left;transition:opacity .2s ease-in-out;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-left:auto;white-space:normal;margin-right:auto;border-radius:3px;scrollbar-width:none;background-color:#fff;-ms-overflow-style:none;font-variant-numeric:lining-nums}.ReviewPillContainer-reviewPreview::-webkit-scrollbar{display:none}.ReviewPillContainer-reviewMeta{gap:8px;color:rgba(0,0,0,.87);display:flex;font-size:14.3px;align-items:center;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px}.ReviewPillContainer-reviewPreviewYear{color:#303f9f;font-size:1rem;font-style:italic}.ReviewPillContainer-reviewerName{margin-left:-3px}");
            _embedStyles("ContentStyles", -1, ".ContentStyles-base{word-break:break-word}.ContentStyles-base pre{border:solid 1px #e0e0e0;margin:1em 0;padding:13px;white-space:pre-wrap;border-radius:5px;background-color:#f5f5f5}.ContentStyles-base code{font-size:.7em;font-family:\"Liberation Mono\",Menlo,Courier,monospace;font-weight:400;line-height:1.42;padding-top:3px;border-radius:2px;padding-bottom:3px;background-color:#f5f5f5}.ContentStyles-base blockquote{margin:0;border-left:solid 3px #e0e0e0;padding:16px}.ContentStyles-base,.ContentStyles-base blockquote,.ContentStyles-base h1,.ContentStyles-base li{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif}.ContentStyles-base,.ContentStyles-base blockquote,.ContentStyles-base li{color:rgba(0,0,0,.87);font-size:18.2px;font-weight:400;line-height:26px;font-variant-numeric:lining-nums}.ContentStyles-base li{margin-bottom:6.5px}.ContentStyles-base h1{font-size:36.4px;margin-top:1em;font-weight:500;line-height:1.13333em;margin-left:-.02em}.ContentStyles-base h1:first-child{margin-top:0;margin-block-start:-3px}.ContentStyles-base h2{font-size:26px;margin-top:1em;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:1.20588em}.ContentStyles-base h2:first-child{margin-top:0;margin-block-start:-2px}.ContentStyles-base h3{font-size:20.8px;margin-top:1em;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:1.20588em}.ContentStyles-base h3:first-child{margin-top:0;margin-block-start:0}.ContentStyles-base h4{font-size:18.2px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:600;line-height:26px;font-variant-numeric:lining-nums}.ContentStyles-base img{max-width:100%}.ContentStyles-base sup{top:-.6em;position:relative;font-size:65%;line-height:0;vertical-align:baseline}.ContentStyles-base sub{top:.2em;position:relative;font-size:70%;padding-right:.07em;vertical-align:baseline}.ContentStyles-base a,.ContentStyles-base a:active,.ContentStyles-base a:hover{color:#3f51b5}.ContentStyles-base a.visited,.ContentStyles-base a:visited{color:#8c4298}.ContentStyles-base table{width:100%;border:1px double #b3b3b3;height:100%;margin:auto;text-align:left;border-spacing:0;border-collapse:collapse}.ContentStyles-base figure.table{width:fit-content;height:fit-content}.ContentStyles-base figure.table:has(>table>tbody>tr>td+td+td+td){overflow-x:auto}.ContentStyles-base td,.ContentStyles-base th{border:1px double #d9d9d9;padding:.4em;min-width:2em;word-break:normal}.ContentStyles-base th{background:#fafafa;font-weight:700}.ContentStyles-base figure{margin:1em auto;max-width:100%;text-align:center}.ContentStyles-base figcaption{color:rgba(0,0,0,.54);font-size:11.7px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:1.375em;font-variant-numeric:lining-nums}.ContentStyles-base ol>li>ol{list-style:lower-alpha}.ContentStyles-base ol>li>ol>li>ol{list-style:lower-roman}.ContentStyles-base a u,.ContentStyles-base a:active u,.ContentStyles-base a:hover u,.ContentStyles-base u{text-decoration:none}.ContentStyles-base p.spoiler{margin:0}.ContentStyles-base .spoiler{padding:8px;pointer-events:auto}.ContentStyles-base div.spoilers{margin:1em 0;overflow:auto}.ContentStyles-base p.spoiler-v2{margin:0;padding:.5em 8px}.ContentStyles-base .spoilers:not(:hover) ::selection,.ContentStyles-base .spoilers:not(:hover)::selection{background-color:transparent}.ContentStyles-base .spoilers>p:hover~p{color:#000;background-color:#000}.ContentStyles-base div.metaculus-preview{background-color:#2c3947}.ContentStyles-base div.estimaker-preview,.ContentStyles-base div.viewpoints-preview{display:flex}.ContentStyles-base figure.media div[data-oembed-url*=\"youtu.be\"],.ContentStyles-base figure.media div[data-oembed-url*=\"youtube.com\"]{height:0;position:relative;padding-bottom:56.2493%}.ContentStyles-base .footnote-item>*{vertical-align:text-top}.ContentStyles-base .footnote-back-link{top:-.2em;position:relative}.ContentStyles-base .footnotes .footnote-back-link>sup{margin-right:0}.ContentStyles-base .footnote-content{width:95%;display:inline-block;padding:0 .3em}.ContentStyles-base .detailsBlock{border:1px solid rgba(0,0,0,.2)!important;margin-top:8px;border-radius:8px;margin-bottom:8px}.ContentStyles-base .detailsBlockTitle{padding:8px;background:rgba(0,0,0,.05)!important;border-radius:0}.ContentStyles-base .detailsBlockTitle[open]{border-bottom-left-radius:0;border-bottom-right-radius:0}.ContentStyles-base summary.detailsBlockTitle{cursor:pointer}.ContentStyles-base .detailsBlockContent{padding:8px}.ContentStyles-base .detailsBlockContent>p:last-child,.ContentStyles-base .detailsBlockTitle>p:last-child{margin-bottom:0!important}.ContentStyles-base .detailsBlockTitle p:has(>br[data-cke-filler=true]:only-child)::after{top:8px;color:rgba(0,0,0,.3);content:\"Collapsible Section Title\";position:absolute;pointer-events:none}.ContentStyles-base .detailsBlock.closed .detailsBlockContent{display:none}.ContentStyles-base div.detailsBlockTitle{position:relative;padding-left:24px}.ContentStyles-base div.detailsBlockTitle::before{left:8px;cursor:pointer;content:\"▼\";position:absolute;font-size:14px;padding-right:4px}.ContentStyles-base .detailsBlock.closed div.detailsBlockTitle::before{content:\"▶\"}.ContentStyles-base .conditionallyVisibleBlock{border:1px solid rgba(0,0,0,.2);padding:8px;border-radius:4px}.ContentStyles-base .ck-cta-button,.ContentStyles-base div.detailsBlockTitle{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif}.ContentStyles-base .ck-cta-button{color:#fff;width:fit-content;cursor:pointer;display:block;padding:12px 16px;font-size:14px;min-width:30px;box-shadow:none;transition:background-color .3s ease;font-weight:600;line-height:20px;border-radius:3px;text-transform:none;background-color:#5f9b65}.ContentStyles-base .ck-cta-button-centered{margin:auto}.ContentStyles-base .footnotes{font-size:.9em;border-top:1px solid rgba(0,0,0,.2);margin-top:40px;padding-top:40px}.ContentStyles-base .footnotes-sep{display:none}.ContentStyles-base hr{width:100%;border:0;height:100%;margin:32px 0;display:flex;background:0 0;text-align:center;align-items:center;justify-content:center}.ContentStyles-base hr:after{color:rgba(0,0,0,.26);content:\"•••\";font-size:13px;margin-left:12px;letter-spacing:12px}.ContentStyles-base .footnotes sup{margin-right:10px}.ContentStyles-base .footnotes ol{margin-block-start:1em;margin-inline-start:1em;padding-inline-start:0}.ContentStyles-base .footnotes li{font-size:.9em}.ContentStyles-base .footnotes blockquote{font-size:.9em;margin-top:-10px;line-height:1.5em;padding:1px 1px 1px 3px}.ContentStyles-base .ck-cta-button:hover{color:#fff;opacity:1;background-color:#303f9f}.ContentStyles-base .ck-cta-button.visited,.ContentStyles-base .ck-cta-button.visited:hover,.ContentStyles-base .ck-cta-button:visited,.ContentStyles-base .ck-cta-button:visited:hover{color:#fff}.ContentStyles-base .ck-cta-button:disabled{color:#fff;opacity:.5;background-color:#5f9b65}.ContentStyles-base .conditionallyVisibleBlock.defaultHidden{display:none}.ContentStyles-base .detailsBlockTitle>*{display:inline}.ContentStyles-base figure.media div[data-oembed-url*=\"lwartifacts.vercel.app\"] iframe{width:100%;border:0;height:525px}.ContentStyles-base figure.media div[data-oembed-url*=\"youtu.be\"] iframe,.ContentStyles-base figure.media div[data-oembed-url*=\"youtube.com\"] iframe{top:0;left:0;width:100%;border:0;height:100%;position:absolute}.ContentStyles-base div.viewpoints-preview iframe{width:100%;border:0;height:300px}.ContentStyles-base div.estimaker-preview iframe,.ContentStyles-base div.metaforecast-preview iframe,.ContentStyles-base div.owid-preview iframe,.ContentStyles-base div.strawpoll-embed iframe{width:100%;border:0;height:400px}.ContentStyles-base div.calendly-preview iframe{width:calc(100% - 10px);border:2px solid;height:750px;padding:0;margin-left:5px;border-color:#eee;margin-right:5px;border-radius:10px}.ContentStyles-base div.neuronpedia-preview iframe{width:100%;border:1px solid;height:360px;max-width:639px;border-color:#e0e0e0;border-radius:6px}.ContentStyles-base div.manifold-preview iframe,.ContentStyles-base div.metaculus-preview iframe{width:100%;border:0;height:400px}.ContentStyles-base .spoilers>p:hover~p a,.ContentStyles-base .spoilers>p:hover~p a::after,.ContentStyles-base .spoilers>p:hover~p a:focus,.ContentStyles-base .spoilers>p:hover~p a:hover,.ContentStyles-base .spoilers>p:hover~p li{color:#000}.ContentStyles-base .spoilers>p:hover~p code{background-color:#000}.ContentStyles-base .spoiler:not(:hover),.ContentStyles-base div.spoilers:not(:hover){color:#000;background-color:#000}.ContentStyles-base div.spoilers:hover{background:rgba(0,0,0,.12)}.ContentStyles-base div.spoilers>p{margin:0!important;padding:.5em 8px!important}.ContentStyles-base .spoiler:not(:hover) a,.ContentStyles-base .spoiler:not(:hover) a::after,.ContentStyles-base .spoiler:not(:hover) a:focus,.ContentStyles-base .spoiler:not(:hover) a:hover,.ContentStyles-base .spoiler:not(:hover) li,.ContentStyles-base div.spoilers:not(:hover) a,.ContentStyles-base div.spoilers:not(:hover) a::after,.ContentStyles-base div.spoilers:not(:hover) a:focus,.ContentStyles-base div.spoilers:not(:hover) a:hover,.ContentStyles-base div.spoilers:not(:hover) li{color:#000}.ContentStyles-base .spoiler:not(:hover) code,.ContentStyles-base div.spoilers:not(:hover) code{background-color:#000}.ContentStyles-base td p,.ContentStyles-base th p{margin-top:.5em;margin-bottom:.5em}.ContentStyles-base td p:first-of-type,.ContentStyles-base th p:first-of-type{margin-top:0}.ContentStyles-base figure.table:has(>table>tbody>tr>td+td+td+td) table{width:700px}.ContentStyles-commentBody,.ContentStyles-postHighlight,.ContentStyles-postHighlight blockquote,.ContentStyles-postHighlight li{color:rgba(0,0,0,.87);font-size:16.64px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:22.75px;font-variant-numeric:lining-nums}.ContentStyles-commentBody,.ContentStyles-postHighlight blockquote,.ContentStyles-postHighlight li{font-size:14.3px;line-height:19.5px}.ContentStyles-answerBody ul,.ContentStyles-postHighlight ul{padding-inline-start:30px}.ContentStyles-commentBody,.ContentStyles-postHighlight li{font-size:16.64px;line-height:23.4px}.ContentStyles-postHighlight h1,.ContentStyles-postHighlight h2,.ContentStyles-postHighlight h3{font-size:1.6rem;margin-block-start:0!important}.ContentStyles-commentBody{font-size:14.3px;margin-top:.5em;word-break:break-word;line-height:19.5px;margin-bottom:.25em;pointer-events:none}.ContentStyles-commentBody .footnotes,.ContentStyles-commentBodyExceptPointerEvents .footnotes,.ContentStyles-debateResponseBody .footnotes,.ContentStyles-llmChat .footnotes,.ContentStyles-tagBody .footnotes,.ContentStyles-ultraFeed .footnotes{margin-top:0;padding-top:8px;padding-left:16px;margin-bottom:0}.ContentStyles-commentBody blockquote,.ContentStyles-commentBodyExceptPointerEvents blockquote,.ContentStyles-debateResponseBody blockquote{font-size:14.3px;border-left:solid 3px #e0e0e0;margin:0 0 0 12px;padding:8px 24px 8px 16px}.ContentStyles-commentBody li,.ContentStyles-commentBodyExceptPointerEvents li,.ContentStyles-debateResponseBody li{font-size:14.3px}.ContentStyles-commentBody h1,.ContentStyles-commentBody h2,.ContentStyles-commentBody h3,.ContentStyles-commentBodyExceptPointerEvents h1,.ContentStyles-commentBodyExceptPointerEvents h2,.ContentStyles-commentBodyExceptPointerEvents h3,.ContentStyles-debateResponseBody h1,.ContentStyles-debateResponseBody h2,.ContentStyles-debateResponseBody h3,.ContentStyles-llmChat h1,.ContentStyles-llmChat h2,.ContentStyles-llmChat h3,.ContentStyles-tagBody h1,.ContentStyles-tagBody h2,.ContentStyles-tagBody h3,.ContentStyles-ultraFeed h1,.ContentStyles-ultraFeed h2,.ContentStyles-ultraFeed h3{font-size:19.5px;margin-top:.5em;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:500;font-variant-numeric:lining-nums}.ContentStyles-commentBody>*{pointer-events:auto}.ContentStyles-commentBody>:hover~.spoiler,.ContentStyles-commentBodyExceptPointerEvents>:hover~.spoiler,.ContentStyles-debateResponseBody>:hover~.spoiler,.ContentStyles-llmChat>:hover~.spoiler,.ContentStyles-tagBody>:hover~.spoiler,.ContentStyles-ultraFeed>:hover~.spoiler{color:#000}.ContentStyles-commentBody>:hover~.spoiler:before,.ContentStyles-commentBodyExceptPointerEvents>:hover~.spoiler:before,.ContentStyles-debateResponseBody>:hover~.spoiler:before,.ContentStyles-llmChat>:hover~.spoiler:before,.ContentStyles-tagBody>:hover~.spoiler:before,.ContentStyles-ultraFeed>:hover~.spoiler:before{color:#fff;content:\"spoiler (hover/select to reveal)\"}.ContentStyles-commentBody hr,.ContentStyles-commentBodyExceptPointerEvents hr,.ContentStyles-debateResponseBody hr,.ContentStyles-llmChat hr,.ContentStyles-tagBody hr,.ContentStyles-ultraFeed hr{margin-top:12px;margin-bottom:12px}.ContentStyles-commentBody blockquote,.ContentStyles-commentBody li,.ContentStyles-commentBodyExceptPointerEvents,.ContentStyles-commentBodyExceptPointerEvents blockquote,.ContentStyles-commentBodyExceptPointerEvents li,.ContentStyles-debateResponseBody,.ContentStyles-debateResponseBody blockquote,.ContentStyles-debateResponseBody li{color:rgba(0,0,0,.87);font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;font-variant-numeric:lining-nums}.ContentStyles-commentBodyExceptPointerEvents,.ContentStyles-debateResponseBody{margin-top:.5em;word-break:break-word;margin-bottom:.25em}.ContentStyles-commentBodyExceptPointerEvents{font-size:14.3px}.ContentStyles-debateResponseBody{font-size:1.35rem;pointer-events:none}.ContentStyles-debateResponseBody>*{pointer-events:auto}.ContentStyles-debateResponseBody .dialogue-message-header+p{margin-top:0}.ContentStyles-debateResponseBody blockquote,.ContentStyles-debateResponseBody li{font-size:1.35rem}.ContentStyles-answerBody,.ContentStyles-answerBody blockquote{color:rgba(0,0,0,.87);font-size:16.64px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:22.75px;font-variant-numeric:lining-nums}.ContentStyles-answerBody blockquote{font-size:14.3px;line-height:19.5px}.ContentStyles-answerBody li{color:rgba(0,0,0,.87);font-size:16.64px;font-weight:400;line-height:23.4px}.ContentStyles-tagBody{margin-top:.5em;word-break:break-word;margin-bottom:18px;pointer-events:none}.ContentStyles-llmChat blockquote,.ContentStyles-tagBody blockquote{border-left:solid 3px #e0e0e0;margin:0 0 0 12px;padding:8px 24px 8px 16px}.ContentStyles-answerBody li,.ContentStyles-llmChat blockquote,.ContentStyles-llmChat li,.ContentStyles-tagBody,.ContentStyles-tagBody blockquote,.ContentStyles-tagBody li,.ContentStyles-tagBody.ContentStyles-tagBody h1,.ContentStyles-tagBody.ContentStyles-tagBody h2,.ContentStyles-tagBody.ContentStyles-tagBody h3{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-variant-numeric:lining-nums}.ContentStyles-llmChat blockquote,.ContentStyles-llmChat li,.ContentStyles-tagBody,.ContentStyles-tagBody blockquote,.ContentStyles-tagBody li{color:rgba(0,0,0,.87);font-size:14.3px;font-weight:400;line-height:19.5px}.ContentStyles-tagBody>*{pointer-events:auto}.ContentStyles-tagBody.ContentStyles-tagBody h1,.ContentStyles-tagBody.ContentStyles-tagBody h2,.ContentStyles-tagBody.ContentStyles-tagBody h3{font-size:2rem;margin-top:3rem;font-weight:600}.ContentStyles-tagBody.ContentStyles-tagBody h2,.ContentStyles-tagBody.ContentStyles-tagBody h3{font-size:1.7rem;margin-top:1.5rem;font-weight:500}.ContentStyles-tagBody.ContentStyles-tagBody h3{font-size:1.3rem}.ContentStyles-tagBody h2:first-child,.ContentStyles-tagBody h3:first-child,.ContentStyles-tagBody.ContentStyles-tagBody h1:first-child{margin-top:0}.ContentStyles-llmChat{color:rgba(0,0,0,.87);font-size:1rem;margin-top:.5em;word-break:break-word;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;margin-bottom:.25em;pointer-events:none;font-variant-numeric:lining-nums}.ContentStyles-llmChat>*{pointer-events:auto}.ContentStyles-llmChat blockquote,.ContentStyles-llmChat li{font-size:1rem}.ContentStyles-ultraFeed{margin-top:0;word-break:break-word;margin-bottom:0;pointer-events:none}.ContentStyles-ultraFeed blockquote{border-left:solid 3px #e0e0e0;margin:0 0 0 12px;padding:8px 24px 8px 16px}.ContentStyles-ultraFeed,.ContentStyles-ultraFeed blockquote,.ContentStyles-ultraFeed li{color:rgba(0,0,0,.87);font-size:14.3px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;font-variant-numeric:lining-nums}.ContentStyles-ultraFeed>*{pointer-events:auto}.ContentStyles-ultraFeed p:first-child{margin-top:0!important}.ContentStyles-ultraFeed p:last-child{margin-bottom:0!important}.ContentStyles-ultraFeed p:first-child>br:first-child{display:none!important}.ContentStyles-ultraFeed p:last-child>br:last-child{display:none!important}@media (max-width:959.95px){.ContentStyles-ultraFeed h1,.ContentStyles-ultraFeed h2,.ContentStyles-ultraFeed h3,.ContentStyles-ultraFeed h4{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-block-start:0!important}.ContentStyles-ultraFeed iframe,.ContentStyles-ultraFeed img{height:auto;max-width:100%}}.ContentStyles-ultraFeedPost{margin-top:0;margin-bottom:0}.ContentStyles-ultraFeedPost p:first-child{margin-top:0!important}.ContentStyles-ultraFeedPost p:last-child{margin-bottom:0!important}.ContentStyles-ultraFeedPost p:first-child>br:first-child{display:none!important}.ContentStyles-ultraFeedPost p:last-child>br:last-child{display:none!important}@media (max-width:959.95px){.ContentStyles-ultraFeedPost h1,.ContentStyles-ultraFeedPost h2,.ContentStyles-ultraFeedPost h3,.ContentStyles-ultraFeedPost h4{margin-block-start:0!important}.ContentStyles-ultraFeedPost iframe,.ContentStyles-ultraFeedPost img{height:auto;max-width:100%}}");
            _embedStyles("PostBodyPrefix", 0, ".PostBodyPrefix-reviewInfo{text-align:center;margin-bottom:32px}.PostBodyPrefix-contentNotice,.PostBodyPrefix-rejectionNotice,.PostBodyPrefix-reviewLabel{color:#757575;font-size:.9em;font-style:italic;word-break:break-word;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-bottom:8px;font-variant-numeric:lining-nums}.PostBodyPrefix-contentNotice,.PostBodyPrefix-rejectionNotice{max-width:600px;margin-bottom:20px}.PostBodyPrefix-rejectionNotice{opacity:.75;margin-bottom:40px}.PostBodyPrefix-infoIcon{color:rgba(0,0,0,.4);width:16px;height:16px;margin-left:8px;vertical-align:top}");
            _embedStyles("AlignmentPendingApprovalMessage", 0, ".AlignmentPendingApprovalMessage-root{color:#757575;font-size:.9em;font-style:italic;word-break:break-word;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-bottom:20px;font-variant-numeric:lining-nums}");
            _embedStyles("PostsPagePostFooter", 0, ".PostsPagePostFooter-footerSection{display:flex;font-size:1.4em;column-gap:20px;align-items:center}.PostsPagePostFooter-bookmarkButton{color:#757575;height:22px;margin-bottom:-5px}.PostsPagePostFooter-actions:hover,.PostsPagePostFooter-bookmarkButton:hover{opacity:.5}.PostsPagePostFooter-actions svg{color:inherit}.PostsPagePostFooter-voteBottom{display:inline-block;position:relative;font-size:42px;text-align:center;margin-left:auto;margin-right:auto;margin-bottom:40px}@media print{.PostsPagePostFooter-voteBottom{display:none}}.PostsPagePostFooter-secondaryInfoRight{flex:none;color:#757575;display:flex;column-gap:20px}.PostsPagePostFooter-bottomNavigation{width:640px;margin:auto}@media (max-width:959.95px){.PostsPagePostFooter-bottomNavigation{width:100%;max-width:720px}}.PostsPagePostFooter-lwVote{margin-top:66px;margin-bottom:70px}.PostsPagePostFooter-footerTagList{margin-top:16px;margin-bottom:66px}@media (min-width:600px){.PostsPagePostFooter-footerTagList{display:none}}");
            _embedStyles("PingbacksList", 0, ".PingbacksList-root{margin-top:16px;margin-bottom:32px}.PingbacksList-loadMore,.PingbacksList-title{display:inline-block;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;line-height:1rem;margin-bottom:-4px;font-variant-numeric:lining-nums}.PingbacksList-loadMore{color:#607e88}.PingbacksList-list{margin-top:8px}");
            _embedStyles("CommentsTableOfContents", 0, ".CommentsTableOfContents-root{color:rgba(0,0,0,.5)}.CommentsTableOfContents-root .TableOfContentsRow-title{border-bottom:none}.CommentsTableOfContents-comment{display:inline-flex}.CommentsTableOfContents-commentKarma{width:20px;text-align:right;margin-right:4px}.CommentsTableOfContents-collapseButtonWrapper{height:24px;margin-left:4px}.CommentsTableOfContents-collapseButton:hover{background:rgba(0,0,0,.1);border-radius:8px}.CommentsTableOfContents-collapseButtonCollapsed{transform:rotate(-90deg)}.CommentsTableOfContents-postTitle{color:rgba(0,0,0,.87);cursor:pointer;display:flex;font-size:1.3rem;min-height:76px;align-items:center;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;padding-top:16px;font-variant:small-caps;padding-bottom:16px;font-variant-numeric:lining-nums}.CommentsTableOfContents-tocPostedAt{color:#757575}.CommentsTableOfContents-highlightUnread{border-left:solid 3px #3f51b5;margin-left:-7px;padding-left:4px}body:has(.headroom--pinned) .CommentsTableOfContentsTitle,body:has(.headroom--unfixed) .CommentsTableOfContentsTitle{opacity:0}");
            _embedStyles("CommentsListSection", 0, ".CommentsListSection-root{margin:0 auto 15px;position:relative;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;font-variant-numeric:lining-nums}.CommentsListSection-maxWidthRoot{max-width:720px}.CommentsListSection-commentsHeadline{font-size:24px;font-weight:600;line-height:36px;margin-bottom:16px}.CommentsListSection-commentCount{color:#757575;margin-left:10px}.CommentsListSection-commentSorting{margin-right:12px}.CommentsListSection-clickToHighlightNewSince,.CommentsListSection-commentSorting{color:rgba(0,0,0,.54);display:inline}@media print{.CommentsListSection-clickToHighlightNewSince{display:none}}.CommentsListSection-button{color:#607e88}.CommentsListSection-newComment{border:1px solid rgba(72,94,144,.16);position:relative;border-radius:3px;margin-bottom:1.3em}@media print{.CommentsListSection-newComment{display:none}}.CommentsListSection-newQuickTake{border:0}.CommentsListSection-newCommentLabel,.CommentsListSection-newCommentSublabel{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding-left:12px;font-variant-numeric:lining-nums}.CommentsListSection-newCommentLabel{color:rgba(0,0,0,.87);font-size:14.3px;margin-top:12px;font-weight:600;line-height:19.5px}.CommentsListSection-newCommentSublabel{color:#757575;font-style:italic;margin-top:4px}");
            _embedStyles("CommentsNewForm", 0, ".CommentsNewForm-rootMinimalist .form-input{width:100%;margin:4px 0 0}.CommentsNewForm-rootMinimalist form{display:flex;flex-direction:row}.CommentsNewForm-rootQuickTakes .form-component-EditorFormComponent{padding:10px;background:#f5f5f5;border-top-left-radius:3px;border-top-right-radius:3px}.CommentsNewForm-loadingRoot{opacity:.5}.CommentsNewForm-form{padding:10px}.CommentsNewForm-formMinimalist{padding:12px 10px 8px}.CommentsNewForm-quickTakesForm{display:flex;flex-direction:column}.CommentsNewForm-modNote,.CommentsNewForm-rateLimitNote{color:#424242;padding-top:4px}.CommentsNewForm-moderationGuidelinesWrapper{background-color:rgba(0,0,0,.07)}");
            _embedStyles("CommentForm", 0, ".CommentForm-fieldWrapper{margin-top:16px;margin-bottom:16px}.CommentForm-cancelButton,.CommentForm-submitButton{font-size:16px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-left:5px;padding-bottom:2px}.CommentForm-submitButton{color:#3f51b5}.CommentForm-cancelButton:hover,.CommentForm-submitButton:hover{background:rgba(0,0,0,.05)}.CommentForm-cancelButton{color:rgba(0,0,0,.4)}.CommentForm-submitSegmented{border-top-right-radius:0;border-bottom-right-radius:0}");
            _embedStyles("FormErrors", 0, ".FormErrors-root{color:#bf360c;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif}");
            _embedStyles("EditorFormComponent", 0, ".EditorFormComponent-editor,.EditorFormComponent-root{position:relative}.EditorFormComponent-label{display:block;font-size:10px;margin-bottom:6px}.EditorFormComponent-sectionTitle{font-size:12px}.EditorFormComponent-markdownEditor{font-size:inherit;font-family:inherit}.EditorFormComponent-postBodyStyles{cursor:text;padding:0}.EditorFormComponent-answerStyles .spoiler:not(:hover) *,.EditorFormComponent-commentBodyStyles .spoiler:not(:hover) *,.EditorFormComponent-commentBodyStylesMinimalist .spoiler:not(:hover) *,.EditorFormComponent-postBodyStyles .spoiler:not(:hover) *{background-color:#000!important}.EditorFormComponent-answerStyles{cursor:text;max-width:620px}.EditorFormComponent-commentBodyStyles,.EditorFormComponent-commentBodyStylesMinimalist{cursor:text;padding:0;margin-top:0;margin-bottom:0;pointer-events:auto}.EditorFormComponent-commentBodyStylesMinimalist{line-height:1em}.EditorFormComponent-commentBodyStylesMinimalist textarea{margin-top:0;max-height:28px}.EditorFormComponent-commentBodyStylesMinimalist textarea:focus{max-height:128px}.EditorFormComponent-ckEditorStyles .ck{--ck-spacing-standard:8px;--ck-color-comment-marker:rgba(255,241,82,.5);--ck-color-base-background:#ffffff;--ck-color-comment-background:#f3f7fb;--ck-color-comment-marker-active:#fdf05d;--ck-color-annotation-wrapper-background:#ffffff;--ck-color-widget-editable-focus-background:#fff}.EditorFormComponent-ckEditorStyles .ck blockquote{margin:0;font-style:unset;border-left:solid 3px #e0e0e0;font-weight:400;padding:16px}.EditorFormComponent-ckEditorStyles .ck.ck-content{margin-left:-8px;--ck-focus-ring:1px solid rgba(0,0,0,0);--ck-inner-shadow:none;--ck-focus-outer-shadow:none;--ck-focus-outer-shadow-geometry:none}.EditorFormComponent-ckEditorStyles .ck .ck-placeholder:before{white-space:break-spaces}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar{--ck-color-comment-count:#3f51b5}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list li,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar li{font-size:unset;font-family:unset;font-weight:unset;line-height:unset;margin-bottom:unset}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment:after,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment:after{display:none}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment__input{margin-top:0;margin-bottom:12px}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-thread__comment-count{color:rgba(0,0,0,.87);font-size:14.3px;align-items:flex-start;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-time,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__main p{margin-top:0;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-time{color:rgba(0,0,0,.87);align-items:flex-start;line-height:19.5px;margin-bottom:12px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__main p,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-presence-list__counter{color:rgba(0,0,0,.87);font-size:14.3px;align-items:flex-start;line-height:19.5px;margin-bottom:12px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-presence-list__counter,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar.ck-presence-list{margin-top:0;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400}.EditorFormComponent-ckEditorStyles .ck.ck-sidebar.ck-presence-list{color:rgba(0,0,0,.87);font-size:14.3px;align-items:flex-start;line-height:19.5px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__input{margin-bottom:12px}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__comment-count{color:rgba(0,0,0,.87);font-size:14.3px;align-items:flex-start;line-height:19.5px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-time,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__main p,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__comment-count{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-time{color:rgba(0,0,0,.87);margin-top:0;align-items:flex-start;line-height:19.5px;margin-bottom:12px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__main p,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-presence-list__counter{color:rgba(0,0,0,.87);font-size:14.3px;margin-top:0;align-items:flex-start;line-height:19.5px;margin-bottom:12px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-content .image>figcaption,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-presence-list__counter,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list.ck-presence-list{font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list.ck-presence-list{color:rgba(0,0,0,.87);font-size:14.3px;margin-top:0;align-items:flex-start;line-height:19.5px;font-variant-numeric:lining-nums}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list.ck-presence-list,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar.ck-presence-list{margin-bottom:32px;--ck-user-avatar-size:20px}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__comment-count,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-thread__comment-count{color:#757575;margin:0;padding-left:16px;padding-bottom:.5em}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment__input{padding-left:16px}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__main,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__input,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__main,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment__input,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-thread__input{width:100%}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-comment__wrapper,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-comment__wrapper{border-top:1px solid rgba(0,0,0,.15)}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__info-time,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-name,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__info-time{color:#757575;font-size:13px}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-annotation__user,.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__user,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-annotation__user,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-thread__user{display:none}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list .ck-thread__comment-count:before,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar .ck-thread__comment-count:before{content:\"▶\"}.EditorFormComponent-ckEditorStyles .ck.ck-presence-list.ck-presence-list .ck-user,.EditorFormComponent-ckEditorStyles .ck.ck-sidebar.ck-presence-list .ck-user,style~.EditorFormComponent-ckEditorStyles .ck blockquote p{margin-top:0}.EditorFormComponent-ckEditorStyles .ck blockquote p,.EditorFormComponent-ckEditorStyles .ck.ck-content p{margin-top:1em;margin-bottom:1em}.EditorFormComponent-ckEditorStyles .ck.ck-content .table table{width:100%;border:1px double #b3b3b3;height:100%;margin:auto;text-align:left;border-spacing:0;border-collapse:collapse}.EditorFormComponent-ckEditorStyles .ck.ck-content .table table td,.EditorFormComponent-ckEditorStyles .ck.ck-content .table table th{border:1px double #d9d9d9;padding:.4em;min-width:2em;word-break:normal}.EditorFormComponent-ckEditorStyles .ck.ck-content .table table th{background:#fafafa;font-weight:700}.EditorFormComponent-ckEditorStyles .ck.ck-content .ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected{outline:0}.EditorFormComponent-ckEditorStyles .ck.ck-content .image>figcaption{color:rgba(0,0,0,.54);font-size:11.7px;line-height:1.375em;background-color:unset}.EditorFormComponent-ckEditorStyles .ck.ck-content hr{width:100%;border:0;height:100%;margin:32px 0;display:flex;background:0 0;text-align:center;align-items:center;justify-content:center}.EditorFormComponent-ckEditorStyles .ck.ck-content ol,.EditorFormComponent-ckEditorStyles .ck.ck-content ul{list-style-type:revert!important}.EditorFormComponent-ckEditorStyles .ck.ck-content ol>li>ol{list-style:lower-alpha!important}.EditorFormComponent-ckEditorStyles .ck.ck-content ol>li>ol>li>ol{list-style:lower-roman!important}.EditorFormComponent-ckEditorStyles .ck.ck-content .footnote-reference{scroll-margin-top:100px}.EditorFormComponent-ckEditorStyles .ck.ck-content hr:after{color:rgba(0,0,0,.26);content:\"•••\";font-size:13px;margin-left:12px;letter-spacing:12px}.EditorFormComponent-ckEditorStyles .ck.ck-content .table table td p,.EditorFormComponent-ckEditorStyles .ck.ck-content .table table th p{margin-top:.5em;margin-bottom:.5em}.EditorFormComponent-ckEditorStyles .ck.ck-content .table table td p:first-of-type,.EditorFormComponent-ckEditorStyles .ck.ck-content .table table th p:first-of-type,.EditorFormComponent-ckEditorStyles .ck.ck-content p:first-of-type{margin-top:0}.EditorFormComponent-ckEditorStyles .ck blockquote p:first-child{margin-top:0}.EditorFormComponent-ckEditorStyles .ck blockquote p:last-child,.EditorFormComponent-commentMinimalistEditorHeight .ck.ck-editor__editable_inline>:last-child{margin-bottom:0}.EditorFormComponent-ckEditorGrey{color:#000!important;width:100%;border:0;resize:none;padding:16px;font-size:14px;background:#efefef;font-weight:500;border-radius:0}.EditorFormComponent-ckEditorGrey::placeholder{color:#757575}.EditorFormComponent-ckEditorGrey:focus,.EditorFormComponent-ckEditorGrey:hover{background:#e4e4e4}.EditorFormComponent-questionWidth{width:640px}@media (max-width:959.95px){.EditorFormComponent-questionWidth{width:100%}}.EditorFormComponent-postEditorHeight,.EditorFormComponent-postEditorHeight .ck.ck-content{min-height:400px}.EditorFormComponent-postEditorHeight .ck-sidebar .ck-content{min-height:unset}.EditorFormComponent-commentEditorHeight,.EditorFormComponent-commentEditorHeight .ck.ck-content,.EditorFormComponent-quickTakesEditorHeight,.EditorFormComponent-quickTakesEditorHeight .ck.ck-content{min-height:100px}.EditorFormComponent-commentMinimalistEditorHeight .ck-editor__editable{max-height:300px}.EditorFormComponent-questionEditorHeight,.EditorFormComponent-questionEditorHeight .ck.ck-content{min-height:400px}.EditorFormComponent-maxHeight{max-height:calc(100vh - 450px);overflow-y:scroll}.EditorFormComponent-select{margin-right:12px}.EditorFormComponent-placeholder{top:0;color:#9e9e9e;position:absolute;white-space:pre-wrap;pointer-events:none}.EditorFormComponent-placeholder *{pointer-events:none}.EditorFormComponent-placeholderCollaborationSpacing{top:60px}.EditorFormComponent-changeDescriptionRow{display:flex;align-items:center}.EditorFormComponent-changeDescriptionLabel{color:rgba(0,0,0,.87);font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin-left:8px;margin-right:8px;font-variant-numeric:lining-nums}.EditorFormComponent-changeDescriptionInput{flex-grow:1}.EditorFormComponent-markdownImgErrText{color:#bf360c;margin:24px 0}.EditorFormComponent-enteredBotTips{opacity:1}");
            _embedStyles("Loading", 0, "@keyframes sk-bouncedelay{0%,80%,to{transform:scale(0)}40%{transform:scale(1)}}.Loading-spinner{height:10px;display:block;max-width:100px;text-align:center;margin-left:auto;margin-right:auto}.Loading-spinner div{width:10px;height:10px;display:inline-block;animation:sk-bouncedelay 1.4s infinite ease-in-out both;border-radius:100%;background-color:rgba(0,0,0,.55);-webkit-animation:sk-bouncedelay 1.4s infinite ease-in-out both}.Loading-whiteSpinner div{background-color:#fff}.Loading-bounce1{margin-right:5px;animation-delay:-.32s!important}.Loading-bounce2{margin-right:5px;animation-delay:-.16s!important}.Loading-bounce3{margin-right:0}");
            _embedStyles("EditorTypeSelect", 0, "");
            _embedStyles("CommentSubmit", 1, ".CommentSubmit-submit{display:flex;justify-content:end}.CommentSubmit-submitQuickTakes{padding:10px;background:#f5f5f5;border-bottom-left-radius:3px;border-bottom-right-radius:3px}.CommentSubmit-formButton{color:#607e88;font-size:16px;margin-left:5px}.CommentSubmit-formButton:hover{opacity:.5;background-color:none}.CommentSubmit-cancelButton{color:#bdbdbd}.CommentSubmit-submitMinimalist{height:fit-content;margin-top:auto;margin-bottom:4px}.CommentSubmit-formButtonMinimalist{color:#fff;padding:2px;font-size:16px;min-width:28px;min-height:28px;overflow-x:hidden;margin-left:5px;background-color:#607e88}.CommentSubmit-formButtonMinimalist:hover{opacity:.8;background-color:#607e88}.CommentSubmit-submitSegmented{border-top-right-radius:0;border-bottom-right-radius:0}.CommentSubmit-submitWrapper{display:flex}");
            _embedStyles("CommentsList", 0, ".CommentsList-button{color:#607e88}.CommentsList-commentsListLoadingSpacer{min-height:100vh}.CommentsList-expandOptions{clear:both;color:#757575;display:flex;font-size:14px;margin-top:24px;align-items:center;margin-bottom:10px;justify-content:space-between}@media (max-width:599.95px){.CommentsList-settingsLabel{display:none}}");
            _embedStyles("PostsPageCrosspostComments", 0, ".PostsPageCrosspostComments-root{border:1px solid rgba(72,94,144,.16);margin:0 auto 1.3em;display:flex;padding:12px;align-items:center;border-radius:3px}");
            _embedStyles("Row", 0, ".Row-root{display:flex;align-items:center}");
            _embedStyles("MetaInfo", 0, ".MetaInfo-root{color:#757575;display:inline;font-size:1rem;margin-right:8px}.MetaInfo-button{cursor:pointer}.MetaInfo-button:active,.MetaInfo-button:focus,.MetaInfo-button:hover{color:#bdbdbd}");
            _embedStyles("AFUnreviewedCommentCount", 0, ".AFUnreviewedCommentCount-root{display:flex;margin-top:32px;font-weight:400;justify-content:center}.AFUnreviewedCommentCount-viewLink{margin-left:4px}");
            _embedStyles("PostBottomRecommendations", 0, ".PostBottomRecommendations-root{padding:60px 0 80px;background:0 0;margin-top:60px}@media (max-width:959.95px){.PostBottomRecommendations-root{margin-left:-8px;margin-right:-8px;padding-left:8px;padding-right:8px}}.PostBottomRecommendations-section{margin:0 auto 60px;max-width:720px}.PostBottomRecommendations-sectionHeading{color:#000;font-size:20px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:600;margin-bottom:16px}.PostBottomRecommendations-largePostItem{margin-top:8px}.PostBottomRecommendations-viewMore{color:#757575;font-size:14px;margin-top:12px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:600}")
        </script>
        <div hidden id="S:1">
            <div>
                <div class="ReadingProgressBar-readingProgressBar"></div>
                <div class="MultiToCLayout-root">
                    <div class="MultiToCLayout-tableOfContents" style="grid-template-areas:&quot;... toc0 gap1 content0 gap2 rhs0 ...&quot;
&quot;... toc0 gap1 content1 gap2 rhs1 ...&quot;
&quot;... toc2 gap1 content2 gap2 rhs2 ...&quot;
&quot;... toc2 gap1 content3 gap2 rhs3 ...&quot;;grid-template-rows:min-content min-content min-content 1fr">
                        <div style="grid-area:toc0" class="MultiToCLayout-toc MultiToCLayout-normalHeaderToc">
                            <div class="MultiToCLayout-stickyBlockScroller MultiToCLayoutStickyBlockScroller">
                                <div class="MultiToCLayout-stickyBlock">
                                    <div></div>
                                </div>
                            </div>
                        </div>
                        <div class="MultiToCLayout-gap1"></div>
                        <div class="MultiToCLayout-content" style="grid-area:content0">
                            <div id="postBody" class="PostsPage-centralColumn PostsPage-postBody PostsPage-audioPlayerHidden">
                                <!--$-->
                                <!--/$-->
                                <div class="PostsPage-title">
                                    <div class="PostsPage-centralColumn">
                                        <div class="LWPostsPageHeader-root">
                                            <div>
                                                <span class="LWPostsPageHeader-topRight">
                                                    <div class="LWPostsPageHeaderTopRight-root">
                                                        <div class="LWPostsPageHeaderTopRight-tagList">
                                                            <span class="FooterTagList-root FooterTagList-alignRight">
                                                                <span class="">
                                                                    <span class="FooterTag-root FooterTag-noBackground">
                                                                        <a href="/w/careers">
                                                                            <span class="FooterTag-name">Careers</span>
                                                                        </a>
                                                                    </span>
                                                                </span>
                                                                <span class="">
                                                                    <span class="FooterTag-root FooterTag-noBackground">
                                                                        <a href="/w/interpretability-ml-and-ai">
                                                                            <span class="FooterTag-name">Interpretability (ML &amp;AI)</span>
                                                                        </a>
                                                                    </span>
                                                                </span>
                                                                <span class="">
                                                                    <span class="FooterTag-root FooterTag-noBackground">
                                                                        <a href="/w/scholarship-and-learning">
                                                                            <span class="FooterTag-name">Scholarship &amp;Learning</span>
                                                                        </a>
                                                                    </span>
                                                                </span>
                                                                <span class="">
                                                                    <span class="FooterTag-root FooterTag-noBackground">
                                                                        <a href="/w/practical">
                                                                            <span class="FooterTag-name">Practical</span>
                                                                        </a>
                                                                    </span>
                                                                </span>
                                                                <span class="">
                                                                    <span class="FooterTag-root FooterTag-noBackground">
                                                                        <a href="/w/ai">
                                                                            <span class="FooterTag-name">AI</span>
                                                                        </a>
                                                                    </span>
                                                                </span>
                                                                <span class="LWTooltip-root">
                                                                    <div class="FooterTagList-frontpageOrPersonal FooterTagList-noBackground FooterTagList-neverCoreStyling">Frontpage</div>
                                                                </span>
                                                            </span>
                                                        </div>
                                                        <div class="LWPostsPageHeaderTopRight-vote">
                                                            <div class="LWPostsPageTopHeaderVote-voteBlockHorizontal">
                                                                <span>
                                                                    <div class="LWPostsPageTopHeaderVote-upvoteHorizontal">
                                                                        <button class="VoteArrowIconSolid-root VoteButton-root VoteArrowIconSolid-up" type="button">
                                                                            <span class="VoteButton-inner">
                                                                                <svg width="9" height="6" viewBox="0 0 9 6" fill="currentColor" style="height:10px;width:10px" class="VoteArrowIconSolid-smallArrow">
                                                                                    <path d="M4.11427 0.967669C4.31426 0.725192 4.68574 0.725192 4.88573 0.967669L8.15534 4.93186C8.42431 5.25798 8.19234 5.75 7.76961 5.75H1.23039C0.807659 5.75 0.575686 5.25798 0.844665 4.93186L4.11427 0.967669Z"></path>
                                                                                </svg>
                                                                                <svg width="9" height="9" viewBox="0 0 9 6" fill="currentColor" class="VoteArrowIconSolid-bigArrow VoteArrowIconSolid-bigArrowSolid">
                                                                                    <path d="m4.499308,0.51796875 c-0.142868,0 -0.284967,0.059816 -0.384961,0.1810547 L0.84481497,4.6646484 c-0.268978,0.32612 -0.03777,0.8173828 0.38496103,0.8173828 H1.999698 L4.114347,2.9173828 c0.199989,-0.242477 0.571689,-0.242477 0.771679,0 l2.114649,2.5646484 h0.768164 c0.42273,0 0.65569,-0.4912628 0.386719,-0.8173828 L4.886026,0.69902345 C4.786031,0.57778495 4.642175,0.51796875 4.499308,0.51796875 Z"></path>
                                                                                </svg>
                                                                            </span>
                                                                        </button>
                                                                    </div>
                                                                </span>
                                                                <div class="LWPostsPageTopHeaderVote-voteScoresHorizontal">
                                                                    <span>
                                                                        <div>
                                                                            <h1 class="Typography-root Typography-headline LWPostsPageTopHeaderVote-voteScore">29</h1>
                                                                        </div>
                                                                    </span>
                                                                </div>
                                                                <span>
                                                                    <div class="LWPostsPageTopHeaderVote-downvoteHorizontal">
                                                                        <button class="VoteArrowIconSolid-root VoteButton-root VoteArrowIconSolid-down" type="button">
                                                                            <span class="VoteButton-inner">
                                                                                <svg width="9" height="6" viewBox="0 0 9 6" fill="currentColor" style="height:10px;width:10px" class="VoteArrowIconSolid-smallArrow">
                                                                                    <path d="M4.11427 0.967669C4.31426 0.725192 4.68574 0.725192 4.88573 0.967669L8.15534 4.93186C8.42431 5.25798 8.19234 5.75 7.76961 5.75H1.23039C0.807659 5.75 0.575686 5.25798 0.844665 4.93186L4.11427 0.967669Z"></path>
                                                                                </svg>
                                                                                <svg width="9" height="9" viewBox="0 0 9 6" fill="currentColor" class="VoteArrowIconSolid-bigArrow VoteArrowIconSolid-bigArrowSolid">
                                                                                    <path d="m4.499308,0.51796875 c-0.142868,0 -0.284967,0.059816 -0.384961,0.1810547 L0.84481497,4.6646484 c-0.268978,0.32612 -0.03777,0.8173828 0.38496103,0.8173828 H1.999698 L4.114347,2.9173828 c0.199989,-0.242477 0.571689,-0.242477 0.771679,0 l2.114649,2.5646484 h0.768164 c0.42273,0 0.65569,-0.4912628 0.386719,-0.8173828 L4.886026,0.69902345 C4.786031,0.57778495 4.642175,0.51796875 4.499308,0.51796875 Z"></path>
                                                                                </svg>
                                                                            </span>
                                                                        </button>
                                                                    </div>
                                                                </span>
                                                            </div>
                                                        </div>
                                                        <div class="PostActionsButton-root LWPostsPageHeaderTopRight-postActionsButton">
                                                            <div>
                                                                <svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path>
                                                                </svg>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </span>
                                                <span class="LWPostsPageHeader-audioPlayerWrapper"></span>
                                            </div>
                                            <div class="LWPostsPageHeader-titleSection">
                                                <div class="LWPostsPageHeader-title">
                                                    <div>
                                                        <h1 class="Typography-root Typography-display3 PostsPageTitle-root">
                                                            <a class="PostsPageTitle-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher">
                                                                How To Become A Mechanistic Interpretability
                                                                <!-- -->
                                                                <span class="PostsPageTitle-lastWord">Researcher</span>
                                                            </a>
                                                        </h1>
                                                    </div>
                                                    <div class="LWPostsPageHeader-authorAndSecondaryInfo">
                                                        <div class="LWPostsPageHeader-authorInfo">
                                                            <span class="Typography-root Typography-body1 PostsAuthors-root">
                                                                by 
                                                                <span class="PostsAuthors-authorName">
                                                                    <span>
                                                                        <span class="">
                                                                            <a class="UsersNameDisplay-noColor" href="/users/neel-nanda-1?from=post_header">Neel Nanda</a>
                                                                        </span>
                                                                    </span>
                                                                </span>
                                                            </span>
                                                        </div>
                                                        <div class="LWPostsPageHeader-date">
                                                            <span class="LWTooltip-root">
                                                                <span class="PostsPageDate-date">
                                                                    <time dateTime="2025-09-02T23:38:43.780Z">2nd Sep 2025</time>
                                                                </span>
                                                            </span>
                                                        </div>
                                                        <div class="LWPostsPageHeader-mobileButtons">
                                                            <div class="LWPostsPageHeader-readTime">
                                                                <span class="LWTooltip-root">
                                                                    <span class="ReadTime-root">66
                                                                    <!-- -->
                                                                    min read</span>
                                                                </span>
                                                            </div>
                                                            <div class="LWCommentCount-root">
                                                                <a class="LWCommentCount-comments LWCommentCount-wideClickTarget" href="#comments">
                                                                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="LWCommentCount-commentsIcon ForumIcon-root">
                                                                        <path stroke-linecap="round" stroke-linejoin="round" d="M2.25 12.76c0 1.6 1.123 2.994 2.707 3.227 1.087.16 2.185.283 3.293.369V21l4.076-4.076a1.526 1.526 0 011.037-.443 48.282 48.282 0 005.68-.494c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0012 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741v6.018z"></path>
                                                                    </svg>
                                                                    10
                                                                </a>
                                                            </div>
                                                            <div class="LWPostsPageHeader-audioToggle"></div>
                                                            <div class="PostActionsButton-root">
                                                                <div>
                                                                    <svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                        <path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path>
                                                                    </svg>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                                <div class="LWPostsPageHeader-mobileHeaderVote">
                                                    <div class="PostsVoteDefault-voteBlock">
                                                        <div class="PostsVoteDefault-upvote">
                                                            <button class="VoteButton-root VoteArrowIconHollow-up" type="button">
                                                                <span class="VoteButton-inner">
                                                                    <svg class="MuiSvgIcon-root VoteArrowIconHollow-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    </svg>
                                                                    <svg class="MuiSvgIcon-root VoteArrowIconHollow-bigArrow VoteArrowIconHollow-exited VoteButton-colorLightSecondary" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    </svg>
                                                                </span>
                                                            </button>
                                                        </div>
                                                        <div class="PostsVoteDefault-voteScores">
                                                            <span>
                                                                <h1 class="Typography-root Typography-headline PostsVoteDefault-voteScore">29</h1>
                                                            </span>
                                                        </div>
                                                        <div class="PostsVoteDefault-downvote">
                                                            <button class="VoteButton-root VoteArrowIconHollow-down" type="button">
                                                                <span class="VoteButton-inner">
                                                                    <svg class="MuiSvgIcon-root VoteArrowIconHollow-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    </svg>
                                                                    <svg class="MuiSvgIcon-root VoteArrowIconHollow-bigArrow VoteArrowIconHollow-exited VoteButton-colorLightError" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    </svg>
                                                                </span>
                                                            </button>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <!--$?-->
                                            <template id="B:2"></template>
                                            <!--/$-->
                                        </div>
                                    </div>
                                </div>
                                <div class="PostsPage-postContent instapaper_body ContentStyles-base content ContentStyles-postBody">
                                    <div class="commentOnSelection">
                                        <div id="postContent">
                                            <template id="P:3"></template>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="MultiToCLayout-rhs" style="grid-area:rhs0">
                            <div class="PostsPage-reserveSpaceForSidenotes"></div>
                            <div class="SideItems-sidebar"></div>
                        </div>
                        <div class="MultiToCLayout-gap1"></div>
                        <div class="MultiToCLayout-content" style="grid-area:content1">
                            <div class="PostsPage-centralColumn PostsPage-betweenPostAndComments">
                                <!--$-->
                                <div class="PostsPagePostFooter-footerTagList">
                                    <span class="FooterTagList-root">
                                        <span class="">
                                            <span class="FooterTag-root">
                                                <a href="/w/careers">
                                                    <span class="FooterTag-name">Careers</span>
                                                </a>
                                            </span>
                                        </span>
                                        <span class="">
                                            <span class="FooterTag-root">
                                                <a href="/w/interpretability-ml-and-ai">
                                                    <span class="FooterTag-name">Interpretability (ML &amp;AI)</span>
                                                </a>
                                            </span>
                                        </span>
                                        <span class="">
                                            <span class="FooterTag-root">
                                                <a href="/w/scholarship-and-learning">
                                                    <span class="FooterTag-name">Scholarship &amp;Learning</span>
                                                </a>
                                            </span>
                                        </span>
                                        <span class="">
                                            <span class="FooterTag-root FooterTag-core">
                                                <a href="/w/practical">
                                                    <span class="FooterTag-name">Practical</span>
                                                </a>
                                            </span>
                                        </span>
                                        <span class="">
                                            <span class="FooterTag-root FooterTag-core">
                                                <a href="/w/ai">
                                                    <span class="FooterTag-name">AI</span>
                                                </a>
                                            </span>
                                        </span>
                                        <span class="LWTooltip-root">
                                            <div class="FooterTagList-frontpageOrPersonal">Frontpage</div>
                                        </span>
                                    </span>
                                </div>
                                <!--/$-->
                                <!--$?-->
                                <template id="B:4"></template>
                                <!--/$-->
                            </div>
                        </div>
                        <div style="grid-area:toc2" class="MultiToCLayout-toc MultiToCLayout-commentToCMargin MultiToCLayout-normalHeaderToc">
                            <div class="MultiToCLayout-stickyBlockScroller MultiToCLayoutStickyBlockScroller MultiToCLayout-commentToCIntersection">
                                <div class="MultiToCLayout-stickyBlock"></div>
                            </div>
                        </div>
                        <div class="MultiToCLayout-gap1"></div>
                        <div class="MultiToCLayout-content" style="grid-area:content2">
                            <span>
                                <span>
                                    <div class="PostsPage-commentsSection">
                                        <div class="CommentsListSection-root CommentsListSection-maxWidthRoot">
                                            <div id="comments"></div>
                                            <div id="posts-thread-new-comment" class="CommentsListSection-newComment">
                                                <div class="CommentsListSection-newCommentLabel">New Comment</div>
                                                <div class="CommentsNewForm-root">
                                                    <div class="CommentsNewForm-form">
                                                        <div>
                                                            <form class="vulcan-form">
                                                                <div class="FormErrors-root form-errors"></div>
                                                                <div class="form-input form-component-EditorFormComponent CommentForm-fieldWrapper">
                                                                    <div class="EditorFormComponent-root">
                                                                        <div>
                                                                            <div class="EditorFormComponent-editor EditorFormComponent-commentBodyStyles ContentStyles-base content ContentStyles-commentBody">
                                                                                <div class="Loading-spinner">
                                                                                    <div class="Loading-bounce1"></div>
                                                                                    <div class="Loading-bounce2"></div>
                                                                                    <div class="Loading-bounce3"></div>
                                                                                </div>
                                                                            </div>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentSubmit-submit">
                                                                    <div class="CommentSubmit-submitWrapper">
                                                                        <button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat CommentSubmit-formButton CommentSubmit-submitButton" type="submit" id="new-comment-submit">
                                                                            <span class="MuiButton-label">Submit</span>
                                                                            <span class="MuiTouchRipple-root"></span>
                                                                        </button>
                                                                    </div>
                                                                </div>
                                                            </form>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class=""></div>
                                            <div>
                                                <div class="Loading-spinner">
                                                    <div class="Loading-bounce1"></div>
                                                    <div class="Loading-bounce2"></div>
                                                    <div class="Loading-bounce3"></div>
                                                </div>
                                            </div>
                                            <div class="Row-root" style="justify-content:flex-end;align-items:center">
                                                <span class="LWTooltip-root">
                                                    <a href="/moderation">
                                                        <span class="Typography-root Typography-body2 MetaInfo-root">Moderation Log</span>
                                                    </a>
                                                </span>
                                            </div>
                                        </div>
                                        <template id="P:5"></template>
                                    </div>
                                </span>
                            </span>
                        </div>
                        <div class="MultiToCLayout-gap1"></div>
                        <div class="MultiToCLayout-content" style="grid-area:content3">
                            <div class="PostBottomRecommendations-root">
                                <div>
                                    <div class="PostBottomRecommendations-section">
                                        <div class="PostBottomRecommendations-sectionHeading">
                                            More from
                                            <!-- -->
                                            <span class="">
                                                <a href="/users/neel-nanda-1">Neel Nanda</a>
                                            </span>
                                        </div>
                                        <div class="Loading-spinner">
                                            <div class="Loading-bounce1"></div>
                                            <div class="Loading-bounce2"></div>
                                            <div class="Loading-bounce3"></div>
                                        </div>
                                        <div class="PostBottomRecommendations-viewMore">
                                            <a href="/users/neel-nanda-1">View more</a>
                                        </div>
                                    </div>
                                    <div class="PostBottomRecommendations-section">
                                        <div class="PostBottomRecommendations-sectionHeading">Curated and popular this week</div>
                                        <div class="Loading-spinner">
                                            <div class="Loading-bounce1"></div>
                                            <div class="Loading-bounce2"></div>
                                            <div class="Loading-bounce3"></div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="MultiToCLayout-tocFooter">
                        <div class="LWCommentCount-root">
                            <a class="LWCommentCount-comments LWCommentCount-wideClickTarget" href="#comments">
                                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="LWCommentCount-commentsIcon ForumIcon-root">
                                    <path stroke-linecap="round" stroke-linejoin="round" d="M2.25 12.76c0 1.6 1.123 2.994 2.707 3.227 1.087.16 2.185.283 3.293.369V21l4.076-4.076a1.526 1.526 0 011.037-.443 48.282 48.282 0 005.68-.494c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0012 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741v6.018z"></path>
                                </svg>
                                0<span class="LWCommentCount-commentsLabel ToCRowHover LWCommentCount-rowOpacity">Comments</span>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div hidden id="S:2"></div>
        <script>
            $RC("B:2", "S:2")
        </script>
        <div hidden id="S:5"></div>
        <script>
            $RS = function(a, b) {
                a = document.getElementById(a);
                b = document.getElementById(b);
                for (a.parentNode.removeChild(a); a.firstChild; )
                    b.parentNode.insertBefore(a.firstChild, b);
                b.parentNode.removeChild(b)
            }
            ;
            $RS("S:5", "P:5")
        </script>
        <div hidden id="S:4"></div>
        <script>
            $RC("B:4", "S:4")
        </script>
        <script>
            _embedStyles("InlineReactSelectionWrapper", 0, ".InlineReactSelectionWrapper-popper{height:0}.InlineReactSelectionWrapper-button{left:12px;z-index:1000;position:relative}");
            _embedStyles("GlossarySidebar", 0, ".GlossarySidebar-glossaryAnchor{top:-30px;position:relative}.GlossarySidebar-jargonTerm{color:#424242;cursor:pointer;font-size:1.1rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;padding-top:2px;text-transform:capitalize;font-variant-numeric:lining-nums}.GlossarySidebar-glossaryContainer{width:fit-content;padding:12px;overflow:hidden;max-height:170px;border-radius:3px}@media (max-width:959.95px){.GlossarySidebar-glossaryContainer{display:none}}.GlossarySidebar-glossaryContainer .JargonTooltip-jargonWord:after{display:none}.GlossarySidebar-glossaryContainer:hover{max-height:unset}.GlossarySidebar-glossaryContainer:hover+.GlossarySidebar-overflowFade{opacity:0}.GlossarySidebar-glossaryContainer:hover .GlossarySidebar-pinIcon,.GlossarySidebar-glossaryContainer:hover .GlossarySidebar-showAllTermsButton,.GlossarySidebar-glossaryContainer:hover .GlossarySidebar-unapprovedTermsCount{opacity:.7}.GlossarySidebar-glossaryContainer:hover .GlossarySidebar-pinIcon{color:#a8742a}.GlossarySidebar-glossaryContainer:hover .GlossarySidebar-pinnedPinIcon{color:#a8742a;filter:brightness(.93);opacity:1}.GlossarySidebar-glossaryContainerClickTarget{cursor:pointer}.GlossarySidebar-outerContainer{height:0}.GlossarySidebar-innerContainer{height:var(--sidebar-column-remaining-height)}.GlossarySidebar-displayedHeightGlossaryContainer{z-index:1;padding-top:30px;padding-bottom:20px;background-color:#fff}@media (max-width:1279.95px){.GlossarySidebar-displayedHeightGlossaryContainer{display:none}}.GlossarySidebar-pinnedGlossaryContainer{top:100px;position:sticky}.GlossarySidebar-pinnedGlossaryContainer .GlossarySidebar-pinIcon{opacity:.5}.GlossarySidebar-pinnedGlossaryContainer .GlossarySidebar-pinnedPinIcon,.GlossarySidebar-pinnedGlossaryContainer .GlossarySidebar-unapprovedTermsCount{opacity:.85}.GlossarySidebar-titleRow{display:flex;align-items:flex-start;margin-bottom:12px}.GlossarySidebar-titleRowTooltipPopper{margin-bottom:12px}.GlossarySidebar-title{color:rgba(0,0,0,.87);font-size:14.3px;margin-top:0;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:600;line-height:19.5px;margin-right:6px;margin-bottom:0;font-variant-numeric:lining-nums}.GlossarySidebar-pinIcon{color:#757575;cursor:pointer;opacity:0;font-size:18px;margin-top:-1px;margin-right:8px}.GlossarySidebar-pinIcon:hover,.GlossarySidebar-pinnedPinIcon:hover{opacity:.7}.GlossarySidebar-unapprovedPinIcon{opacity:.4}.GlossarySidebar-pinnedPinIcon{color:#212121;display:block;opacity:.5}.GlossarySidebar-termTooltip{margin-right:5px}.GlossarySidebar-overflowFade{top:160px;width:100%;height:40px;opacity:1;position:absolute;background:linear-gradient(0deg,#fff,transparent);pointer-events:none}.GlossarySidebar-unapproved{color:#bdbdbd}.GlossarySidebar-showAllTermsButton{color:#bdbdbd;cursor:pointer;font-size:1rem;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding-top:8px;padding-bottom:8px;font-variant-numeric:lining-nums}.GlossarySidebar-showAllTermsTooltipPopper{max-width:200px}.GlossarySidebar-unapprovedTermsCount{color:rgba(0,0,0,.87);opacity:.5;font-size:1rem;margin:-3px 0 0;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px}");
            _embedStyles("DefaultPreview", 0, ".DefaultPreview-hovercard{color:#757575;overflow:hidden;font-size:1.1rem;max-width:500px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;white-space:nowrap;padding:8px 12px;text-overflow:ellipsis;font-variant-numeric:lining-nums}");
            _embedStyles("LinkStyles", 0, ".LinkStyles-link:after{top:-7px;width:4px;border:1.2px solid #3f51b5;height:4px;content:\"\";display:inline-block;position:relative;background:#f8f8f8;margin-left:2px;margin-right:0;border-radius:50%}.LinkStyles-link.visited:after,.LinkStyles-link:visited:after{border:1.2px solid #8c4298;background:#8c4298}.LinkStyles-redLink{color:#bf360c!important}.LinkStyles-redLink.visited:after,.LinkStyles-redLink:after,.LinkStyles-redLink:visited:after{border:1.2px solid #bf360c}.LinkStyles-owidIframe{width:600px;border:0;height:375px;max-width:100vw}.LinkStyles-metaculusIframe{width:400px;border:0;height:250px;max-width:100vw}.LinkStyles-metaculusBackground{background-color:#2c3947}.LinkStyles-fatebookIframe{width:560px;border:0;height:200px;max-width:100vw;box-shadow:0 4px 8px rgba(0,0,0,.12);border-radius:3px;background-color:#fff}.LinkStyles-manifoldIframe{width:560px;border:0;height:405px;max-width:100vw}.LinkStyles-neuronpediaIframe{width:100%;border:1px solid;height:360px;max-width:639px;border-color:#e0e0e0;border-radius:6px}.LinkStyles-estimakerIframe,.LinkStyles-metaforecastIframe,.LinkStyles-viewpointsIframe{width:560px;border:0;height:405px;max-width:100vw}.LinkStyles-viewpointsIframe{height:300px}");
            _embedStyles("FootnotePreview", 0, ".FootnotePreview-hovercard{color:#424242;padding:16px;font-size:1.1rem;max-width:500px;font-family:GreekFallback,\"freight-sans-pro\",Frutiger,\"Frutiger Linotype\",Univers,Calibri,\"Gill Sans\",\"Gill Sans MT\",Myriad Pro,Myriad,\"Liberation Sans\",\"Nimbus Sans L\",Tahoma,Geneva,\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:400;line-height:19.5px;font-variant-numeric:lining-nums}.FootnotePreview-hovercard a{color:#3f51b5}.FootnotePreview-hovercard .footnote-back-link,.FootnotePreview-sidenote .footnote-back-link,.FootnotePreview-sidenote a[href^=\"#fnref\"]{display:none}.FootnotePreview-anchorHover{border:2px solid #303f9f;margin:-2px;border-radius:2px}.FootnotePreview-sidenote{width:300px;padding:12px}@media (max-width:1279.95px){.FootnotePreview-sidenote{display:none}}.FootnotePreview-sidenote .footnote-content{width:auto!important;max-width:100%}.FootnotePreview-sidenote .footnote-content ol:first-child,.FootnotePreview-sidenote .footnote-content ul:first-child{margin-top:0}.FootnotePreview-footnoteMobileIndicator{display:none}@media (max-width:1279.95px){.FootnotePreview-footnoteMobileIndicator{display:inline-block}}.FootnotePreview-lineColor{background:rgba(0,0,0,.4)}.FootnotePreview-sidenoteWithIndex{display:flex}.FootnotePreview-sidenoteContent,.FootnotePreview-sidenoteIndex{color:rgba(0,0,0,.65);display:inline-block;font-size:15px;line-height:19px;vertical-align:top}.FootnotePreview-sidenoteIndex{white-space:nowrap;margin-right:0}.FootnotePreview-sidenoteHover .FootnotePreview-sidenoteContent,.FootnotePreview-sidenoteHover .FootnotePreview-sidenoteIndex{color:rgba(0,0,0,.87)}.FootnotePreview-sidenoteContent{overflow:hidden;position:relative;max-width:100%;max-height:200px}.FootnotePreview-sidenoteContent li{font-size:.9em;line-height:1.4em}.FootnotePreview-overflowFade{top:160px;width:100%;height:40px;position:absolute;background:linear-gradient(0deg,#fff,transparent)}.FootnotePreview-sidenoteHover{background:rgba(190,120,80,.05);border-radius:3px}.FootnotePreview-sidenoteHover .FootnotePreview-sidenoteContent{max-height:unset}.FootnotePreview-sidenoteHover .FootnotePreview-overflowFade{display:none}")
        </script>
        <div hidden id="S:3">
            <div>
                <div>
                    <p id="block0">
                        <strong>Note</strong>
                        : If you’ll forgive the shameless self-promotion, <strong>applications for </strong>
                        <span>
                            <span>
                                <a href="http://tinyurl.com/neel-mats-app">
                                    <strong>my MATS stream</strong>
                                </a>
                            </span>
                        </span>
                        <strong>are open until</strong>
                        <strong>Sept 12</strong>
                        . I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.
                    </p>
                    <p id="block1">
                        <i>Last updated Sept 2 2025</i>
                    </p>
                    <h2 id="TL_DR" data-internal-id="TL_DR">
                        <span id="TL_DR">TL;DR</span>
                    </h2>
                    <ul>
                        <li id="block2">
                            This post is about the mindset and process I recommend if you want to <i>do</i>
                            mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.
                            <ul>
                                <li id="block3">Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.</li>
                                <li id="block4">
                                    <strong>Learn the minimum viable basics, then do research.</strong>
                                    Mech interp is an empirical science
                                </li>
                            </ul>
                        </li>
                        <li id="block5">
                            Three stages:
                            <ul>
                                <li id="block6">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_1__Learning_the_Ropes">
                                        <strong>Learn the ropes</strong>
                                    </a>
                                    <strong>(≤1 month)</strong>
                                    learn the essentials, go breadth-first;
                                </li>
                                <li id="block7">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_2__Practicing_Research_with_Mini_Projects">
                                        <strong>Learn with research mini-projects</strong>
                                    </a>
                                    practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;
                                </li>
                                <li id="block8">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_3__Working_Up_To_Full_Research_Projects">
                                        <strong>Work up to full projects</strong>
                                    </a>
                                    , do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.
                                </li>
                            </ul>
                        </li>
                        <li id="block9">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_1__Learning_the_Ropes">
                                <strong>Stage 1:</strong>
                            </a>
                            <strong>Learning the Ropes</strong>
                            <ul>
                                <li id="block10">
                                    <strong>Breadth over depth; get a good baseline not perfection</strong>
                                </li>
                                <li id="block11">
                                    <strong>Learn the basics</strong>
                                    : <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Machine_Learning___Transformer_Basics">Code a transformer from scratch</a>
                                    , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Mechanistic_Interpretability_Techniques">key mech interp techniques</a>
                                    , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Using_LLMs_for_Learning">the landscape of the field</a>
                                    , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Machine_Learning___Transformer_Basics">linear algebra intuitions</a>
                                    , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Mechanistic_Interpretability_Coding___Tooling">how to write mech interp code</a>
                                    (
                                    <span>
                                        <span>
                                            <a href="https://arena-chapter1-transformer-interp.streamlit.app/">ARENA is your friend</a>
                                        </span>
                                    </span>
                                    )
                                </li>
                                <li id="block12">
                                    <strong>Get your hands dirty</strong>
                                    : Do <i>not</i>
                                    just read things. Mech interp is a fundamentally empirical science
                                </li>
                                <li id="block13">
                                    <strong>Move on after a month</strong>
                                    . Don’t expect to feel “done” or to have covered <i>all </i>
                                    of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real
                                </li>
                                <li id="block14">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Using_LLMs_for_Learning">
                                        <strong>Use LLMs extensively</strong>
                                    </a>
                                    - they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)
                                </li>
                            </ul>
                        </li>
                        <li id="block15">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#The_Big_Picture__Learning_the_Craft_of_Research">
                                <strong>Unpacking the research process</strong>
                            </a>
                            :
                            <ul>
                                <li id="block16">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Unpacking_the_Research_Process">Many skills</a>
                                    , categorise them by the feedback loops.
                                    <ul>
                                        <li id="block17">Fast skills (minutes-hours) like write/run/debug experiments</li>
                                        <li id="block18">Slow (weeks) like how to prioritise and when to pivot</li>
                                        <li id="block19">Very slow (months) like generating good research ideas</li>
                                    </ul>
                                </li>
                                <li id="block20">
                                    <strong>Do </strong>
                                    <i>
                                        <strong>not</strong>
                                    </i>
                                    <strong>try to learn all skills at once</strong>
                                    . Focus on fast/medium skills first, then slowly expand
                                </li>
                                <li id="block21">
                                    <a class="LinkStyles-link" href="/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand">4 phases of research</a>
                                    : finding an idea (<strong>ideation</strong>
                                    ) -&gt;building intuition and hunches (<strong>exploration</strong>
                                    ) -&gt;testing hypotheses (<strong>understanding</strong>
                                    ) -&gt;refining and writing up (<strong>distillation</strong>
                                    )
                                </li>
                            </ul>
                        </li>
                        <li id="block22">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_2__Practicing_Research_with_Mini_Projects">
                                <strong>Stage 2:</strong>
                            </a>
                            <strong>Mini projects</strong>
                            (1-5 days each for 2-4 weeks)
                            <ul>
                                <li id="block23">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Practicing_Exploration">Exploration mindset</a>
                                    : <strong>Maximise information gain per unit time</strong>
                                    , learn how to get unstuck. You don &#x27;t need a plan, so long as you &#x27;re learning
                                </li>
                                <li id="block24">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Practicing_Understanding">Understanding mindset</a>
                                    : <strong>Every research result is false until proven otherwise</strong>
                                    . The more exciting a result is, the more likely it is to be false. Be your own greatest critic
                                </li>
                                <li id="block25">
                                    Idea quality (ideation) and write-ups (distillation) aren &#x27;t the priority yet; <strong>taste and prioritization are learned by doing things</strong>
                                    .
                                </li>
                                <li id="block26">
                                    Having good research ideas takes forever to learn, <strong>to choose early projects, cheat</strong>
                                    ! <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Choose_A_Project">Pick well scoped projects</a>
                                    , eg extending a paper (ideas)
                                </li>
                                <li id="block27">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Using_LLMs_for_Research_Code">
                                        <strong>Use LLMs extensively</strong>
                                    </a>
                                    <strong></strong>
                                    - they should speed up your research/coding a <i>lot</i>
                                    (if you know how to use them properly!)
                                </li>
                            </ul>
                        </li>
                        <li id="block28">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_3__Working_Up_To_Full_Research_Projects">
                                <strong>Stage 3:</strong>
                            </a>
                            <strong>Towards full projects</strong>
                            <ul>
                                <li id="block29">
                                    <strong>Work in 1-2 week sprints</strong>
                                    , post-mortem after each, pivot to another project unless it &#x27;s going <i>great</i>
                                </li>
                                <li id="block30">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Deepening_Your_Skills">
                                        <strong>Slower skills</strong>
                                    </a>
                                    <strong>and </strong>
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Key_Research_Mindsets">
                                        <strong>key mindsets</strong>
                                    </a>
                                    : careful skepticism, awareness of the literature, prioritization, high productivity
                                </li>
                                <li id="block31">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Doing_Good_Science">
                                        <strong>Do good science</strong>
                                    </a>
                                    <strong>, not flashy science</strong>
                                    - be honest about limitations, give proof you &#x27;re not cherry picking, read your data, do the simple things that work, use real baselines.
                                </li>
                                <li id="block32">
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Write_up_your_work_">
                                        <strong>Write-up</strong>
                                    </a>
                                    <strong>your work</strong>
                                    ! Distill it into a narrative, then iteratively expand it to a write-up
                                    <ul>
                                        <li id="block33">
                                            <strong>Good public work is </strong>
                                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Why_aim_for_public_output_">
                                                <strong>your best credential</strong>
                                            </a>
                                            - for careers, PhDs, finding mentors, etc
                                        </li>
                                        <li id="block34">
                                            <strong>Writing is not an afterthought</strong>
                                            - make time for it. <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Common_mistakes">The reader will understand less than you think</a>
                                        </li>
                                    </ul>
                                </li>
                                <li id="block35">
                                    <strong>Practice </strong>
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Practicing_Ideation">
                                        <strong>generating research ideas</strong>
                                    </a>
                                    . If possible, try to imitation learn <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Research_Taste_Exercises">a mentor &#x27;s research taste.</a>
                                    <ul>
                                        <li id="block36">
                                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Avoiding_Fads">Avoid fads</a>
                                            , and think about <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#What_s_New_In_Mech_Interp_">what’s new and exciting in mech interp</a>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block37">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Advice_on_finding_a_mentor">
                                <strong>Proactively reach out to mentors</strong>
                            </a>
                            Everything is <i>much</i>
                            easier with a good mentor. Cold email, apply for mentoring programs, etc.
                            <ul>
                                <li id="block38">Reach out to researchers who &#x27;ll have time, not the most famous</li>
                            </ul>
                        </li>
                        <li id="block39">
                            <strong>Careers:</strong>
                            If you want to work in the field, apply for things! <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Where_to_apply">Jobs</a>
                            , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Mentoring_programs">mentoring programs</a>
                            , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Applying_for_grants">funding</a>
                            , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Relevant_Academic_Labs">academic labs</a>
                            .
                            <ul>
                                <li id="block40">
                                    Bonus thoughts: <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#What_do_hiring_managers_look_for">what do hiring managers look for</a>
                                    , <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#So_what_does_a_research_mentor_actually_do_">what does a good research mentor actually do</a>
                                    , and <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Should_you_do_a_PhD_">should you do a PhD</a>
                                    ?
                                </li>
                            </ul>
                        </li>
                        <li id="block41">
                            I also give various thoughts on how I &#x27;m thinking about the field nowadays, and what I’ve changed my mind about. I separate these from the practical advice, so you can take it or leave it.
                            <ul>
                                <li id="block42">
                                    Covering: <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Interlude__What_is_mech_interp_">how I currently define the field</a>
                                    , why I &#x27;m <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#A_Pragmatic_Vision_for_Mech_Interp">pessimistic on ambitious reverse engineering, and excited about more pragmatic approaches</a>
                                    , 
                                    <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#What_s_New_In_Mech_Interp_">
                                        what recent work I am<i></i>
                                        excited about
                                    </a>
                                    and recommend building on.
                                </li>
                                <li id="block43">
                                    And if any of that worldview appeals, you may want to apply to work with me via 
                                    <span>
                                        <span>
                                            <a href="http://tinyurl.com/neel-mats-app">MATS, due Sept 12</a>
                                        </span>
                                    </span>
                                    !
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <h2 id="Introduction" data-internal-id="Introduction">
                        <span id="Introduction">Introduction</span>
                    </h2>
                    <p id="block44">Mechanistic interpretability (mech interp) is, in my incredibly biased opinion, one of the most exciting research areas out there. We have these incredibly complex AI models that we don &#x27;t understand, yet there are tantalizing signs of real structure inside them. Even partial understanding of this structure opens up a world of possibilities, yet is neglected by 99% of machine learning researchers. There’s so much to do!</p>
                    <p id="block45">I think mech interp is an unusually easy field to learn about on your own: there’s a lot of educational materials, you don’t need too much compute, and there’s short feedback loops. But if you &#x27;re new, it can feel pretty intimidating to get started. This is my updated guide on how to skill up, get involved, reach the point where you can do actual research, and some advice on how to go from there to a career/academic role in the field.</p>
                    <p id="block46">This guide is deliberately highly opinionated. My goal is to convey a productive mindset and concrete steps that I think will work well, and give a sense of direction, rather than trying to give a fully broad overview or perfect advice. (And many of the links are to my own work because that &#x27;s what I know best. Sorry!)</p>
                    <h3 id="High_Level_Framing" data-internal-id="High_Level_Framing">
                        <span id="High_Level_Framing">High-Level Framing</span>
                    </h3>
                    <p id="block47">My core philosophy for getting into mech interp is this: learn the absolute minimal basics as quickly as possible, and then immediately transition to learning by doing research.</p>
                    <p id="block48">The goal is not to read every paper before you touch research. When doing research you &#x27;ll notice gaps and go back to learn more. But being grounded in a project will give you vastly more direction to guide your learning, and contextualise why anything you’re learning actually matters. You just want enough grounding to start a project with some understanding of what you’re doing.</p>
                    <p id="block49">
                        Don &#x27;t stress about the research quality at first, or having the perfect project idea. Key skills, like <a class="LinkStyles-link" href="/s/5GT3yoYM9gRmMEKqL/p/Ldrss6o3tiKT6NdMm">research taste</a>
                        and the ability to prioritize, take time to develop. Gaining experience—even messy experience—will teach you the basics like how to run and interpret experiments, which in turn help you learn the high-level skills.
                    </p>
                    <p id="block50">I break this down into three stages:</p>
                    <ol>
                        <li id="block51">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_1__Learning_the_Ropes">
                                <strong>Learning the ropes</strong>
                            </a>
                            , where you work through the basics breadth first, and after at most a month, move on to stage 2
                        </li>
                        <li id="block52">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_2__Practicing_Research_with_Mini_Projects">
                                <strong>Practicing research with mini-projects</strong>
                            </a>
                            . Work on throwaway, 1-5 day research projects. Focus on practicing the basic research skills with the fastest feedback loops, don’t stress about having the best ideas, or writing them up. After 2-4 weeks, move on to stage 3
                        </li>
                        <li id="block53">
                            <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_3__Working_Up_To_Full_Research_Projects">
                                <strong>Work up to full-projects</strong>
                            </a>
                            : work in 1-2 week sprints. After each, do a post-mortem and pivot to something else, <i>unless </i>
                            it was going great and has momentum. Eventually, you should end up working on something longer-term. Start thinking about the deeper skills and research mindsets, practice having good ideas, and prioritize making good public write-ups of sprints that went well
                        </li>
                    </ol>
                    <h2 id="Stage_1__Learning_the_Ropes" data-internal-id="Stage_1__Learning_the_Ropes">
                        <span id="Stage_1__Learning_the_Ropes">Stage 1: Learning the Ropes</span>
                    </h2>
                    <p id="block54">Your goal here is learning the basics: how to write experiments with a mech interp library, understanding the key concepts, getting the lay of the land.</p>
                    <p data-internal-id="ftnt_ref1" id="block55">
                        <span id="ftnt_ref1">
                            Your aim is learning enough that the rest of your learning can be done via doing research, <i>not</i>
                            finishing learning. Prioritize ruthlessly. <strong>After max 1 month</strong>
                            <span data-footnote-reference="" data-footnote-index="1" data-footnote-id="nifk1wb1jum" role="doc-noteref" id="fnrefnifk1wb1jum" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fnnifk1wb1jum" class="">[1]</a>
                                    </span>
                                </sup>
                            </span>
                            <strong>, move on to stage 2</strong>
                            . I’ve flagged which parts of this I think are essential, vs just nice to have.
                        </span>
                    </p>
                    <p id="block56">
                        <strong>Do not just read papers </strong>
                        - a common mistake among academic types is to spend months reading as many papers as they can get their hands on before writing code. Don’t do it. Mech interp is an empirical science, getting your hands dirty gives key context for your learning. Intersperse reading papers with doing coding tutorials or small research explorations. See 
                        <span>
                            <span>
                                <a href="https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T">my research walkthroughs</a>
                            </span>
                        </span>
                        for an idea of what tiny exploratory projects can look like.
                    </p>
                    <p id="block57">
                        LLMs are a key tool - see <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#h.ab01gbohcxm5">the section below</a>
                        for advice on using them well
                    </p>
                    <h3 id="Machine_Learning___Transformer_Basics" data-internal-id="Machine_Learning___Transformer_Basics">
                        <span id="Machine_Learning___Transformer_Basics">
                            <strong>Machine Learning &amp;Transformer Basics</strong>
                        </span>
                    </h3>
                    <p id="block58">
                        <i>Assuming you already know basic Python and introductory ML concepts.</i>
                    </p>
                    <ul>
                        <li id="block59">
                            <strong>Maths:</strong>
                            <ul>
                                <li id="block60">
                                    <strong>Linear Algebra is King (Essential):</strong>
                                    You need to think in vectors and matrices fluently. This is by far the highest value set of generic math you should learn to do mech interp or ML research.
                                    <ul>
                                        <li id="block61">
                                            <i>Resource:</i>
                                            3Blue1Brown &#x27;s
                                            <span>
                                                <span>
                                                    <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a>
                                                </span>
                                            </span>
                                            .
                                        </li>
                                        <li id="block62">
                                            <strong>Highly recommended</strong>
                                            : Put 
                                            <span>
                                                <span>
                                                    <a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework For Transformer Circuits</a>
                                                </span>
                                            </span>
                                            in the context window and have the LLM generate exercises to test your intuitions about transformer internals.
                                        </li>
                                        <li id="block63">
                                            LLMs are great for checking whether linear algebra actually clicks. Try summarizing what you &#x27;ve learned and the links between different concepts and ask an LLM whether you are correct. For example:
                                            <ul>
                                                <li id="block64">Ensure you understand SVD and why it works</li>
                                                <li id="block65">What does changing basis mean and why does it matter</li>
                                                <li id="block66">Key ways a low rank and full rank matrix differ</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                                <li id="block67">
                                    <strong>Other Bits:</strong>
                                    Basic probability, info theory, optimization, vector calculus.
                                    <ul>
                                        <li id="block68">Use an LLM tutor to quiz your understanding on the parts most relevant to transformers</li>
                                    </ul>
                                </li>
                                <li id="block69">Generally don’t bother learning other areas of maths (unless doing it for fun!)</li>
                            </ul>
                        </li>
                        <li id="block70">
                            <strong>Practical ML with PyTorch: (Essential)</strong>
                            <ul>
                                <li id="block71">
                                    <p data-internal-id="ftnt_ref2" id="block72">
                                        <span id="ftnt_ref2">
                                            Code a simple Transformer (like GPT-2) from scratch. ARENA Chapter 1.1 is a great coding tutorial
                                            <span data-footnote-reference="" data-footnote-index="2" data-footnote-id="ue9pdw6v8rj" role="doc-noteref" id="fnrefue9pdw6v8rj" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fnue9pdw6v8rj" class="">[2]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                        </span>
                                    </p>
                                    <ul>
                                        <li id="block73">
                                            <p data-internal-id="ftnt_ref2" id="block74">
                                                <span id="ftnt_ref2">
                                                    This builds intuitions for mech interp <i>and </i>
                                                    on using PyTorch.
                                                </span>
                                            </p>
                                        </li>
                                        <li id="block75">
                                            <p data-internal-id="ftnt_ref2" id="block76">
                                                <span id="ftnt_ref2">
                                                    I have two video tutorials on this, starting from the basics - 
                                                    <span>
                                                        <span>
                                                            <a href="https://www.youtube.com/watch?v=bOYE6E8JrtU&amp;list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz">start here</a>
                                                        </span>
                                                    </span>
                                                    if you’re not sure what to do!
                                                </span>
                                            </p>
                                        </li>
                                        <li id="block77">
                                            <p data-internal-id="ftnt_ref2" id="block78">
                                                <span id="ftnt_ref2">And use LLMs to fill in any background things you’re missing, like PyTorch basics</span>
                                            </p>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block79">
                            <strong>Cloud GPUs:</strong>
                            <ul>
                                <li id="block80">You’ll need to be able to run language models, which (typically) needs a GPU</li>
                                <li id="block81">
                                    You can start with Google Colab to get started fast, but it’ll be very constraining to use long-term. Learn to rent and use a cloud GPU.
                                    <ul>
                                        <li id="block82">Newer Macbook Pros, or computers with powerful gaming GPUs may also be able to run LLMs locally</li>
                                    </ul>
                                </li>
                                <li id="block83">
                                    <i>Resource:</i>
                                    ARENA has a
                                    <span>
                                        <span>
                                            <a href="https://arena-appendix.streamlit.app/cloud-gpus"></a>
                                        </span>
                                    </span>
                                    <span>
                                        <span>
                                            <a href="https://arena-chapter0-fundamentals.streamlit.app/#vm-setup-instructions">guide</a>
                                        </span>
                                    </span>
                                    . I like
                                    <span>
                                        <span>
                                            <a href="http://runpod.io/"></a>
                                        </span>
                                    </span>
                                    <span>
                                        <span>
                                            <a href="http://runpod.io">runpod.io</a>
                                        </span>
                                    </span>
                                    as a provider;
                                    <span>
                                        <span>
                                            <a href="http://vast.ai/">vast.ai</a>
                                        </span>
                                    </span>
                                    is cheaper.
                                </li>
                                <li id="block84">
                                    nnsight also lets you do some 
                                    <span>
                                        <span>
                                            <a href="https://nnsight.net/notebooks/tutorials/get_started/start_remote_access/">interpretability on certain models they host themselves</a>
                                        </span>
                                    </span>
                                    , including LLaMA 3 405B, which can be a great way to work with larger models.
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <h3 id="Mechanistic_Interpretability_Techniques" data-internal-id="Mechanistic_Interpretability_Techniques">
                        <span id="Mechanistic_Interpretability_Techniques">Mechanistic Interpretability Techniques</span>
                    </h3>
                    <p id="block85">A lot of mech interp research looks like knowing the right technique to apply and in what context. This is a key thing to prioritise getting your head around when starting out. You’ll learn this with a mix of reading educational materials and doing coding tutorials like ARENA (discussed in next sub-section).</p>
                    <ul>
                        <li id="block86">
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2405.00208">Ferrando et al</a>
                                </span>
                            </span>
                            is a good <strong>overview</strong>
                            of the key techniques - it’s long enough that you shouldn’t prioritise reading it in full, but it’s a great reference
                            <ul>
                                <li id="block87">Put it in a LLM context window and ask questions, or to write you exercises</li>
                            </ul>
                        </li>
                        <li id="block88">
                            <p data-internal-id="ftnt_ref3" id="block89">
                                <span id="ftnt_ref3">
                                    <strong>Essential</strong>
                                    : Make sure you understand these <strong>core techniques</strong>
                                    , well enough that you can code it up yourself on a simple model like GPT-2 Small
                                    <span data-footnote-reference="" data-footnote-index="3" data-footnote-id="hh6mwdeo4zm" role="doc-noteref" id="fnrefhh6mwdeo4zm" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnhh6mwdeo4zm" class="">[3]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    :
                                </span>
                            </p>
                            <ul>
                                <li id="block90">
                                    <p data-internal-id="ftnt_ref3" id="block91">
                                        <span id="ftnt_ref3">Activation Patching</span>
                                    </p>
                                </li>
                                <li id="block92">
                                    <p data-internal-id="ftnt_ref3" id="block93">
                                        <span id="ftnt_ref3">Linear Probes</span>
                                    </p>
                                </li>
                                <li id="block94">
                                    <p data-internal-id="ftnt_ref3" id="block95">
                                        <span id="ftnt_ref3">Using Sparse Autoencoders (SAEs) (you only need to write code that uses an SAE, not trains one)</span>
                                    </p>
                                </li>
                                <li id="block96">
                                    <p data-internal-id="ftnt_ref3" id="block97">
                                        <span id="ftnt_ref3">Max Activating Dataset Examples</span>
                                    </p>
                                </li>
                                <li id="block98">
                                    <p data-internal-id="ftnt_ref3" id="block99">
                                        <span id="ftnt_ref3">Nice-to-have:</span>
                                    </p>
                                    <ul>
                                        <li id="block100">
                                            <p data-internal-id="ftnt_ref3" id="block101">
                                                <span id="ftnt_ref3">Steering Vectors</span>
                                            </p>
                                        </li>
                                        <li id="block102">
                                            <p data-internal-id="ftnt_ref3" id="block103">
                                                <span id="ftnt_ref3">Direct Logit Attribution (DLA) (a simpler version is called logit lens)</span>
                                            </p>
                                        </li>
                                    </ul>
                                </li>
                                <li id="block104">
                                    <p data-internal-id="ftnt_ref3" id="block105">
                                        <span id="ftnt_ref3">
                                            <strong>Key exercise</strong>
                                            : Describe each technique to an LLM with Ferrando et al in the context window and ask for feedback. Iterate until you get it all right.
                                        </span>
                                    </p>
                                    <ul>
                                        <li id="block106">
                                            <p data-internal-id="ftnt_ref3" id="block107">
                                                <span id="ftnt_ref3">Use an anti-sycophancy prompt to get real feedback, by pretending someone else wrote your answer, e.g. “I saw someone claim this, it seems pretty off to me, can you help me give them direct but constructive feedback on what they missed? [insert your description]”</span>
                                            </p>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block108">
                            Remember that there’s a bunch of valuable <strong>black-box interpretability </strong>
                            techniques! (ie that don’t use the model’s internals) You can often correctly guess a model’s algorithm by reading its chain of thought. Careful variation of the prompt is a powerful way to causally test hypotheses.
                            <ul>
                                <li id="block109">
                                    They’re an additional tool. Often the correct first step in an investigation is just talking to the model and bunch and observing its behaviour. Don’t be a purist and dismiss them as “not rigorous” - they have uses and flaws, just like any other technique.
                                    <ul>
                                        <li id="block110">
                                            One <a class="LinkStyles-link" href="/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the">project I supervised</a>
                                            on interpreting “self-preservation” in frontier models started with simple black-box techniques, and it just worked, we never needed anything fancier.
                                        </li>
                                    </ul>
                                </li>
                                <li id="block111">
                                    Understand fancier black-box techniques like 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2312.12321">token forcing</a>
                                        </span>
                                    </span>
                                    (aka prefill attacks) where you put words in a model’s mouth.
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <h3 id="Mechanistic_Interpretability_Coding___Tooling" data-internal-id="Mechanistic_Interpretability_Coding___Tooling">
                        <span id="Mechanistic_Interpretability_Coding___Tooling">Mechanistic Interpretability Coding &amp;Tooling</span>
                    </h3>
                    <ul>
                        <li id="block112">
                            <p data-internal-id="ftnt_ref4" id="block113">
                                <span id="ftnt_ref4">
                                    <strong>Goal:</strong>
                                    Get comfortable running experiments and &quot;playing &quot;with model internals. Get the engineering basics down
                                    <span data-footnote-reference="" data-footnote-index="4" data-footnote-id="sxyjce3nii" role="doc-noteref" id="fnrefsxyjce3nii" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnsxyjce3nii" class="">[4]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    . Get your hands dirty<strong>.</strong>
                                </span>
                            </p>
                        </li>
                        <li id="block114">
                            <strong>ARENA</strong>
                            : ARENA has 
                            <span>
                                <span>
                                    <a href="https://arena-chapter1-transformer-interp.streamlit.app/">a set of fantastic coding tutorials by Callum McDougall</a>
                                </span>
                            </span>
                            , you should just go do these. But there’s tons, so <strong>prioritize ruthlessly</strong>
                            .
                            <ul>
                                <li id="block115">
                                    <strong>Essential</strong>
                                    <i>
                                        <strong>:</strong>
                                    </i>
                                    <strong>Chapter 1.2</strong>
                                    (Interpretability Basics – prioritize the first 3 sections on tooling, direct observation, and patching).
                                </li>
                                <li id="block116">
                                    <i>Recommended: </i>
                                    1.4.1 (Causal Interventions &amp;Activation Patching – this is a core technique).
                                </li>
                                <li id="block117">
                                    <i>Worthwhile</i>
                                    : 1.3.2 (Sparse Autoencoders (SAEs) – Skim or Skip section 1, the key thing to get from the rest is an intuition for what SAEs are, strengths and weaknesses, and how to use an open source SAE. Don’t worry about training them).
                                </li>
                            </ul>
                        </li>
                        <li id="block118">
                            <strong>Tooling </strong>
                            (<strong>Essential</strong>
                            )<strong>:</strong>
                            Get proficient with at least one mech interp library, this is what you’ll use to run experiments.
                            <ul>
                                <li id="block119">
                                    <span>
                                        <span>
                                            <a href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a>
                                        </span>
                                    </span>
                                    : best for small models &lt;=9B where you want to write more complex interpretability experiments, or work with many models at once.
                                    <ul>
                                        <li id="block120">
                                            As of early Sept 2025, TransformerLens 
                                            <span>
                                                <span>
                                                    <a href="https://github.com/TransformerLensOrg/TransformerLens/releases/tag/v3.0.0a5">v3</a>
                                                </span>
                                            </span>
                                            is in alpha, works well with large models and is far more flexible.
                                        </li>
                                    </ul>
                                </li>
                                <li id="block121">
                                    <span>
                                        <span>
                                            <a href="http://nnsight.net/">nnsight</a>
                                        </span>
                                    </span>
                                    : More performant, works well on larger models, it’s just a wrapper around standard LLM libraries like HuggingFace transformers
                                </li>
                            </ul>
                        </li>
                        <li id="block122">
                            <strong>LLM APIs</strong>
                            : Learn how to use an LLM API to call an LLM programmatically. This is super useful for measuring qualitative things about some data, and for generating synthetic datasets
                            <ul>
                                <li id="block123">
                                    I like 
                                    <span>
                                        <span>
                                            <a href="http://openrouter.ai">openrouter.ai</a>
                                        </span>
                                    </span>
                                    which lets you access almost all the important LLMs from a single place. GPT5 and Gemini are reasonably priced and good defaults, they have a range of sizes
                                    <ul>
                                        <li id="block124">
                                            Cerebras and Groq have <i>way </i>
                                            higher throughput than normal providers, and serve a handful of open source models, they may be worth checking out.
                                        </li>
                                    </ul>
                                </li>
                                <li id="block125">
                                    <p data-internal-id="ftnt_ref6" id="block126">
                                        <span id="ftnt_ref6">
                                            Exercise: Make a happiness steering vector (for e.g. GPT-2 Small) by having an LLM via an API generate 32 happy prompts and 32 sad prompts, and taking the difference in mean activations
                                            <span data-footnote-reference="" data-footnote-index="5" data-footnote-id="kte6u8splw" role="doc-noteref" id="fnrefkte6u8splw" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fnkte6u8splw" class="">[5]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                            (e.g. the residual stream at the middle layer). Add this vector to the model’s residual stream
                                            <span data-footnote-reference="" data-footnote-index="6" data-footnote-id="2ob115pcmet" role="doc-noteref" id="fnref2ob115pcmet" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fn2ob115pcmet" class="">[6]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                            while generating responses to some example prompts, and use an LLM API to rate how happy they seem, and see this score go up when steering.
                                        </span>
                                    </p>
                                </li>
                            </ul>
                        </li>
                        <li id="block127">
                            <strong>Open source LLMs</strong>
                            : You’ll want to work a lot with open source LLMs, as the thing you’re trying to interpret. The best open source LLM changes a lot
                            <ul>
                                <li id="block128">
                                    <p data-internal-id="ftnt_ref7" id="block129">
                                        <span id="ftnt_ref7">
                                            As of early Sept 2025, Qwen3 is a good default model family. Each model has reasoning and non-reasoning mode, there’s a good range of sizes, and most are dense
                                            <span data-footnote-reference="" data-footnote-index="7" data-footnote-id="1b9r0ass7sd" role="doc-noteref" id="fnref1b9r0ass7sd" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fn1b9r0ass7sd" class="">[7]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                        </span>
                                    </p>
                                    <ul>
                                        <li id="block130">
                                            <p data-internal-id="ftnt_ref7" id="block131">
                                                <span id="ftnt_ref7">Gemma 3 and LLaMA 3.3 are decent non-reasoning models. I’ve heard bad things about gpt-oss and LLaMA 4</span>
                                            </p>
                                        </li>
                                    </ul>
                                </li>
                                <li id="block132">
                                    <i>Gotcha: </i>
                                    The different open source LLMs often have different tokenizations and formats for chat or reasoning tokens. Using the wrong token format can only somewhat degrade performance and may be hard to notice while corrupting your results - keep an eye out, try hard to find where this might be documented, and sanity check by e.g. comparing to official evals
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <h3 id="Understanding_the_literature" data-internal-id="Understanding_the_literature">
                        <span id="Understanding_the_literature">Understanding the literature</span>
                    </h3>
                    <p id="block133">Your priority is to understand the concepts and the basics, but you want a sense for the landscape of the field, so you should practice reading at least some papers.</p>
                    <ul>
                        <li id="block134">
                            Remember, <strong>breadth over depth</strong>
                            . Skim things, get a sense of what &#x27;s out there, and only dive into the things that are most interesting.
                            <ul>
                                <li id="block135">
                                    You should be heavily using <strong>LLMs</strong>
                                    here. Give them something you &#x27;re considering reading and get a summary, ask questions about the work, summarise your understanding to it and ask for feedback (with an anti-sycophancy prompt).
                                    <ul>
                                        <li id="block136">If you aren &#x27;t able to verify yourself, cross-reference by asking multiple LLMs and making sure they all say consistent things.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block137">
                            Here’s <a class="LinkStyles-link" href="/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite">a list of my favourite papers</a>
                            (as of mid 2024) with summaries and opinions
                            <ul>
                                <li id="block138">
                                    Do <i>not </i>
                                    try to read all of these in full. Skim summaries, skim abstracts, pick a few to explore deeper with an LLM, <i>then</i>
                                    decide if you want to read the full paper.
                                </li>
                                <li id="block139">
                                    <span>
                                        <span>
                                            <a href="https://www.youtube.com/@neelnanda2469">My YouTube Channel</a>
                                        </span>
                                    </span>
                                    :
                                    <span>
                                        <span>
                                            <a href="https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T&amp;index=1"></a>
                                        </span>
                                    </span>
                                    <span>
                                        <span>
                                            <a href="https://www.youtube.com/watch?v=KV5gbOmHbjU&amp;list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&amp;pp=gAQB">Paper walkthroughs</a>
                                        </span>
                                    </span>
                                    , 
                                    <span>
                                        <span>
                                            <a href="https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T">recordings of myself doing research</a>
                                        </span>
                                    </span>
                                    , and talks.
                                </li>
                            </ul>
                        </li>
                        <li id="block140">
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2501.16496">Open Problems In Mechanistic Interpretability</a>
                                </span>
                            </span>
                            is a decent recent literature review, that a lot of top mech interp people were involved in
                            <ul>
                                <li id="block141">Be warned that the paper basically consists of a bunch of opinionated and disagreeable researchers writing their own sections and often having strong takes. Don’t defer to it too much, but it &#x27;s a good way to quickly assess what &#x27;s out there.</li>
                            </ul>
                        </li>
                        <li id="block142">
                            <strong>Deep dives</strong>
                            : You should read at least one paper carefully and in full. This is a useful skill that you will use in research projects where there’s a handful of extremely relevant papers to your project
                            <ul>
                                <li id="block143">This is much more than just reading the words! You should write out a summary, try to understand the surrounding context with LLM help, be able to describe why the paper exists, the motivation, the problem it &#x27;s trying to solve, etc.</li>
                                <li id="block144">Aim for a barbell strategy: put minimal effort into most papers and a lot of effort into a few.</li>
                            </ul>
                        </li>
                        <li id="block145">
                            <strong>LLMs</strong>
                            : LLMs are a super useful tool for exploring the literature, but easy to shoot yourself in the foot with.
                            <ul>
                                <li id="block146">
                                    As a search engine over the literature (especially with some lit reviews in context, or a starting paper), basically doing a lit review, finding relevant work for a question you have, etc.
                                    <ul>
                                        <li id="block147">
                                            <p data-internal-id="ftnt_ref8" id="block148">
                                                <span id="ftnt_ref8">
                                                    As a tool to help you skim a paper - put the paper in the context window
                                                    <span data-footnote-reference="" data-footnote-index="8" data-footnote-id="bzop9pji3nl" role="doc-noteref" id="fnrefbzop9pji3nl" class="footnote-reference">
                                                        <sup>
                                                            <span>
                                                                <a href="#fnbzop9pji3nl" class="">[8]</a>
                                                            </span>
                                                        </sup>
                                                    </span>
                                                    then get a summary, ask it questions, etc
                                                </span>
                                            </p>
                                        </li>
                                        <li id="block149">If you’re concerned about hallucinations, you can ask it to support answers with quotes (and verify these are real and make sense), or give its answer to another LLM and ask for harsh critique of all the inaccuracies. Honestly, I often don’t bother though, frontier reasoning models are pretty good now.</li>
                                    </ul>
                                </li>
                                <li id="block150">As a tool to help with deep dives - you need to actually read the paper, but I recommend having the LLM chat open as you read with the paper in the context and asking it questions, for context, etc every time you get confused.</li>
                            </ul>
                        </li>
                    </ul>
                    <h3 id="Using_LLMs_for_Learning" data-internal-id="Using_LLMs_for_Learning">
                        <span id="Using_LLMs_for_Learning">Using LLMs for Learning</span>
                    </h3>
                    <p id="block151">
                        <i>Note: I expect this section to go out of date fast! Written early Sept 2025</i>
                    </p>
                    <p id="block152">LLMs are a super useful tool for learning, especially in a new field. While they struggle to beat experts, they often beat novices. If you aren’t using them regularly throughout this process, I’d guess you’re leaving a bunch of value on the table.</p>
                    <p id="block153">But LLMs have weird flaws and strengths, and it’s worth being intentional about how you use them:</p>
                    <ul>
                        <li id="block154">
                            <strong>Use a good model</strong>
                            : The best paid models are way better than e.g. free ChatGPT. Don &#x27;t be a cheapskate; if you can, get a $20/month subscription, it makes a big difference. Gemini 2.5 Pro, Claude 4.1 Opus with extended thinking, and GPT-5 Thinking are all reasonable. (do <i>not </i>
                            use non-thinking GPT-5 or anything older like GPT-4o, reasoning models are a big upgrade)
                            <ul>
                                <li id="block155">If you can’t get a subscription, Gemini 2.5 Pro is also available for free, and is the best.</li>
                                <li id="block156">
                                    Use Gemini 2.5 Pro via 
                                    <span>
                                        <span>
                                            <a href="https://aistudio.google.com/prompts/new_chat">AI Studio</a>
                                        </span>
                                    </span>
                                    , it’s way better than the main Gemini interface and has much nicer rate limits for free users. Always use compare mode (the button in the header with two arrows) to see two responses in parallel from Pro
                                </li>
                                <li id="block157">
                                    See <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher?commentId=jDzbZGnjWDMsNjDPQ">thoughts</a>
                                    from my MATS alum Paul Bogdan comparing different LLMs for learning, and why he currently prefers Gemini
                                </li>
                            </ul>
                        </li>
                        <li id="block158">
                            <strong>System Prompts:</strong>
                            System prompts make a big difference - be concrete and specific about what you want, and how you want it done.
                            <ul>
                                <li id="block159">LLMs are good at this: I &#x27;ll just ramble at one about what the task is, my criteria, the failure modes I don &#x27;t want, and then it’ll just write the prompt for me</li>
                                <li id="block160">If the prompt doesn’t work, tell the LLM what it did wrong, and see if it can rewrite the prompt for you.</li>
                            </ul>
                        </li>
                        <li id="block161">
                            <strong>Merge perspectives</strong>
                            :
                            <ul>
                                <li id="block162">
                                    Ask a Q to multiple different frontier LLMs, give LLM B’s response to LLM A and ask it to assess the strengths and weaknesses then merge.
                                    <ul>
                                        <li id="block163">If a point is in both original responses, it’s probably not a hallucination</li>
                                    </ul>
                                </li>
                                <li id="block164">If you want to fact check an LLM’s answer, give it to another LLM with an anti-sycophancy prompt</li>
                            </ul>
                        </li>
                        <li id="block165">
                            <strong>Anti-Sycophancy Prompts:</strong>
                            LLMs are bad at giving critical feedback. Frame your request so the sycophantic thing to do is to be critical, by pretending someone else wrote the thing you want feedback on.
                            <ul>
                                <li id="block166">
                                    <i>&quot;A friend wrote this explanation and asked for brutally honest feedback. They &#x27;ll be offended if I hold back. Please help me give them the most useful feedback.&quot;</i>
                                </li>
                                <li id="block167">
                                    <i>&quot;I saw someone claiming this, but it seems pretty dumb to me. What do you think?&quot;</i>
                                </li>
                                <li id="block168">
                                    <i>“Some moron wrote this thing, and I find this really annoying. Please write me a brutal but truthful response”</i>
                                </li>
                            </ul>
                        </li>
                        <li id="block169">
                            <strong>Learn actively, not passively:</strong>
                            <ul>
                                <li id="block170">
                                    <strong>Summarize </strong>
                                    your understanding back to the LLM in your own words and ask for critical feedback. Do this every time you read a paper or learn about a new concept
                                </li>
                                <li id="block171">
                                    Try having it teach you <strong>socratically</strong>
                                    . Note: you can probably design a better system prompt than the official “study mode”
                                </li>
                                <li id="block172">
                                    Ask the LLM to <strong>generate exercises</strong>
                                    to test your understanding, including maths and coding exercises as appropriate.
                                    <ul>
                                        <li id="block173">Gemini can make multiple choice quizzes, which some enjoy</li>
                                        <li id="block174">Coding exercises can be requested with accompanying tests, and template code with blank functions for you to fill out, a la the ARENA tutorials.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block175">
                            <p data-internal-id="ftnt_ref9" id="block176">
                                <span id="ftnt_ref9">
                                    <strong>Context engineering:</strong>
                                    Modern LLMs are much more useful with relevant info in context. If you give them the paper in question, or source code of the relevant library
                                    <span data-footnote-reference="" data-footnote-index="9" data-footnote-id="207k0k5nobb" role="doc-noteref" id="fnref207k0k5nobb" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn207k0k5nobb" class="">[9]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    , they’ll be far more helpful.
                                </span>
                            </p>
                            <ul>
                                <li id="block177">
                                    <p data-internal-id="ftnt_ref9" id="block178">
                                        <span id="ftnt_ref9">
                                            See 
                                            <span>
                                                <span>
                                                    <a href="https://drive.google.com/drive/u/0/folders/1GfrgKJwndk-twnJ8K7Ba-TE9i_8wBWAU">this folder</a>
                                                </span>
                                            </span>
                                            for a bunch of saved context files for mech interp queries. If you don’t know what you need, just use 
                                            <span>
                                                <span>
                                                    <a href="https://drive.google.com/file/d/18cF3lkU17_elUSv0zk8KSVejM1jGfNnz/view?usp=drive_link">this default file</a>
                                                </span>
                                            </span>
                                            .
                                        </span>
                                    </p>
                                </li>
                                <li id="block179">
                                    <p data-internal-id="ftnt_ref9" id="block180">
                                        <span id="ftnt_ref9">
                                            I recommend Gemini 2.5 Pro (1M context window) via
                                            <span>
                                                <span>
                                                    <a href="http://aistudio.google.com/">aistudio.google.com</a>
                                                </span>
                                            </span>
                                            ; the UI is better. Always turn compare mode on, you get two answers in parallel
                                        </span>
                                    </p>
                                </li>
                            </ul>
                        </li>
                        <li id="block181">
                            <strong>Voice dictation</strong>
                            : If you dictate to your LLM, via free speech-to-text software, and run it with no editing, it’ll understand fine. I personally find this much easier, especially when brain-dumping.
                            <ul>
                                <li id="block182">
                                    <span>
                                        <span>
                                            <a href="http://superwhisper.com">Superwhisper</a>
                                        </span>
                                    </span>
                                    on Mac is great; Superwhisper is not currently available on Windows, but Windows users can use 
                                    <span>
                                        <span>
                                            <a href="https://wisprflow.ai/">Whispr Flow</a>
                                        </span>
                                    </span>
                                    .
                                </li>
                            </ul>
                        </li>
                        <li id="block183">
                            <strong>Coding</strong>
                            : LLM tools like Cursor are great for coding, but <i>not </i>
                            if your goal is to learn. For things like ARENA, only let yourself use browser-based LLMs, and only use them as a tutor. Don’t copy and paste code, your goal is to learn not complete exercises.
                        </li>
                    </ul>
                    <h2 id="Interlude__What_is_mech_interp_" data-internal-id="Interlude__What_is_mech_interp_">
                        <span id="Interlude__What_is_mech_interp_">Interlude: What is mech interp?</span>
                    </h2>
                    <p id="block184">
                        <i>Feel free to skip to the</i>
                        <i>“</i>
                        <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#The_Big_Picture__Learning_the_Craft_of_Research">
                            <i>what should I do next</i>
                        </a>
                        <i>” part</i>
                    </p>
                    <p data-internal-id="ftnt_ref10" id="block185">
                        <span id="ftnt_ref10">
                            At this point it’s worth reflecting on what mech interp actually <i>is</i>
                            . What are we even doing here? There isn &#x27;t a consensus definition on how exactly to define mechanistic interpretability, and different researchers will give very different takes. But <i>my</i>
                            working definition is as follows
                            <span data-footnote-reference="" data-footnote-index="10" data-footnote-id="979wnkvgpa4" role="doc-noteref" id="fnref979wnkvgpa4" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fn979wnkvgpa4" class="">[10]</a>
                                    </span>
                                </sup>
                            </span>
                            .
                        </span>
                    </p>
                    <ul>
                        <li id="block186">
                            <strong>Interpretability</strong>
                            is the study of understanding models, gaining insight into their behavior, the cognition inside of them, why and how they work, etc. This is the important part and the heart of the field.
                        </li>
                        <li id="block187">
                            <strong>Mechanistic</strong>
                            means using the internals of the model, the weights and activations
                        </li>
                        <li id="block188">
                            So <strong>mechanistic interpretability </strong>
                            is any approach to understanding the model that uses its internals.
                            <ul>
                                <li id="block189">
                                    This is distinct from some other worthwhile directions, like <strong>black box interpretability</strong>
                                    , understanding models without using the internals, and <strong>model internals</strong>
                                    , using the internals of the model for other things like steering vectors.
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block190">
                        <strong>Why this definition?</strong>
                        To do impactful research, it &#x27;s often good to find the directions that other people are missing. I think of most of machine learning as non-mechanistic non-interpretability. 99% of ML research just looks at the inputs and outputs to models, and treats its north star as controlling their behavior. Progress is defined by making a number go up, not to explain why it works. This has been very successful, but IMO leaves a lot of value on the table. Mechanistic interpretability is about doing better than this, and has achieved a bunch of cool stuff, like 
                        <span>
                            <span>
                                <a href="https://arxiv.org/abs/2310.16410">teaching grandmasters how to play chess better by interpreting AlphaZero</a>
                            </span>
                        </span>
                        .
                    </p>
                    <p id="block191">
                        <strong>Why care?</strong>
                        Obviously, our goal is not “do things if and only if they fit the above definition”, but I find it a useful one. To discuss this, let’s first consider our actual goals here. To me, <strong>the ultimate goal is to make human-level AI systems (or beyond) safer</strong>
                        . I do mech interp because I think we’ll find enough understanding of what happens inside a model to be pragmatically useful here (also, because mech interp is fun!): to better understand how they work, detect if they &#x27;re lying to us, detect and diagnose unexpected failure modes, etc. But people’s goals vary, e.g. real-world usefulness today, aesthetic beauty, or scientific insight. It’s worth thinking about what yours are.
                    </p>
                    <p id="block192">Some implications of this framing worth laying out:</p>
                    <ul>
                        <li id="block193">
                            My ultimate <strong>north star is pragmatism</strong>
                            - achieve enough understanding to be (reliably) useful. Subgoals like “completely reverse engineer the model” are just means to an end.
                            <ul>
                                <li id="block194">
                                    One of my big shifts in research prioritization in recent years is concluding that <strong>reverse engineering is not the right aim</strong>
                                    . Instead, I think we should just more directly try to do pragmatic work that enables us to do useful things using internals. I discuss this shift more <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#A_Pragmatic_Vision_for_Mech_Interp">later on</a>
                                    .
                                </li>
                            </ul>
                        </li>
                        <li id="block195">
                            <p data-internal-id="ftnt_ref11" id="block196">
                                <span id="ftnt_ref11">
                                    This is a <strong>broad definition</strong>
                                    . Historically, the field has focused on more specific agendas, like ambitious reverse engineering of models. But I think we shouldn’t limit ourselves, there’s many other important and neglected directions and the field is large enough to cover a lot of ground
                                    <span data-footnote-reference="" data-footnote-index="11" data-footnote-id="3zw26zes9dx" role="doc-noteref" id="fnref3zw26zes9dx" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn3zw26zes9dx" class="">[11]</a>
                                            </span>
                                        </sup>
                                    </span>
                                </span>
                            </p>
                        </li>
                        <li id="block197">
                            It’s about <strong>understanding</strong>
                            , not just using internals - model internals methods like steering vectors can be useful for shaping a model’s behaviour, but compete with many powerful methods like prompting and fine-tuning. Very few areas of ML can achieve understanding
                        </li>
                        <li id="block198">
                            <strong>Don’t be a purist</strong>
                            - using internals is a means to an end. If black-box methods are the right tool, use them
                        </li>
                    </ul>
                    <h2 id="The_Big_Picture__Learning_the_Craft_of_Research" data-internal-id="The_Big_Picture__Learning_the_Craft_of_Research">
                        <span id="The_Big_Picture__Learning_the_Craft_of_Research">The Big Picture: Learning the Craft of Research</span>
                    </h2>
                    <p data-internal-id="ftnt_ref12" id="block199">
                        <span id="ftnt_ref12">
                            So, you &#x27;ve gone through the tutorials, you understand the core concepts, and you can write some basic experimental code. Now comes the hard part: learning how to actually do mech interp research
                            <span data-footnote-reference="" data-footnote-index="12" data-footnote-id="7cxhc64szn8" role="doc-noteref" id="fnref7cxhc64szn8" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fn7cxhc64szn8" class="">[12]</a>
                                    </span>
                                </sup>
                            </span>
                            .
                        </span>
                    </p>
                    <p id="block200">
                        This is an inherently difficult thing to learn, of course. But IMO people often misunderstand what they need to do here, try to learn everything at once, or more generally make life unnecessarily hard for themselves. The key is to break the process down, understand the different skills involved, and focus on <strong>learning the pieces with the fastest feedback loops first</strong>
                        .
                    </p>
                    <p data-internal-id="ftnt_ref13" id="block201">
                        <span id="ftnt_ref13">
                            I suggest breaking this down into two stages
                            <span data-footnote-reference="" data-footnote-index="13" data-footnote-id="9wj0u0qz3q" role="doc-noteref" id="fnref9wj0u0qz3q" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fn9wj0u0qz3q" class="">[13]</a>
                                    </span>
                                </sup>
                            </span>
                            .
                        </span>
                    </p>
                    <p id="block202">
                        <strong>Stage 2</strong>
                        : working on a bunch of throwaway mini projects of 1-5 days each. Don &#x27;t stress about choosing the best projects or producing public output. The goal is to learn the skills with the fastest feedback loops.
                    </p>
                    <p id="block203">
                        <strong>Stage 3: </strong>
                        After a few weeks of these, start to be more ambitious: paying more attention to how you choose your projects, gaining the subtler skills, and how to write things up. I still recommend working iteratively, in one to two week sprints, but ending up with longer-term projects if things go well.
                    </p>
                    <p id="block204">Note: Unlike stage 1 to 2, the transition from stages two to three should be fairly gradual as you take on larger projects and become more ambitious. A good default would be after three to four weeks in stage two, but you don’t need to have a big formal shift.</p>
                    <p id="block205">
                        <strong>Mentorship</strong>
                        : A good mentor is a major accelerator, and finding one should be a major priority for you. In the careers section, I provide advice on <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Advice_on_finding_a_mentor">how to go about finding a good mentor</a>
                        , and <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#So_what_does_a_research_mentor_actually_do_">how concretely they can add value</a>
                        . In the rest of the post I &#x27;ll write most of it assuming you do not have a mentor and then flag the ways to use a mentor where appropriate.
                    </p>
                    <h3 id="Unpacking_the_Research_Process" data-internal-id="Unpacking_the_Research_Process">
                        <span id="Unpacking_the_Research_Process">Unpacking the Research Process</span>
                    </h3>
                    <p id="block206">
                        I find it helpful to think of research as a cycle of four distinct stages. Read <a class="LinkStyles-link" href="/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand">my blog post on the research proces</a>
                        for full details, but in brief:
                    </p>
                    <ul>
                        <li id="block207">
                            <strong>Ideation:</strong>
                            You choose a research problem or a general domain to focus on.
                        </li>
                        <li id="block208">
                            <strong>Exploration:</strong>
                            You may not have a specific hypothesis yet; you’re just trying to figure out the right questions to ask, and build deeper intuition for the domain. Your north star is to gain information and surface area.
                        </li>
                        <li id="block209">
                            <strong>Understanding:</strong>
                            This begins when you have a concrete hypothesis, and some intuitive understanding of the domain. Your north star is to convince yourself that the hypothesis is true or false.
                        </li>
                        <li id="block210">
                            <strong>Distillation:</strong>
                            Once you’re convinced, your north star is to compress your findings into concise, rigorous truth that you can communicate to the world - create enough experimental evidence to convince others, write it up clearly, and share it.
                        </li>
                    </ul>
                    <p id="block211">Underpinning these stages is a host of skills, best separated by how quickly you can apply them and get feedback. We learn by doing things and getting feedback, so you’ll learn the fast ones much more quickly. I put a rough list and categorization below.</p>
                    <p id="block212">
                        My general advice is <strong>to prioritize learning these in order of feedback loops</strong>
                        . If it seems like you need a slow skill to get started, like the taste to choose a good research problem, find a way to cheat rather than stressing about not having that skill (e.g. doing an incremental extension to a paper, getting one from a mentor, etc).
                    </p>
                    <ul>
                        <li id="block213">
                            <strong>Fast Loop (minutes-hours):</strong>
                            <ul>
                                <li id="block214">
                                    Planning and writing experiment code
                                    <ul>
                                        <li id="block215">
                                            <strong>Medium</strong>
                                            : Designing great experiments
                                        </li>
                                        <li id="block216">
                                            <strong>Medium</strong>
                                            : Knowing when to write hacky vs. quality code.
                                        </li>
                                    </ul>
                                </li>
                                <li id="block217">
                                    Running/debugging experiments
                                    <ul>
                                        <li id="block218">
                                            <strong>Medium/Slow</strong>
                                            : Spotting and fixing subtle bugs (e.g., you got your tokenization subtly wrong, you didn’t search hyper-parameters well enough, etc)
                                        </li>
                                    </ul>
                                </li>
                                <li id="block219">
                                    Interpreting the results of a single experiment.
                                    <ul>
                                        <li id="block220">
                                            <strong>Medium</strong>
                                            : Understanding whether your results support your conclusions
                                        </li>
                                        <li id="block221">
                                            <strong>Slow</strong>
                                            : Spotting subtle interpretability illusions where your results don &#x27;t actually support your claims
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block222">
                            <strong>Medium Loop (days):</strong>
                            <ul>
                                <li id="block223">
                                    Developing a conceptual understanding of mech interp
                                    <ul>
                                        <li id="block224">
                                            <strong>Slow</strong>
                                            : Noticing and fixing your own subtle confusions
                                        </li>
                                        <li id="block225">
                                            <strong>Slow</strong>
                                            : Build a deep knowledge of the literature
                                        </li>
                                    </ul>
                                </li>
                                <li id="block226">Knowing how to explore without getting stuck</li>
                                <li id="block227">
                                    Writing up results
                                    <ul>
                                        <li id="block228">
                                            <strong>Slow</strong>
                                            : Communicating your work in a way that’s genuinely clear to people.
                                        </li>
                                        <li id="block229">
                                            <strong>Slow</strong>
                                            : Communicating why your work is <i>interesting</i>
                                            to people
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block230">
                            <strong>Slow Loop (weeks):</strong>
                            <ul>
                                <li id="block231">Prioritizing which experiment to do next</li>
                                <li id="block232">Knowing when to continue with a research direction or pivot to another angle of attack/another project</li>
                                <li id="block233">
                                    Identifying bad research ideas, <i>without </i>
                                    doing a project on them first
                                </li>
                            </ul>
                        </li>
                        <li id="block234">
                            <strong>Very Slow Loop (months):</strong>
                            <ul>
                                <li id="block235">Coming up with good research ideas. This is the core of &quot;research taste.&quot;</li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block236">Your progression should be simple: First, focus on the fast/medium skills behind exploration and understanding with throwaway projects. Then, graduate to end-to-end projects where you can intentionally practice the deeper skills, and practice ideation and distillation too.</p>
                    <h3 id="What_is_research_taste_" data-internal-id="What_is_research_taste_">
                        <span id="What_is_research_taste_">What is research taste?</span>
                    </h3>
                    <p id="block237">
                        A particularly important and fuzzy type of skill is called research taste. I basically think of this as the bundle of intuitions you get with enough research experience that let you do things like come up with good ideas, predict if an idea is promising, have conviction in good research directions, etc. Check out <a class="LinkStyles-link" href="/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research">my post on the topic</a>
                        for more thoughts.
                    </p>
                    <p id="block238">I broadly think you should just ignore it for now, find ways to compensate for not having much yet, and focus on learning the fast-medium skills, and this will give you a much better base for learning it. In particular, it &#x27;s much faster to learn with a mentor, so if you don &#x27;t have a mentor at the start, you should prioritize other things.</p>
                    <p id="block239">But you want to learn it eventually, so it &#x27;s good to be mindful of it throughout, and look for opportunities to practice and learn lessons. I recommend treating it as a nice-to-have but not stressing about it</p>
                    <p id="block240">
                        Note, one important trap here is that having good taste can often manifest as having confidence and conviction in some research direction. But often novice researchers develop this confidence and conviction significantly <i>before </i>
                        they develop the ability to not be confident in bad ideas. It’s often a good learning experience to once or twice pursue a thing you feel really convinced is going to be epic and then discover you &#x27;re wrong, so it &#x27;s not that bad an outcome, especially in stage 2 (mini-projects) but be warned.
                    </p>
                    <h2 id="Stage_2__Practicing_Research_with_Mini_Projects" data-internal-id="Stage_2__Practicing_Research_with_Mini_Projects">
                        <span id="Stage_2__Practicing_Research_with_Mini_Projects">Stage 2: Practicing Research with Mini-Projects</span>
                    </h2>
                    <p id="block241">With that big picture in mind, let &#x27;s get our hands dirty. You want to do a series of ~1-5 day mini-projects, for maybe 2-4 weeks. The goal right now is to learn the craft, not to produce groundbreaking research.</p>
                    <p id="block242">Focus on practicing exploration and understanding and gaining the fast/medium skills, leave aside ideation and distillation for now. If you produce something cool and want to write it up, great! But that’s a nice-to-have, not a priority.</p>
                    <p id="block243">Once you finish a mini-project, remember to do a post-mortem. Spend at least an hour analyzing: what did you do? What did you try? What worked? What didn &#x27;t? What mistakes did you make? What would you do differently if doing this again? And how can you integrate this into your research strategy going forwards?</p>
                    <h3 id="Choose_A_Project" data-internal-id="Choose_A_Project">
                        <span id="Choose_A_Project">Choose A Project</span>
                    </h3>
                    <p id="block244">Some suggested starter projects</p>
                    <ul>
                        <li id="block245">
                            <strong>Replicate and Extend a Paper:</strong>
                            A classic for a reason. Replicate a key result, then extend it. Suggestions:
                            <ul>
                                <li id="block246">
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2406.11717">Refusal is mediated by a single direction</a>
                                        </span>
                                    </span>
                                    <ul>
                                        <li id="block247">Extending papers can vary a lot in difficulty. For example, applying the method to study refusal on a new model is easy as you can reuse the same data, while applying it to a new concept is harder.</li>
                                        <li id="block248">Skills: practicing activation patching and steering vectors.</li>
                                    </ul>
                                </li>
                                <li id="block249">
                                    <span>
                                        <span>
                                            <a href="http://thought-anchors.com">Thought Anchors</a>
                                        </span>
                                    </span>
                                    : apply these reasoning model interpretability methods to new types of prompts, or explore some prompts using the linked interface, or see if you can improve on the methods/invent your own.
                                    <ul>
                                        <li id="block250">Skills: reasoning model interpretability, using LLM APIs, and working with modern models</li>
                                    </ul>
                                </li>
                                <li id="block251">
                                    Replicate the truth probes in 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2310.06824">Geometry of Truth</a>
                                        </span>
                                    </span>
                                    on a more modern model and try applying them in more interesting settings. How well do they generalise? Can you break them? If so, can you fix this?
                                    <ul>
                                        <li id="block252">Skills: probing, supervised learning, dataset creation</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block253">
                            <strong>Play around with something interesting:</strong>
                            <ul>
                                <li id="block254">
                                    Use 
                                    <span>
                                        <span>
                                            <a class="LinkStyles-link" href="https://www.neuronpedia.org/gemma-2-2b/graph">Neuronpedia &#x27;s attribution graphs</a>
                                        </span>
                                    </span>
                                    to form a hypothesis about Gemma 2B, then use other methods (e.g. prompting) to verify it.
                                    <ul>
                                        <li id="block255">Skills: Attribution graphs, scientific mindset, prompting</li>
                                    </ul>
                                </li>
                                <li id="block256">
                                    Play with 
                                    <span>
                                        <span>
                                            <a href="https://huggingface.co/collections/bcywinski/gemma-2-9b-it-taboo-6826efbb186dfce0616dd174">Bartosz Cywiński &#x27;s taboo models</a>
                                        </span>
                                    </span>
                                    that have a secret word programmed in and test as many methods as you can to find it.
                                    <ul>
                                        <li id="block257">If you’re feeling ambitious: train your own models with a more complex secret, and try to interpret those.</li>
                                        <li id="block258">Skills: Logit lens, SAEs, black box methods</li>
                                    </ul>
                                </li>
                                <li id="block259">
                                    Explore 
                                    <span>
                                        <span>
                                            <a href="https://github.com/clarifying-EM/model-organisms-for-EM">the models</a>
                                        </span>
                                    </span>
                                    from the 
                                    <span>
                                        <span>
                                            <a href="https://www.emergent-misalignment.com/">emergent</a>
                                        </span>
                                    </span>
                                    <a class="LinkStyles-link" href="/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy">misalignment</a>
                                    <span>
                                        <span>
                                            <a href="https://openai.com/index/emergent-misalignment/">papers</a>
                                        </span>
                                    </span>
                                    .
                                    <ul>
                                        <li id="block260">Skills: steering vectors, SAEs, maybe fine-tuning</li>
                                    </ul>
                                </li>
                                <li id="block261">
                                    Pick some prompts from 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2503.08679">Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</a>
                                        </span>
                                    </span>
                                    and try to gain a deeper understanding of what’s happening
                                    <ul>
                                        <li id="block262">Skills: Open ended exploration, using whichever tools seem appropriate</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block263">Those cover two kinds of starter projects:</p>
                    <ul>
                        <li id="block264">
                            <strong>Understanding-heavy</strong>
                            , where you take a well-known domain and try to test a hypothesis there (e.g. extending a paper you’ve read closely)
                            <ul>
                                <li id="block265">
                                    Note that you still want to do <i>some</i>
                                </li>
                            </ul>
                        </li>
                        <li id="block266">
                            <strong>Exploration-heavy</strong>
                            , where you take some phenomena (a technique, a model, a phenomena, etc) play around with it, and try to understand what’s going on.
                            <ul>
                                <li id="block267">Exploration-heavy projects are often a less familiar style, so make sure to do some of those!</li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block268">Common mistakes:</p>
                    <ul>
                        <li id="block269">People often get hung up on finding the “best” project. Sadly, that’s not going to happen. Instead, just do something and see what happens - better ideas and inspiration come with time.</li>
                        <li id="block270">Don &#x27;t get too attached to your first project. It was probably badly chosen! These are throwaway projects, just move on once you’re not learning as much.</li>
                        <li id="block271">Conversely, don &#x27;t flit between ideas so much that you never build your &quot;getting unstuck &quot;toolkit.</li>
                        <li id="block272">Avoid compute-heavy and/papers (e.g., training cross-layer transcoders) or highly technical papers (e.g., Sparse Feature Circuits).</li>
                    </ul>
                    <h3 id="Practicing_Exploration" data-internal-id="Practicing_Exploration">
                        <span id="Practicing_Exploration">Practicing Exploration</span>
                    </h3>
                    <p id="block273">The idea of exploration as a phase in itself often trips up people new to mech interp. They feel like they always need to have a plan, a clear thing they &#x27;re doing at any given point, etc. In my experience, you will often spend more than half of a project trying to figure out what the hell is happening and what you think your plan is. This is totally fine!</p>
                    <p data-internal-id="ftnt_ref14" id="block274">
                        <span id="ftnt_ref14">
                            You don &#x27;t need a plan. It &#x27;s okay to be confused. However, this does <i>not </i>
                            mean you should just screw around. Your North Star: gain information and surface area
                            <span data-footnote-reference="" data-footnote-index="14" data-footnote-id="xw1ra5pqnd" role="doc-noteref" id="fnrefxw1ra5pqnd" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fnxw1ra5pqnd" class="">[14]</a>
                                    </span>
                                </sup>
                            </span>
                            on the problem. Your job is to take actions that maximise information gained per unit time. If you &#x27;ve learned nothing in 2 hours, pivot to another approach. If 2-3 approaches were dead ends, it’s fine to just pick another problem.
                        </span>
                    </p>
                    <p id="block275">
                        I have 
                        <span>
                            <span>
                                <a href="https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T">several research walkthroughs on my YouTube channel</a>
                            </span>
                        </span>
                        that I think demonstrates the mindset of exploration. What I think is an appropriate speed to be moving. E.g. I think you should aim to make a new plot every few minutes (or faster!) if experiments don &#x27;t take too long to run.
                    </p>
                    <p id="block276">A common difficulty is feeling “stuck” and not knowing what to do. IMO, this is largely a skill issue. Here &#x27;s my recommended toolkit when this happens:</p>
                    <ul>
                        <li id="block277">Use &quot;gain surface area &quot;techniques, things that can surface new ideas and connections and just give you raw data to work with: look at the model &#x27;s output/chain-of-thought, change the prompt, probe for a concept, look at an SAE/attribution graph, read examples from your dataset, try logit lens or steering, etc.</li>
                        <li id="block278">
                            Set a 
                            <span>
                                <span>
                                    <a href="https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers">5-minute timer</a>
                                </span>
                            </span>
                            and brainstorm things you &#x27;re curious about or directions to try.
                        </li>
                        <li id="block279">
                            If you’re confused/curious about something, set a 
                            <span>
                                <span>
                                    <a href="https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers">5 minute timer</a>
                                </span>
                            </span>
                            and brainstorm what could be happening.
                        </li>
                    </ul>
                    <p id="block280">Other advice:</p>
                    <ul>
                        <li id="block281">
                            Before any &gt;30 minute experiment, stop and brainstorm alternatives. Is this <i>really</i>
                            the fastest way to gain information?
                        </li>
                        <li id="block282">It &#x27;s totally fine to pause for half a day to go learn some key background knowledge.</li>
                        <li id="block283">
                            Get in the habit of keeping a research log of your findings and a &quot;highlights &quot;doc for the really cool stuff.
                            <ul>
                                <li id="block284">If applicable, it can be cool to have your research log be a slack/discord channel</li>
                            </ul>
                        </li>
                        <li id="block285">Remember: when exploring and thinking through how to explain mysterious phenomena, most of your probability mass should be on &quot;something I haven &#x27;t thought of yet.&quot;</li>
                        <li id="block286">Practice following your curiosity, but be aware that it’ll often lead you astray at first. When it does, pay attention! What can you learn from this?</li>
                    </ul>
                    <h3 id="Practicing_Understanding" data-internal-id="Practicing_Understanding">
                        <span id="Practicing_Understanding">Practicing Understanding</span>
                    </h3>
                    <p id="block287">If exploration goes well, you &#x27;ll start to form hunches about the problem. E.g. thinking that you are successfully (linearly) probing for some concept. Or that you found a direction that mediates refusal. Or that days of the week are represented as a circle in a 2D subspace.</p>
                    <p id="block288">Once you have this, you want to go to figure out if it &#x27;s actually true. Be warned, the feeling of “being really convinced that it &#x27;s true” is very different from actually being true. Part of being a good researcher is being good enough at testing and falsifying your pet hypotheses that, when you fail to falsify one, there’s a good chance that it &#x27;s true. But you &#x27;re probably not there yet.</p>
                    <p id="block289">Note: While I find it helpful to think of these as discrete stages, often you &#x27;ll be flitting back and forth. A great way to explore is coming up with guesses and micro-hypotheses about what &#x27;s going on, running a quick experiment to test them, and integrating the results into your understanding of the problem, going back to the drawing board.</p>
                    <p id="block290">Your North Star: convince yourself a hypothesis is true or false. The key mindset is skepticism. Advice:</p>
                    <ul>
                        <li id="block291">Before testing a hypothesis, set a five-minute timer and brainstorm, &quot;What are the ways this could be false?&quot;</li>
                        <li id="block292">
                            Alternatively, write out the best possible case for your hypothesis and see where the argument feels weak.
                            <ul>
                                <li id="block293">Try using an LLM with an anti-sycophancy prompt (&quot;My friend wrote this and wants brutal feedback...&quot;) to red-team your arguments - it probably won’t work, but might be helpful</li>
                            </ul>
                        </li>
                        <li id="block294">Or set a 5 minute timer and brainstorm alternative explanations for your observations</li>
                    </ul>
                    <p id="block295">
                        You then want to convert these flaws and alternative hypotheses into concrete experiments. <strong>Experiment design is a deep skill</strong>
                        . Honestly, I &#x27;m not sure how to teach it other than through experience. But one recommendation is to pay close attention to the experiments in papers you admire and analyze what made them so clever and effective. I also recommend that, every time you feel like you’ve (approximately) proven or falsified a hypothesis, adding them to a running doc of “things I believe to be true” with hypotheses, experiments, and results.
                    </p>
                    <h3 id="Using_LLMs_for_Research_Code" data-internal-id="Using_LLMs_for_Research_Code">
                        <span id="Using_LLMs_for_Research_Code">Using LLMs for Research Code</span>
                    </h3>
                    <p id="block296">In my opinion, coding is one of the domains where LLMs are most obviously useful. It was very striking to me how much better my math scholars were six months ago than 12 months ago, and I think a good chunk of this is attributable by them having much better LLMs to use. If you are not using LLMs as a core part of your coding workflow, I think you &#x27;re making a mistake.</p>
                    <ul>
                        <li id="block297">
                            <strong>Use</strong>
                            <span>
                                <span>
                                    <a href="http://cursor.com/">
                                        <strong>Cursor</strong>
                                    </a>
                                </span>
                            </span>
                            <strong>:</strong>
                            It &#x27;s VS Code with fantastic AI integration. Make sure to add the docs for libraries with @ so the AI has context. The $20/month plan is worth it, if possible, and there’s a 
                            <span>
                                <span>
                                    <a href="https://cursor.com/students">free student version</a>
                                </span>
                            </span>
                            .
                            <ul>
                                <li id="block298">Claude Code is tempting but bad for learning and iteration. I’d use it for throwaway things and first drafts - if the draft has a bunch of bugs, go read the code yourself/throw it away and start again. Cursor facilitates reading the AI’s code better than Claude code does IMO</li>
                            </ul>
                        </li>
                        <li id="block299">
                            <strong>A caveat:</strong>
                            If learning a new library (like in ARENA), first try writing things yourself. Use the LLM when stuck, not to replace the learning process.
                        </li>
                        <li id="block300">Later on, when thinking about writing up results, if key experiments were mostly vibe-coded, I recommend re-implementing them by hand to make sure no dumb LLM bugs slipped in.</li>
                    </ul>
                    <h2 id="Interlude__What_s_New_In_Mechanistic_Interpretability_" data-internal-id="Interlude__What_s_New_In_Mechanistic_Interpretability_">
                        <span id="Interlude__What_s_New_In_Mechanistic_Interpretability_">Interlude: What’s New In Mechanistic Interpretability?</span>
                    </h2>
                    <p id="block301">
                        <i>Feel free to skip to the “</i>
                        <a class="LinkStyles-link" href="/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher#Stage_3__Working_Up_To_Full_Research_Projects">
                            <i>what should I do next</i>
                        </a>
                        <i>” part</i>
                    </p>
                    <p id="block302">Things move fast in mechanistic interpretability. Newcomers to the field who &#x27;ve kept up from afar are often pretty out of date. Here &#x27;s what I think you need to know, again, filtered through my own opinions and biases.</p>
                    <h3 id="Avoiding_Fads" data-internal-id="Avoiding_Fads">
                        <span id="Avoiding_Fads">Avoiding Fads</span>
                    </h3>
                    <p id="block303">
                        This interlude is particularly important because <strong>the field often has fads</strong>
                        : lines of research that are very popular for a year or so, make some progress and find many limitations, and then the field moves on. But if you’re new, and catching up on the literature, you might not realise. I often see people new to the field working on older things, that I don’t think are too productive to work on any more. Historical fads include:
                    </p>
                    <ul>
                        <li id="block304">
                            Interpreting toy models trained on algorithmic tasks (e.g. my 
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2301.05217">grokking work</a>
                                </span>
                            </span>
                            )
                            <ul>
                                <li id="block305">I no longer recommend working on this, as I think we basically know that “sometimes models trained on algorithmic tasks are interpretable”, and they’re sufficiently artificial and divorced from real models that I am pessimistic about deeper and more specific insights generalising</li>
                            </ul>
                        </li>
                        <li id="block306">
                            Circuit analysis via causal interventions on model components (e.g. the 
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2211.00593">IOI paper</a>
                                </span>
                            </span>
                            )
                            <ul>
                                <li id="block307">This is slightly more complicated. I think that &#x27;s worth learning about, and techniques like activation and attribution patching are genuinely useful.</li>
                                <li id="block308">But the core problem is that once you got a sparse subgraph of a model responsible for a task, there wasn &#x27;t really a “what next?”. This didn &#x27;t tend to result in deeper insight because the nodes (eg layers or maybe attention heads) weren &#x27;t monosemantic, and it was often more complicated than naive stories suggested but we didn’t have the tools to dig deeper.</li>
                                <li id="block309">It was pretty cool to see that this was possible at all, but there have been more than enough works in this area that the bar for a novel contribution is now much higher.</li>
                                <li id="block310">
                                    Simply identifying a circuit is no longer enough; you need to use that circuit to reveal a deeper, non-obvious property of the model. I recommend exploring 
                                    <span>
                                        <span>
                                            <a class="LinkStyles-link" href="https://www.neuronpedia.org/graph/info">attribution-graph style approaches</a>
                                        </span>
                                    </span>
                                </li>
                            </ul>
                        </li>
                        <li id="block311">
                            <p data-internal-id="ftnt_ref15" id="block312">
                                <span id="ftnt_ref15">
                                    We &#x27;re at the tail end of a fad of incremental 
                                    <span>
                                        <span>
                                            <a href="https://transformer-circuits.pub/2023/monosemantic-features">sparse autoencoder research</a>
                                        </span>
                                    </span>
                                    <span data-footnote-reference="" data-footnote-index="15" data-footnote-id="tq4gws0zq69" role="doc-noteref" id="fnreftq4gws0zq69" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fntq4gws0zq69" class="">[15]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    (i.e. focusing on simple uses and refinements of the basic technique)
                                </span>
                            </p>
                            <ul>
                                <li id="block313">
                                    <p data-internal-id="ftnt_ref15" id="block314">
                                        <span id="ftnt_ref15">Calling this one a fad is probably more controversial (if only because it &#x27;s more recent).</span>
                                    </p>
                                </li>
                                <li id="block315">
                                    <p data-internal-id="ftnt_ref15" id="block316">
                                        <span id="ftnt_ref15">
                                            The <i>specific</i>
                                            thing I am critiquing is the spate of papers, including ones I was involved in, that are about incremental improvements to the sparse autoencoder architecture, or initial demonstrations that you can apply SAEs to do things, or picking some downstream task and seeing what SAEs do on it.
                                        </span>
                                    </p>
                                    <ul>
                                        <li id="block317">
                                            <p data-internal-id="ftnt_ref15" id="block318">
                                                <span id="ftnt_ref15">
                                                    I think this made some sense when it seemed like SAEs could be a total gamechanger for the field, and where we were learning things from each new such paper. I think this moment has passed; I do not think they were a gamechanger in the way that I hoped they might be. See <a class="LinkStyles-link" href="/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft">more of my thoughts here</a>
                                                    .
                                                </span>
                                            </p>
                                        </li>
                                    </ul>
                                </li>
                                <li id="block319">
                                    <p data-internal-id="ftnt_ref15" id="block320">
                                        <span id="ftnt_ref15">
                                            I am <i>not</i>
                                            discouraging work on the following:
                                        </span>
                                    </p>
                                    <ul>
                                        <li id="block321">
                                            <p data-internal-id="ftnt_ref15" id="block322">
                                                <span id="ftnt_ref15">
                                                    Attribution graph-based circuit analysis, which I don &#x27;t think has played out yet - see 
                                                    <span>
                                                        <span>
                                                            <a class="LinkStyles-link" href="https://www.neuronpedia.org/graph/info">a recent overview of that sub-field I co-wrote</a>
                                                        </span>
                                                    </span>
                                                    .
                                                </span>
                                            </p>
                                        </li>
                                        <li id="block323">
                                            <p data-internal-id="ftnt_ref15" id="block324">
                                                <span id="ftnt_ref15">
                                                    Trying meaningfully different approaches to dictionary learning (eg 
                                                    <span>
                                                        <span>
                                                            <a href="https://arxiv.org/abs/2506.20790">SPD</a>
                                                        </span>
                                                    </span>
                                                    or 
                                                    <span>
                                                        <span>
                                                            <a href="https://arxiv.org/abs/2505.17769">ITDA</a>
                                                        </span>
                                                    </span>
                                                    ), or things targeted to fix conceptual limitations of current techniques (eg 
                                                    <span>
                                                        <span>
                                                            <a href="https://arxiv.org/abs/2503.17547">Matryoshka</a>
                                                        </span>
                                                    </span>
                                                    ).
                                                </span>
                                            </p>
                                        </li>
                                        <li id="block325">
                                            <p data-internal-id="ftnt_ref15" id="block326">
                                                <span id="ftnt_ref15">
                                                    Using SAEs as a tool, whether as part of a broader project investigating weird phenomena in model biology, or as a baseline/approach on some downstream task. The key is that the project’s motivation should <i>not </i>
                                                    just be “what if we used SAEs for X?” unless there’s a good argument
                                                </span>
                                            </p>
                                            <ul>
                                                <li id="block327">
                                                    <p data-internal-id="ftnt_ref15" id="block328">
                                                        <span id="ftnt_ref15">I particularly recommend them for tasks where you don’t know exactly what you’re looking for, e.g. trying to explore some mysterious phenomena</span>
                                                    </p>
                                                </li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block329">
                        Note that I am putting this after stage 2 because I think that <strong>for initial throwaway projects you should </strong>
                        <i>
                            <strong>not</strong>
                        </i>
                        <strong>be stressing about novelty and avoiding fads</strong>
                        - your goal is just to learn. But as we move into stage 3 you should start to be a bit more mindful about choosing more exciting/impactful projects where possible.
                    </p>
                    <p id="block330">Also, take these as nudges and recommendations, not as instructions. If there &#x27;s a direction you believe in that fits the things I &#x27;m critiquing, maybe I &#x27;m just wrong, maybe your thing is an exception, go wild, see what happens.</p>
                    <h3 id="What_s_New_In_Mech_Interp_" data-internal-id="What_s_New_In_Mech_Interp_">
                        <span id="What_s_New_In_Mech_Interp_">What’s New In Mech Interp?</span>
                    </h3>
                    <p id="block331">OK, so those are my hot takes on what not to do. What should you do? I think that some really cool new opportunities have opened up in mech interp over the last year, and newcomers may not have come across these. Here are some of the key themes in my favorite papers over the last year, that I’d love to see readers build on:</p>
                    <ol>
                        <li id="block332">
                            <strong>Downstream tasks</strong>
                            . If our North Star is to be able to do useful things with interpretability, a great way to test our techniques is on real-world tasks, to see if they beat baselines or enable us to do a new thing
                            <ol>
                                <li id="block333">
                                    Even if we can &#x27;t directly test on the tasks we most care about, if models can let us do new things in <i>some</i>
                                    domains, that &#x27;s reasonable evidence that we &#x27;re learning something real in a way that is hard to fake.
                                </li>
                                <li id="block334">
                                    One of my favourite examples is <strong>auditing games</strong>
                                    : 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2503.10965">Marks et al</a>
                                        </span>
                                    </span>
                                    . is a fantastic paper that trained a model to have a hidden goal and then had different teams compete to find this goal, some with interpretability techniques, some without. Hypothesis: If interpretability does something real, then interp teams should win.
                                    <ol>
                                        <li id="block335">This study itself was a bit ambiguous (the mech interp teams were 1st and 3rd, a black box team was 2nd), but I think this is a fantastic way to test interpretability techniques, and has a ton of potential for follow-on work evaluating interpretability in different kinds of games.</li>
                                        <li id="block336">The general idea is just that there is a model with some known hidden property and different techniques are used to see which can elicit it. Things like having different teams, allowed to use different techniques, etc. is just one implementation.</li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                        <li id="block337">
                            <p data-internal-id="ftnt_ref16" id="block338">
                                <span id="ftnt_ref16">
                                    <strong>Model organisms</strong>
                                    : The auditing games paper was made possible by the fact that they were able to make a model with a hidden goal
                                    <span data-footnote-reference="" data-footnote-index="16" data-footnote-id="cdmsagzbqkp" role="doc-noteref" id="fnrefcdmsagzbqkp" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fncdmsagzbqkp" class="">[16]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    , a model organism to study. In general, we’re collecting techniques like 
                                    <span>
                                        <span>
                                            <a href="https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/">synthetic document fine-tuning</a>
                                        </span>
                                    </span>
                                    to make really interesting model organisms.
                                </span>
                            </p>
                            <ol>
                                <li id="block339">
                                    <p data-internal-id="ftnt_ref16" id="block340">
                                        <span id="ftnt_ref16">This kind of thing has a lot of potential! If we want to make a lie detector, a core challenge is that we don’t know how to test if it works or not. But if we can insert beliefs or deceptive behaviours into a model, many more projects become possible</span>
                                    </p>
                                </li>
                                <li id="block341">
                                    <p data-internal-id="ftnt_ref16" id="block342">
                                        <span id="ftnt_ref16">
                                            A great intro project is playing around with open source model organisms, e.g. from 
                                            <span>
                                                <span>
                                                    <a href="https://arxiv.org/abs/2505.14352">Cywinski et al</a>
                                                </span>
                                            </span>
                                        </span>
                                    </p>
                                </li>
                            </ol>
                        </li>
                        <li id="block343">
                            <strong>Practice on the real AGI Safety problems</strong>
                            : Historically, interpretability could only practice on very dull toy problems like 
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2301.05217">modular addition</a>
                                </span>
                            </span>
                            . But we now have models that exhibit complex behaviors that seem genuinely relevant to safety concerns, and we can just study them directly, making it far easier to make real progress.
                            <ol>
                                <li id="block344">
                                    E.g. <a class="LinkStyles-link" href="/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the">Rajamanoharan et al</a>
                                    debunking assumed self-preservation, and 
                                    <span>
                                        <span>
                                            <a href="https://www.apolloresearch.ai/research/deception-probes">Goldowsky-Dill et al</a>
                                        </span>
                                    </span>
                                    probing for deception
                                </li>
                                <li id="block345">
                                    Weird behaviours: models can 
                                    <span>
                                        <span>
                                            <a href="https://www.apolloresearch.ai/research/deception-probes">insider trade then lie about it</a>
                                        </span>
                                    </span>
                                    , 
                                    <span>
                                        <span>
                                            <a href="https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations">tell when they’re being evaluated</a>
                                        </span>
                                    </span>
                                    (and act differently), 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2412.14093">fake alignment</a>
                                        </span>
                                    </span>
                                    , 
                                    <span>
                                        <span>
                                            <a href="https://metr.org/blog/2025-06-05-recent-reward-hacking/">reward hack</a>
                                        </span>
                                    </span>
                                    , and more.
                                </li>
                            </ol>
                        </li>
                        <li id="block346">
                            <strong>Real-World Uses of Interpretability</strong>
                            : Model interpretability-based techniques are starting to have genuine uses in frontier language models!
                            <ol>
                                <li id="block347">
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/1610.01644">Linear probes</a>
                                        </span>
                                    </span>
                                    , one of the simplest possible techniques, are a highly competitive way to 
                                    <span>
                                        <span>
                                            <a href="https://alignment.anthropic.com/2025/cheap-monitors/">cheaply monitor systems</a>
                                        </span>
                                    </span>
                                    for things like users trying to make bioweapons.
                                </li>
                                <li id="block348">I find it incredibly cool that interpretability can actually be useful, and kind of embarrassing that only a decade-old technique seems very helpful. Someone should do something about that. Maybe that someone could be you!</li>
                                <li id="block349">This needs a very different kind of research: careful evaluation, comparison to strong baselines, and refinement of methods</li>
                            </ol>
                        </li>
                        <li id="block350">
                            <strong>Attribution graph-based circuit analysis</strong>
                            . The core problem with trying to analyze circuits in terms of things like a model &#x27;s attention heads and layers is that often these things don &#x27;t actually have a clear meaning. 
                            <span>
                                <span>
                                    <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">Attribution graphs</a>
                                </span>
                            </span>
                            use techniques like 
                            <span>
                                <span>
                                    <a href="https://arxiv.org/abs/2406.11944">transcoders</a>
                                </span>
                            </span>
                            , popularized in 
                            <span>
                                <span>
                                    <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Anthropic &#x27;s model biology</a>
                                </span>
                            </span>
                            work, to approximate models with a computational graph with meaningful nodes.
                            <ol>
                                <li id="block351">
                                    <p data-internal-id="ftnt_ref17" id="block352">
                                        <span id="ftnt_ref17">
                                            See this 
                                            <span>
                                                <span>
                                                    <a class="LinkStyles-link" href="https://www.neuronpedia.org/graph/info">cross-org blog post</a>
                                                </span>
                                            </span>
                                            for the ongoing follow-on work across the community, and an open problems list I co-wrote!
                                            <span data-footnote-reference="" data-footnote-index="17" data-footnote-id="p0f0m03b55r" role="doc-noteref" id="fnrefp0f0m03b55r" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fnp0f0m03b55r" class="">[17]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                        </span>
                                    </p>
                                </li>
                                <li id="block353">
                                    You can make and analyse your own attribution graphs on 
                                    <span>
                                        <span>
                                            <a class="LinkStyles-link" href="https://www.neuronpedia.org/gemma-2-2b/graph">Neuronpedia</a>
                                        </span>
                                    </span>
                                </li>
                            </ol>
                        </li>
                        <li id="block354">
                            <strong>Understanding model failures</strong>
                            : Models often do weird things. If we were any good at interpretability, we should be able to understand these. Recently, we’ve seen signs of life!
                            <ol>
                                <li id="block355">
                                    <span>
                                        <span>
                                            <a href="https://transluce.org/observability-interface">Meng et al</a>
                                        </span>
                                    </span>
                                    on why some models think 9.8 &lt;9.11
                                </li>
                                <li id="block356">
                                    <p data-internal-id="ftnt_ref18" id="block357">
                                        <span id="ftnt_ref18">
                                            A line of work studying 
                                            <span>
                                                <span>
                                                    <a href="https://www.emergent-misalignment.com/">emergent misalignment</a>
                                                </span>
                                            </span>
                                            - why training models on narrowly evil tasks like writing insecure code turns them into Nazis - has found some insights. 
                                            <span>
                                                <span>
                                                    <a href="https://arxiv.org/abs/2506.19823">Wang et al</a>
                                                </span>
                                            </span>
                                            found this was driven by sparse autoencoder latents
                                            <span data-footnote-reference="" data-footnote-index="18" data-footnote-id="g12d8d1lqu" role="doc-noteref" id="fnrefg12d8d1lqu" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fng12d8d1lqu" class="">[18]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                            associated with movie villains, and in <a class="LinkStyles-link" href="/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy">Turner et al</a>
                                            we found that the model <i>could </i>
                                            have learned the narrow solution, but this was in some sense less “efficient” and “stable”
                                        </span>
                                    </p>
                                </li>
                            </ol>
                        </li>
                        <li id="block358">
                            <p data-internal-id="ftnt_ref20" id="block359">
                                <span id="ftnt_ref20">
                                    <strong>Automated interpretability</strong>
                                    : Using LLMs to automate interpretability. We saw signs of life on this from Bills et al and 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2404.14394">Shaham et al</a>
                                        </span>
                                    </span>
                                    , but LLMs are actually good now! It’s now possible to make basic interpretability agents that can do things like 
                                    <span>
                                        <span>
                                            <a href="https://alignment.anthropic.com/2025/automated-auditing/">solve auditing games</a>
                                        </span>
                                    </span>
                                    <span data-footnote-reference="" data-footnote-index="19" data-footnote-id="0td6a2gxwht" role="doc-noteref" id="fnref0td6a2gxwht" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn0td6a2gxwht" class="">[19]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    . And interpretability agents are the worst they’ll ever be
                                    <span data-footnote-reference="" data-footnote-index="20" data-footnote-id="5bdglmkdzr" role="doc-noteref" id="fnref5bdglmkdzr" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn5bdglmkdzr" class="">[20]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    .
                                </span>
                            </p>
                        </li>
                        <li id="block360">
                            <p data-internal-id="ftnt_ref22" id="block361">
                                <span id="ftnt_ref22">
                                    <strong>Reasoning model interpretability</strong>
                                    : All current frontier models are reasoning models—models that are trained with reinforcement learning to think
                                    <span data-footnote-reference="" data-footnote-index="21" data-footnote-id="wuxdh4f7kh" role="doc-noteref" id="fnrefwuxdh4f7kh" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnwuxdh4f7kh" class="">[21]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    for a while before producing an answer. In my opinion, this requires a major rethinking of many existing interpretability approaches
                                    <span data-footnote-reference="" data-footnote-index="22" data-footnote-id="3qxoen8tddk" role="doc-noteref" id="fnref3qxoen8tddk" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn3qxoen8tddk" class="">[22]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    , and calls for exploring new paradigms. IMO this is currently being neglected by the field, but will become a big deal.
                                </span>
                            </p>
                            <ol>
                                <li id="block362">
                                    <p data-internal-id="ftnt_ref22" id="block363">
                                        <span id="ftnt_ref22">
                                            In 
                                            <span>
                                                <span>
                                                    <a href="http://thought-anchors.com">Bogdan et al</a>
                                                </span>
                                            </span>
                                            , we explored what a possible paradigm could look like. Notably, there are far more interesting and sophisticated black box techniques with reasoning models, like resampling the second half of the chain of thought, or every time the model says a specific kind of sentence, deleting and regenerating that sentence.
                                        </span>
                                    </p>
                                </li>
                            </ol>
                        </li>
                    </ol>
                    <h3 id="A_Pragmatic_Vision_for_Mech_Interp" data-internal-id="A_Pragmatic_Vision_for_Mech_Interp">
                        <span id="A_Pragmatic_Vision_for_Mech_Interp">A Pragmatic Vision for Mech Interp</span>
                    </h3>
                    <p id="block364">Attentive readers may notice that the list above focuses on work to do with understanding the more qualitative high-level properties of models, and not ambitious reverse engineering. This is largely because, in my opinion, the former has gone great, while we have not seen much progress towards the fundamental blockers on the latter.</p>
                    <p id="block365">I used to be very excited about ambitious reverse engineering, but I currently think that the dream of completely reverse engineering a model down to something human understandable seems basically doomed. My interpretation of the research so far is that models have some human understandable high-level structure that drives important actions, and a very long tail of increasingly niche and irrelevant heuristics and biases. For pragmatic purposes, these can be largely ignored, but not if we want things like guarantees, or to claim that we have understood most of a model. I think that trying to understand as much as we can is still a reasonable proxy for getting to the point of being pragmatically useful, but think it’s historically been too great a focus of the field, and many other approaches seem more promising if our ultimate goals are pragmatic.</p>
                    <p id="block366">In some ways, this has actually made me more optimistic about interpretability ultimately being useful for AGI safety! Ambitious reverse engineering would be awesome but was always a long shot. But I think we &#x27;ve seen some real results for pragmatic approaches to mechanistic interpretability, and feel fairly confident we are going to be able to do genuinely useful things that are hard to achieve with other methods.</p>
                    <h2 id="Stage_3__Working_Up_To_Full_Research_Projects" data-internal-id="Stage_3__Working_Up_To_Full_Research_Projects">
                        <span id="Stage_3__Working_Up_To_Full_Research_Projects">Stage 3: Working Up To Full Research Projects</span>
                    </h2>
                    <p id="block367">Once you have a few mini-projects done, you should start being more ambitious. You want to think about gaining the deeper (medium/slow) skills, and exploring ideation and distillation.</p>
                    <p id="block368">
                        However, you should still expect projects to often fail, and want to lean into breadth over depth and avoid getting bogged down in an unsuccessful project you can’t bear to give up on. To resolve this tension, I recommend <strong>working in 1-2 week sprints</strong>
                        . At the end of each sprint, reflect and make a deliberate decision: <strong>continue, or pivot?</strong>
                        The default should be to pivot unless the project feels truly promising. It’s great to give up on things, if it means you spend your time even better! But if it’s going great, by all means continue.
                    </p>
                    <p id="block369">
                        This strategy should mean that you eventually end up working on something longer-term when you find something <i>good</i>
                        , but don &#x27;t just get bogged down in the first ambitious idea you tried.
                    </p>
                    <p id="block370">I recommend reviewing the list of skills earlier and just for each one, reflecting for a bit on how on top of it you think you feel and how you could intentionally practice it in your next project. Then after each sprint, before deciding whether to pivot, take an hour or two to do a post-mortem: what did you learn, what progress did you make on different skills, and what would you do differently next time? Your goal is to learn, and you learn much better if you make time to actually process your accumulated data!</p>
                    <h3 id="Key_Research_Mindsets" data-internal-id="Key_Research_Mindsets">
                        <span id="Key_Research_Mindsets">Key Research Mindsets</span>
                    </h3>
                    <p id="block371">
                        One way to decompose your learning is to think about research mindsets: the traits and mindsets a good researcher needs to have, that cut across many of these stages. See <a class="LinkStyles-link" href="/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking">my blog post on the topic for more</a>
                        , but here &#x27;s a brief view of how I &#x27;m currently thinking about it.
                    </p>
                    <ol>
                        <li id="block372">
                            <p data-internal-id="ftnt_ref23" id="block373">
                                <span id="ftnt_ref23">
                                    <strong>Skepticism/Truth-seeking:</strong>
                                    The default state of the world is that your research is false, because doing research is hard. Your north star should always be to find <i>true </i>
                                    insights
                                    <span data-footnote-reference="" data-footnote-index="23" data-footnote-id="lm5ixkfuzk" role="doc-noteref" id="fnreflm5ixkfuzk" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnlm5ixkfuzk" class="">[23]</a>
                                            </span>
                                        </sup>
                                    </span>
                                </span>
                            </p>
                            <ol>
                                <li id="block374">
                                    <p data-internal-id="ftnt_ref23" id="block375">
                                        <span id="ftnt_ref23">It generally doesn &#x27;t come naturally to people to constantly aggressively think about all the ways their work could be false and make a good faith effort to test it. You can learn to do better than this, but it often takes practice.</span>
                                    </p>
                                </li>
                                <li id="block376">
                                    <p data-internal-id="ftnt_ref23" id="block377">
                                        <span id="ftnt_ref23">This is crucial in understanding, somewhat important in exploration, and crucial in distillation.</span>
                                    </p>
                                </li>
                                <li id="block378">
                                    <p data-internal-id="ftnt_ref23" id="block379">
                                        <span id="ftnt_ref23">A common mistake is to grasp at straws to find a “positive” result, thinking that nothing else is worth sharing.</span>
                                    </p>
                                    <ol>
                                        <li id="block380">
                                            <p data-internal-id="ftnt_ref23" id="block381">
                                                <span id="ftnt_ref23">In my opinion, negative or inconclusive results that are well-analyzed are much better than a poorly supported positive result. I’ll often think well of someone willing to release nuanced negative results, and poorly of someone who pretends their results are better than they are.</span>
                                            </p>
                                        </li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                        <li id="block382">
                            <strong>Prioritization:</strong>
                            Your time is scarce. Research involves making a bunch of decisions that are essentially searching through a high-dimensional space. The difference between a great and a mediocre researcher is being able to make these decisions well.
                            <ol>
                                <li id="block383">If you have a good mentor, you can lean on them for this at first, but you will need to learn how to do this yourself eventually.</li>
                                <li id="block384">This is absolutely crucial in exploration and ideation, but fairly important throughout.</li>
                                <li id="block385">A good way to learn this one is to reflect on decisions you &#x27;ve made after the fact, eg in a sprint post-mortem, and think about how you could have made them better, and what generalisable lessons to take to the future</li>
                            </ol>
                        </li>
                        <li id="block386">
                            <p data-internal-id="ftnt_ref24" id="block387">
                                <span id="ftnt_ref24">
                                    <strong>Productivity</strong>
                                    <span data-footnote-reference="" data-footnote-index="24" data-footnote-id="idab8074tka" role="doc-noteref" id="fnrefidab8074tka" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnidab8074tka" class="">[24]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    <strong>:</strong>
                                    The best researchers I &#x27;ve worked with get more than twice as much done as the merely good ones. Part of this is good research taste and making good prioritization decisions, but part of this is just being good at getting shit done.
                                </span>
                            </p>
                            <ol>
                                <li id="block388">
                                    <p data-internal-id="ftnt_ref24" id="block389">
                                        <span id="ftnt_ref24">Now, this doesn &#x27;t necessarily mean pushing yourself until the point of burnout by working really long hours. Or cutting corners and being sloppy. This is about productivity integrated over the long term.</span>
                                    </p>
                                    <ol>
                                        <li id="block390">
                                            <p data-internal-id="ftnt_ref24" id="block391">
                                                <span id="ftnt_ref24">For example, sometimes the most productive thing to do is to hold off on starting work, set a 5 minute timer, brainstorm possible things to do next, and then pick the best idea</span>
                                            </p>
                                        </li>
                                    </ol>
                                </li>
                                <li id="block392">
                                    <p data-internal-id="ftnt_ref24" id="block393">
                                        <span id="ftnt_ref24">This takes many forms, and the highest priority for you:</span>
                                    </p>
                                    <ol>
                                        <li id="block394">
                                            <p data-internal-id="ftnt_ref24" id="block395">
                                                <span id="ftnt_ref24">Know when to write good code without bugs, to avoid wasting time debugging later, and when to write a hacky thing that just works.</span>
                                            </p>
                                        </li>
                                        <li id="block396">
                                            <p data-internal-id="ftnt_ref24" id="block397">
                                                <span id="ftnt_ref24">Know the right keyboard shortcuts to move fast when coding.</span>
                                            </p>
                                        </li>
                                        <li id="block398">
                                            <p data-internal-id="ftnt_ref24" id="block399">
                                                <span id="ftnt_ref24">Know when to ask for help and have people who can help you get unblocked where appropriate.</span>
                                            </p>
                                        </li>
                                        <li id="block400">
                                            <p data-internal-id="ftnt_ref24" id="block401">
                                                <span id="ftnt_ref24">Be good at managing your time and tasks so that once you &#x27;ve decided what the highest priority thing to work on is, you in fact go and work on it.</span>
                                            </p>
                                        </li>
                                        <li id="block402">
                                            <p data-internal-id="ftnt_ref24" id="block403">
                                                <span id="ftnt_ref24">Be able to make time to achieve deep focus on the key problems.</span>
                                            </p>
                                        </li>
                                    </ol>
                                </li>
                                <li id="block404">
                                    <p data-internal-id="ftnt_ref24" id="block405">
                                        <span id="ftnt_ref24">
                                            Exercise: Occasionally <strong>audit your time</strong>
                                            . Use a tool like 
                                            <span>
                                                <span>
                                                    <a href="http://toggl.com">Toggl</a>
                                                </span>
                                            </span>
                                            for a day or two to log what you &#x27;re doing, then reflect: where did time go? What was inefficient? How could I do this 10% faster next time?
                                        </span>
                                    </p>
                                    <ol>
                                        <li id="block406">
                                            <p data-internal-id="ftnt_ref24" id="block407">
                                                <span id="ftnt_ref24">The goal isn &#x27;t to feel guilty, but to spot opportunities for improvement, like making a utility function for a tedious task.</span>
                                            </p>
                                        </li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                        <li id="block408">
                            <strong>Knowing the literature</strong>
                            : At this point, there’s a lot of accumulated wisdom (and a lot of BS) in prior papers, in mech interp and beyond.
                            <ol>
                                <li id="block409">
                                    This cuts across all stages:
                                    <ol>
                                        <li id="block410">In ideation, you don’t want to accidentally reinvent the wheel. And often great ideas are inspired by prior work</li>
                                        <li id="block411">In exploration, you want to be able to spot connections, borrow interesting techniques, etc</li>
                                        <li id="block412">In understanding, you want to know the right standards of proof to check for, the best techniques to use, alternative hypotheses (that may have been raised in other works), etc</li>
                                        <li id="block413">
                                            <p data-internal-id="ftnt_ref25" id="block414">
                                                <span id="ftnt_ref25">
                                                    In distillation, when writing a paper you’re expected to be able to contextualise it relative to existing work (i.e. write a related work section
                                                    <span data-footnote-reference="" data-footnote-index="25" data-footnote-id="wpekmwudkpd" role="doc-noteref" id="fnrefwpekmwudkpd" class="footnote-reference">
                                                        <sup>
                                                            <span>
                                                                <a href="#fnwpekmwudkpd" class="">[25]</a>
                                                            </span>
                                                        </sup>
                                                    </span>
                                                    ) which is important for other researchers knowing whether to care. And if you don’t know the standard methods of proof, key baselines everyone will ask about, key gotchas to check for etc, no one will believe your work.
                                                </span>
                                            </p>
                                        </li>
                                    </ol>
                                </li>
                                <li id="block415">
                                    LLMs are an incredibly useful tool here. GPT-5 thinking or Claude 4 with web search are both pretty useful tools here, as are the slower but more comprehensive deep research tools (Note that Google &#x27;s is available for free, as of the time of writing)
                                    <ol>
                                        <li id="block416">I recommend using these regularly and creatively throughout a project.</li>
                                        <li id="block417">You don &#x27;t necessarily need to go and read the works that get surfaced, but even just having LLM summaries can get you more awareness of what &#x27;s out there, and over time you &#x27;ll build this into deeper knowledge.</li>
                                    </ol>
                                </li>
                                <li id="block418">
                                    Of course, when there <i>does</i>
                                    seem to be a very relevant paper to your work, you should go do a deep dive and read it properly, not just relying on LLM summaries.
                                </li>
                                <li id="block419">Don’t stress - deep knowledge of the literature takes time to build. But you want to ensure you’re on an upwards gradient here, rather than assuming the broader literature is useless</li>
                                <li id="block420">
                                    <p data-internal-id="ftnt_ref26" id="block421">
                                        <span id="ftnt_ref26">
                                            On the flip side, many papers <i>are </i>
                                            highly misleading/outright false, so please don’t just critically believe them!
                                            <span data-footnote-reference="" data-footnote-index="26" data-footnote-id="1bau7vsh9tk" role="doc-noteref" id="fnref1bau7vsh9tk" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fn1bau7vsh9tk" class="">[26]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                        </span>
                                    </p>
                                </li>
                            </ol>
                        </li>
                    </ol>
                    <p id="block422">Okay, so how does this all tie back to the stages of research? Now you &#x27;re going to be thinking about all four. We &#x27;ll start by talking about how to deepen your existing skills with exploration and understanding, and then we &#x27;ll talk about what practicing ideation and actually writing up your work should look like.</p>
                    <h3 id="Deepening_Your_Skills" data-internal-id="Deepening_Your_Skills">
                        <span id="Deepening_Your_Skills">Deepening Your Skills</span>
                    </h3>
                    <p id="block423">You’ll still be exploring and understanding, but with a greater focus on rigor and the slower skills. In addition to the thoughts when discussing mindsets above, here’s some more specific advice</p>
                    <ul>
                        <li id="block424">
                            <strong>Deeper Exploration</strong>
                            is about internalizing the mindset of maximising productivity, which here means maximising information gain per unit time. Always ask, &quot;Am I learning something?&quot;
                            <ul>
                                <li id="block425">
                                    <i>Avoid Rabbit Holes:</i>
                                    A common mistake is finding one random anomaly and zooming in on it. Knowing when to pivot is crucial. Set a timer every hour or two to zoom out and ask if you’re making progress.
                                    <ul>
                                        <li id="block426">I recommend any time you notice yourself feeling a bit stuck or distracted or off track, setting a five minute timer and thinking about what could I be doing next, what should I be doing next, and am I doing the most important thing?</li>
                                    </ul>
                                </li>
                                <li id="block427">
                                    <i>Avoid Spreading Yourself Too Thin:</i>
                                    Doing lots of things superficially means none of them will be interesting.
                                </li>
                                <li id="block428">
                                    If you have spent more than five hours without learning something new, you should probably try a different approach
                                    <ul>
                                        <li id="block429">And if you have spent more than two days without learning something new, you should seriously consider pivoting and doing something else.</li>
                                    </ul>
                                </li>
                                <li id="block430">
                                    To practice prioritization, be intentional about your decisions: write down <i>why</i>
                                    you think an experiment is the right call, and later reflect on whether you were right. This makes your intuitions explicit and easier to update.
                                </li>
                            </ul>
                        </li>
                        <li id="block431">
                            <strong>Deeper Understanding</strong>
                            is about practicing skepticism and building a bulletproof case. Red-team your results relentlessly.
                            <ul>
                                <li id="block432">
                                    Some experiments are much more impactful and informative than others! Don &#x27;t just do the first experiment that pops into your head. Think about the key ways the hypothesis <i>could </i>
                                    be false, and how you could test that. Or about whether a skeptic could explain away a positive experimental results
                                    <ul>
                                        <li id="block433">A useful exercise is imagining you &#x27;re talking to a really obnoxious skeptic who keeps complaining that they don &#x27;t believe you and coming up with arguments for why your thing is wrong. What could you do such that they don &#x27;t have a leg to stand on?</li>
                                    </ul>
                                </li>
                                <li id="block434">Of course, there &#x27;s also an element of prioritization. Sometimes a shallow case that could be wrong is the right thing to aim for, if you’re working on an unimportant side claim/something that seems super plausible on priors, at which point you should just move on and do something else more interesting.</li>
                                <li id="block435">Exercise: To practice spotting subtle illusions, try red-teaming papers you read, thinking about potential flaws, and ideally run the experiments yourself.</li>
                            </ul>
                        </li>
                    </ul>
                    <p data-internal-id="Doing_Good_Science" id="block436">
                        <span id="Doing_Good_Science">Doing Good Science</span>
                    </p>
                    <ul>
                        <li id="block437">
                            <strong>Avoid cherry-picking</strong>
                            : Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling.
                            <ul>
                                <li id="block438">When you write up work, always include some randomly selected examples, especially if you present extensive qualitative analysis of specific things. It &#x27;s fine to put this in the appendix if space is scarce, but it should be there.</li>
                            </ul>
                        </li>
                        <li id="block439">
                            <strong>Use baselines</strong>
                            : A common mistake is for people to try to show a technique works by demonstrating it gets &#x27;decent &#x27;results, rather than showing it achieves better results than plausible alternatives that people might have used or are standard in the field. If you want people to e.g. use your cool steering vector results you need to show it beats changing the system prompt.
                        </li>
                        <li id="block440">
                            <strong>Don’t sandbag your baselines</strong>
                            : Similarly, it &#x27;s easy to put in much more effort finding good hyperparameters for your technique than for your baselines. Try to make sure you &#x27;re achieving comparable results with your baselines that prior work in the field has.
                        </li>
                        <li id="block441">
                            <strong>Do ablations on your fancy method</strong>
                            : It &#x27;s easy for people to have a fancy method with lots of moving parts, when many actually are unnecessary. You should always try removing one part and see if the method breaks. Do this for each part.
                            <ul>
                                <li id="block442">
                                    For example, the 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2403.03218v1">original unlearning method</a>
                                        </span>
                                    </span>
                                    in the 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2403.03218">RMU paper</a>
                                        </span>
                                    </span>
                                    claimed it was based on finding a meaningful steering vector, until follow-up work found that it was just about adding a vector with really high norm that broke the model, and a random vector performed just as well.
                                </li>
                            </ul>
                        </li>
                        <li id="block443">
                            <strong>(Informally) pre-register claims</strong>
                            : It &#x27;s important to clearly track which experimental results were obtained before versus after you formulated your claim. Post-hoc analysis (interpreting results after they &#x27;re seen) is inherently less impressive than predictions confirmed by pre-specified experiments
                        </li>
                        <li id="block444">
                            <strong>Be reproducible</strong>
                            : Where practical, share your code, data and models.
                            <ul>
                                <li id="block445">If you have time, make sure that it runs on a fresh machine and include a helpful readme that links to key model weights and datasets.</li>
                                <li id="block446">
                                    <p data-internal-id="ftnt_ref27" id="block447">
                                        <span id="ftnt_ref27">
                                            This both means others can check if your work is true and makes it more likely people will believe and build on your work
                                            <span data-footnote-reference="" data-footnote-index="27" data-footnote-id="adytzr5d7y" role="doc-noteref" id="fnrefadytzr5d7y" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fnadytzr5d7y" class="">[27]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                            because they can see replications that are more likely to exist and because it &#x27;s now low friction.
                                        </span>
                                    </p>
                                </li>
                            </ul>
                        </li>
                        <li id="block448">
                            <strong>Simplicity:</strong>
                            Bias towards trying the simple, obvious methods first. Fancy techniques can be a trap. Good research is pragmatic, not about showing off.
                            <ul>
                                <li id="block449">If you’re designing a fancy technique/experiment, each new detail is one more thing that can break</li>
                                <li id="block450">
                                    If trying to explain something mysterious, novice researchers often neglect simple, dumb hypotheses like “maybe MLP0 is incredibly important on <i>every </i>
                                    input, and there’s nothing special going on with my prompt”
                                </li>
                            </ul>
                        </li>
                        <li id="block451">
                            <strong>Be qualitative </strong>
                            <i>
                                <strong>and </strong>
                            </i>
                            <strong>quantitative</strong>
                            : One of the major drivers of progress of modern machine learning is being quantitative, having benchmarks and showing that a technique increases numbers on them. One of the key drivers of progress in mech interp is an openness to qualitative research: summary statistics lose a ton of information. What can we learn by actually looking deeply into what &#x27;s happening?
                            <ul>
                                <li id="block452">In my opinion, the best research tries to get the best of both worlds. It tries to understand what &#x27;s happening via qualitative analysis and then validates it with more quantitative methods. If your paper only does one, it’s probably missing out</li>
                            </ul>
                        </li>
                        <li id="block453">
                            <strong>Read your data</strong>
                            : A fantastic use of time, especially during the exploration phase, is just actually reading the data you &#x27;re working with, or model chains of thought and responses.
                            <ul>
                                <li id="block454">Often, the quality of the data is a crucial driver of the results of your experiments. Often, it is quite bad.</li>
                                <li id="block455">Sometimes most of the work of a project is in noticing flaws in your data and making a better data set. Time figuring this out is extremely well spent.</li>
                                <li id="block456">Ditto, include random examples of the data in an appendix for readers to do spot checks of their own.</li>
                            </ul>
                        </li>
                        <li id="block457">
                            <p data-internal-id="ftnt_ref28" id="block458">
                                <span id="ftnt_ref28">
                                    <strong>Don’t reinvent the wheel</strong>
                                    :  A common mistake in mech interp is doing something that &#x27;s already been done
                                    <span data-footnote-reference="" data-footnote-index="28" data-footnote-id="va7mhfkrhm" role="doc-noteref" id="fnrefva7mhfkrhm" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnva7mhfkrhm" class="">[28]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    . We have LLM-powered literature reviews now. You have way less of an excuse. Check first!
                                </span>
                            </p>
                        </li>
                        <li id="block459">
                            <strong>Excitement is evidence of bullshit</strong>
                            : Generally, most true results are not exciting, but a fair amount of false results are. So from a Bayesian perspective, if a result is exciting and cool, it’s even more likely to be false than normal!
                            <ul>
                                <li id="block460">Resist the impulse to get really excited! The correct attitude to exciting results is deep skepticism until you have tried really hard to falsify it and run out of ideas.</li>
                            </ul>
                        </li>
                    </ul>
                    <h3 id="Practicing_Ideation" data-internal-id="Practicing_Ideation">
                        <span id="Practicing_Ideation">Practicing Ideation</span>
                    </h3>
                    <p id="block461">
                        Okay, so you want to actually come up with good research ideas to work on. What does this look like? I recommend breaking this down into <strong>generating ideas</strong>
                        and then <strong>evaluating </strong>
                        them to find the best ones.
                    </p>
                    <p id="block462">To generate ideas, I &#x27;d often start with just taking a blank doc, blocking out at least an hour, and then just writing down as many ideas as you can come up with. Aim for quantity over quality. Go for at least 20.</p>
                    <p id="block463">There are other things you can do to help with generation:</p>
                    <ul>
                        <li id="block464">Throughout your previous sprints, every time you had an idle curiosity or noticed something weird, write it down in one massive long-running doc.</li>
                        <li id="block465">Likewise, when reading papers, note down confusions, curiosities, obviousnesses to do.</li>
                    </ul>
                    <p id="block466">Okay, so now you have a big list. What does finding the best ones look like?</p>
                    <ul>
                        <li id="block467">
                            Ideally, if you have a mentor or at least collaborators, you can just ask them to rate them.
                            <ul>
                                <li id="block468">If you do this, rate them yourself privately out of 10 before you look at their responses. Compare them and every time you have substantially different numbers, talk to the mentor and try to figure out why your intuitions disagree. This is a great source of supervised data for research taste.</li>
                            </ul>
                        </li>
                        <li id="block469">Even if you don’t have a mentor, I think that just going through, rating each idea yourself based on gut feel and sorting is as good a way to prune down a long list as any</li>
                        <li id="block470">
                            For the top few, I recommend trying to answer a few questions about them.
                            <ul>
                                <li id="block471">What would success look like here?</li>
                                <li id="block472">How surprised would I be if I did this for a month and nothing interesting had happened?</li>
                                <li id="block473">What skills does this require? Do I have them/could I easily gain them?</li>
                                <li id="block474">What models, data, computational resources, etc. does this require?</li>
                                <li id="block475">How does this compare to what the most relevant prior work did? Can I check for prior work and see if anything relevant comes up?</li>
                            </ul>
                        </li>
                    </ul>
                    <p data-internal-id="Research_Taste_Exercises" id="block476">
                        <span id="Research_Taste_Exercises">Research Taste Exercises</span>
                    </p>
                    <p id="block477">
                        Gaining research taste is slow because the feedback loops are long. You can accelerate it with exercises that give you faster, proxy feedback. (Credit to 
                        <span>
                            <span>
                                <a href="https://colah.github.io/notes/taste/">Chris Olah for inspiration here</a>
                            </span>
                        </span>
                        )
                    </p>
                    <ul>
                        <li id="block478">
                            If you have a mentor, query their taste for fast data and try to imitate it. Concretely:
                            <ul>
                                <li id="block479">Before each meeting, write a list of questions, then try to write up predictions for what the mentor will say, then actually ask the mentor, see what happens, and compare. If there are discrepancies, chat to the mentor and try to understand why.</li>
                                <li id="block480">Likewise, if the mentor makes a suggestion or asks a question you didn &#x27;t expect, try to ask questions about where the thought came from.</li>
                                <li id="block481">
                                    <p data-internal-id="ftnt_ref29" id="block482">
                                        <span id="ftnt_ref29">
                                            Regularly paraphrase back to the mentor in your own words what you think they &#x27;re saying, and then ask them to correct anything you &#x27;re wrong about
                                            <span data-footnote-reference="" data-footnote-index="29" data-footnote-id="tt0owz8koks" role="doc-noteref" id="fnreftt0owz8koks" class="footnote-reference">
                                                <sup>
                                                    <span>
                                                        <a href="#fntt0owz8koks" class="">[29]</a>
                                                    </span>
                                                </sup>
                                            </span>
                                        </span>
                                    </p>
                                </li>
                            </ul>
                        </li>
                        <li id="block483">
                            <strong>Learning from papers as &quot;offline data &quot;:</strong>
                            When you read a paper, don &#x27;t just passively consume it. Read the introduction, then stop. Try to predict what methods they used and what their key results will be. Then, continue reading and see how your predictions compare. Analyze why the authors made different choices. This trains your intuition on a much larger and faster dataset than your own research.
                        </li>
                    </ul>
                    <p id="block484">
                        It’s also worth dwelling on what research taste actually is. <a class="LinkStyles-link" href="/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research">See my post</a>
                        for more, but I break it down as follows:
                    </p>
                    <ol>
                        <li id="block485">
                            <strong>Intuition (System 1):</strong>
                            This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.
                        </li>
                        <li id="block486">
                            <strong>Conceptual Framework (System 2)</strong>
                            : This is deep domain knowledge and understanding of underlying principles.
                        </li>
                        <li id="block487">
                            <strong>Strategic Big Picture</strong>
                            : Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?
                        </li>
                    </ol>
                    <h3 id="Write_up_your_work_" data-internal-id="Write_up_your_work_">
                        <span id="Write_up_your_work_">Write up your work!</span>
                    </h3>
                    <p id="block488">
                        At this stage, you should be thinking seriously about how to write up your work. Often, writing up work is the first time you really understand what a project has been about, or you identify key limitations, or experiments you forgot to do. You should check out <a class="LinkStyles-link" href="/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers">my blog post on writing ML papers</a>
                        for much more detailed thoughts (which also apply to high-effort blog posts!) but I &#x27;ll try to summarize them below.
                    </p>
                    <p data-internal-id="Why_aim_for_public_output_" id="block489">
                        <span id="Why_aim_for_public_output_">Why aim for public output?</span>
                    </p>
                    <p data-internal-id="ftnt_ref30" id="block490">
                        <span id="ftnt_ref30">
                            If producing something public is intimidating, for now, you can start by just writing up a private Google Doc and maybe share it with some friends or collaborators. But I heavily encourage people to aim for public output where they can. Generally, your research will not matter if no one reads it. The goal of research is to contribute to the sum of human
                            <span data-footnote-reference="" data-footnote-index="30" data-footnote-id="e3252d8idmr" role="doc-noteref" id="fnrefe3252d8idmr" class="footnote-reference">
                                <sup>
                                    <span>
                                        <a href="#fne3252d8idmr" class="">[30]</a>
                                    </span>
                                </sup>
                            </span>
                            knowledge. And if no one understands what you did, then it doesn &#x27;t matter.
                        </span>
                    </p>
                    <p id="block491">
                        Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, <strong>public research output is your best credential</strong>
                        . It &#x27;s very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they &#x27;re trying to figure out whether they should work with you, pay attention to what you &#x27;re saying, etc. It doesn’t matter if you wrote it in a prestigious PhD program or as a random independent researcher, if it’s good enough then people care.
                    </p>
                    <p id="block492">There are a few options for what this can look like:</p>
                    <ul>
                        <li id="block493">A blog post (e.g. on a personal blog or LessWrong) - the simplest and least formal kind</li>
                        <li id="block494">
                            <p data-internal-id="ftnt_ref31" id="block495">
                                <span id="ftnt_ref31">
                                    An Arxiv paper - much more legible than a blog post, and honestly not much extra effort if you have a high-quality blog post
                                    <span data-footnote-reference="" data-footnote-index="31" data-footnote-id="8354hd0flji" role="doc-noteref" id="fnref8354hd0flji" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn8354hd0flji" class="">[31]</a>
                                            </span>
                                        </sup>
                                    </span>
                                </span>
                            </p>
                        </li>
                        <li id="block496">
                            <p data-internal-id="ftnt_ref32" id="block497">
                                <span id="ftnt_ref32">
                                    A workshop paper
                                    <span data-footnote-reference="" data-footnote-index="32" data-footnote-id="9oppcf0ftrh" role="doc-noteref" id="fnref9oppcf0ftrh" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn9oppcf0ftrh" class="">[32]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    (i.e. something you submit for peer review to a workshop, typically part of a major ML conference, the bar is much lower than for a conference paper)
                                </span>
                            </p>
                        </li>
                        <li id="block498">
                            <p data-internal-id="ftnt_ref34" id="block499">
                                <span id="ftnt_ref34">
                                    A conference paper (the equivalent of top journals in ML, there’s a reasonably high quality bar
                                    <span data-footnote-reference="" data-footnote-index="33" data-footnote-id="fmh579omuc6" role="doc-noteref" id="fnreffmh579omuc6" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnfmh579omuc6" class="">[33]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    , but also a <i>lot </i>
                                    of noise
                                    <span data-footnote-reference="" data-footnote-index="34" data-footnote-id="f09vsa4w37e" role="doc-noteref" id="fnreff09vsa4w37e" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnf09vsa4w37e" class="">[34]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    )
                                </span>
                            </p>
                        </li>
                    </ul>
                    <p id="block500">If this all seems overwhelming, starting out with blog posts is fine, but I think people generally overestimate the bar for arxiv or workshop papers - if you think you learned something cool in a project, this is totally worth turning into a paper!</p>
                    <p data-internal-id="How_to_write_stuff_up_" id="block501">
                        <span id="How_to_write_stuff_up_">How to write stuff up?</span>
                    </p>
                    <p id="block502">The core of a paper is the narrative. Readers will not take away more than a few sentences worth of content. Your job is to make sure these are the right handful of sentences and make sure the reader is convinced of them.</p>
                    <p id="block503">You want to distill your paper down into one to three key claims (your contribution), the evidence you provide that the contribution is true, the motivation for why a reader should care about them, and work all of this into a coherent narrative.</p>
                    <p id="block504">
                        <strong>Iterate</strong>
                        : I &#x27;m a big fan of writing things iteratively. You first figure out the contribution and narrative. You then write a condensed summary, the abstract (in a blog post, this should be a TL;DR/executive summary - also very important!). You then write a bullet point outline of the paper: what points you want to cover, what evidence you want to provide, how you intend to build up to that evidence, how you want to structure and order things, etc. If you have mentors or collaborators, the bullet point outline is often the best time to get feedback. Or the narrative formation stage, if you have an engaged mentor. Then write the introduction, and make sure you’re happy with that. Then (or even before the intro) make the figures - figures are incredibly important! Then flesh it out into prose. People spend a <i>lot</i>
                        more time reading the abstract and the intro than the main body, especially when you account for all the people who read the abstract and then stop. So you should spend a lot more time per unit word on those.
                    </p>
                    <p id="block505">
                        <strong>LLMs</strong>
                        : I think LLMs are a really helpful writing tool. They &#x27;re super useful for getting feedback, especially if writing in an unfamiliar style like an academic ML paper may be for you. Remember to use anti-sycophanty prompts so you get real feedback. However, it &#x27;s often quite easy to tell when you &#x27;re reading LLM written slop. So use them as a tool, but don &#x27;t just have them write the damn thing for you. But if you e.g. have writer’s block, having an LLM help you brainstorm or produce a first draft for inspiration, and can be very helpful.
                    </p>
                    <p data-internal-id="Common_mistakes" id="block506">
                        <span id="Common_mistakes">Common mistakes</span>
                    </p>
                    <ul>
                        <li id="block507">
                            <strong>The reader does not have context</strong>
                            : Your paper will be clear in your head, because you have just spent weeks to months steeped in this research project. The reader has not. You will overestimate how clear things are to the reader, and so you should be massively erring in the other direction and spelling everything out as blatantly as possible.
                            <ul>
                                <li id="block508">
                                    <strong>This is an incredibly common mistake</strong>
                                    - assume it will happen to you
                                </li>
                                <li id="block509">
                                    The main solution is to get feedback from people with enough research context that they can actually engage and who are also willing to give you substantial negative feedback.
                                    <ul>
                                        <li id="block510">Notice the feeling of surprise when people are confused by something you thought was clear. Try to understand why they were confused and iterate on fixing it until it &#x27;s clear.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block511">
                            <strong>Writing is not an afterthought</strong>
                            : People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute.
                        </li>
                        <li id="block512">
                            <strong>Acknowledge limitations</strong>
                            : There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing
                        </li>
                        <li id="block513">
                            <strong>Good writing is simple</strong>
                            : There &#x27;s a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they feel impressive. I think this is a highly ineffective strategy
                        </li>
                        <li id="block514">
                            <strong>Remember to motivate things</strong>
                            : It will typically not be obvious to the reader why your paper matters or is interesting. They do not have the context you do. It is your job to convince them, ideally in the abstract or perhaps intro, why they should care about your work, lest they just give up and stop reading.
                        </li>
                    </ul>
                    <p data-internal-id="h.sk0e3iwce7ck" id="block515">
                        <span id="h.sk0e3iwce7ck"></span>
                    </p>
                    <h2 id="Mentorship__Collaboration_and_Sharing_Your_Work" data-internal-id="Mentorship__Collaboration_and_Sharing_Your_Work">
                        <span id="Mentorship__Collaboration_and_Sharing_Your_Work">Mentorship, Collaboration and Sharing Your Work</span>
                    </h2>
                    <p id="block516">A common theme in the above is that it &#x27;s incredibly useful to have a mentor, or at least collaborators. Here I &#x27;ll try to unpack that and give advice about how to go about finding one.</p>
                    <p id="block517">Though it &#x27;s also worth saying that many mentors are not actually great researchers and may have bad research taste or research taste that &#x27;s not very well suited to mech interp. What you do about this is kind of up to you.</p>
                    <h3 id="So_what_does_a_research_mentor_actually_do_" data-internal-id="So_what_does_a_research_mentor_actually_do_">
                        <span id="So_what_does_a_research_mentor_actually_do_">So what does a research mentor actually do?</span>
                    </h3>
                    <p id="block518">A good mentor is an incredible accelerator. Dysfunctional as academia is, there is a reason it works under the apprenticeship-like system of PhD students and supervisors. When I started supervising, I was very surprised at how much of a difference a weekly check in could make! Here’s my best attempt to breakdown how a good mentor can add value:</p>
                    <ul>
                        <li id="block519">
                            <strong>Suggest research ideas</strong>
                            when you &#x27;re starting out, letting you bypass the hardest skill (ideation) to focus on execution.
                        </li>
                        <li id="block520">
                            <strong>Help you prioritize</strong>
                            which experiments to run, lending you their more experienced judgment, so you get more done.
                        </li>
                        <li id="block521">
                            <p data-internal-id="ftnt_ref35" id="block522">
                                <span id="ftnt_ref35">
                                    <strong>When to pivot</strong>
                                    : if your research direction isn’t working out, having a mentor to pressure you to pivot can be extremely valuable
                                    <span data-footnote-reference="" data-footnote-index="35" data-footnote-id="7ruxx269r2s" role="doc-noteref" id="fnref7ruxx269r2s" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fn7ruxx269r2s" class="">[35]</a>
                                            </span>
                                        </sup>
                                    </span>
                                </span>
                            </p>
                        </li>
                        <li id="block523">
                            <strong>Provide supervised data for research taste</strong>
                            : For the slow/very-slow skills like coming up with research ideas, and prioritization, a <i>far </i>
                            faster way to gain them at first is by learning to mimic your mentor’s.
                        </li>
                        <li id="block524">
                            <strong>Act as an interface to the literature</strong>
                            : pointing you to the relevant work before you &#x27;ve built up deep knowledge yourself. Flagging standard baselines, standard metrics, relevant techniques, prior work so you don’t reinvent the wheel, etc.
                        </li>
                        <li id="block525">
                            <strong>Red-team your results</strong>
                            , helping you spot subtle interpretability illusions and flaws in your reasoning that you &#x27;re too close to see.
                        </li>
                        <li id="block526">
                            <strong>Point out skills you &#x27;re missing</strong>
                            that you didn &#x27;t even notice were skills. Generally guide your learning and help you prioritize
                        </li>
                        <li id="block527">
                            <strong>Walk you through communicating your work</strong>
                            , helping you distill your findings and present them clearly to the world.
                        </li>
                        <li id="block528">
                            <strong>Motivation/accountability</strong>
                            : Many find it extremely helpful to have someone, even if very hands-off, who they present work to, so they feel motivated and accountable (especially if they e.g. want to impress the mentor, want a job, etc. Of course, these also increase stress!)
                            <ul>
                                <li id="block529">To those prone to analysis paralysis, being able to defer to a mentor on uncertain decisions can be highly valuable</li>
                            </ul>
                        </li>
                        <li id="block530">
                            <strong>References</strong>
                            : Having a mentor who can vouch for your skill is very helpful, especially if they know people who may be hiring you in future.
                        </li>
                    </ul>
                    <h3 id="Advice_on_finding_a_mentor" data-internal-id="Advice_on_finding_a_mentor">
                        <span id="Advice_on_finding_a_mentor">Advice on finding a mentor</span>
                    </h3>
                    <p id="block531">Here are some suggested ways to get some mentorship while transitioning into the field. I discuss higher commitment ways, like doing a PhD or getting a research job, below.</p>
                    <p id="block532">Note: whatever you do to find a mentor, having evidence that you can do research yourself, that is, public output that demonstrates ability to self-motivate and put in effort, and ideally demonstrates actually interesting research findings, is incredibly helpful and should be a priority.</p>
                    <p data-internal-id="Mentoring_programs" id="block533">
                        <span id="Mentoring_programs">Mentoring programs</span>
                    </p>
                    <p id="block534">
                        I think mentoring programs like 
                        <span>
                            <span>
                                <a href="http://matsprogram.org">MATS</a>
                            </span>
                        </span>
                        are an incredibly useful way into the field, you typically do a full-time, several month program where you write a paper, with weekly check-ins with a more experienced researcher. Your experience will vary wildly depending on mentor quality, but at least for my MATS scholars, often people totally new to mech interp can publish a top conference paper in a few months. See my MATS application doc for a bunch more details.
                    </p>
                    <p id="block535">
                        There’s <strong>a wide range of backgrounds</strong>
                        among people who do them and get value - people totally new to a field, people with 1+ years of interpretability research experience who want to work with a more experienced mentor, young undergrads, mid-career professionals (including a handful of professors), and more.
                    </p>
                    <p id="block536">
                        <span>
                            <span>
                                <a href="http://matsprogram.org">MATS 9.0</a>
                            </span>
                        </span>
                        applications are open, due <strong>Oct 2 2025</strong>
                        , and 
                        <span>
                            <span>
                                <a href="http://tinyurl.com/neel-mats-app">mine</a>
                            </span>
                        </span>
                        close on <strong>Sept 12</strong>
                        .
                    </p>
                    <p id="block537">Other programs (which I think are generally lower quality than MATS, but often still worth applying to depending on the mentor)</p>
                    <ul>
                        <li id="block538">
                            <i>Full-time/In-person:</i>
                            <span>
                                <span>
                                    <a href="https://www.matsprogram.org/">MATS</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://www.pivotal-research.org/fellowship">Pivotal</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://www.lasrlabs.org/">LASR</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://pibbss.ai/fellowship/">PIBBSS</a>
                                </span>
                            </span>
                        </li>
                        <li id="block539">
                            <i>Part-time/Remote:</i>
                            <span>
                                <span>
                                    <a href="https://www.cambridgeaisafety.org/mars"></a>
                                </span>
                            </span>
                            <span>
                                <span>
                                    <a href="https://sparai.org/">SPAR</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://www.cambridgeaisafety.org/mars">MARS</a>
                                </span>
                            </span>
                        </li>
                    </ul>
                    <p data-internal-id="Cold_emails" id="block540">
                        <span id="Cold_emails">Cold emails</span>
                    </p>
                    <p id="block541">You can also take matters into your own hands and try to convince someone to be your mentor. Reaching out to people, ideally via a warm introduction, but even just via a cold email, can be highly effective. However, I get lots of cold emails and I think many are not very effective, so here &#x27;s some advice:</p>
                    <ul>
                        <li id="block542">
                            <strong>Don &#x27;t just email the most prominent people</strong>
                            . A lot of people will just email the most prominent people in the field and ask for mentorship. This is a bad plan! These people are very busy and they also get lots of emails. I just reflexively respond to any email requesting mentorship with “please apply to my MATS cohort”.
                            <ul>
                                <li id="block543">However, there are lots of less prominent people who can provide a bunch of useful mentorship. These people are much more likely to be excited to get a cold email, to have time to engage, potentially even the spare capacity to properly mentor a project.</li>
                                <li id="block544">
                                    I think that many people who &#x27;ve recently joined my team or people who worked on a great paper with me during MATS are able to add a lot of value to people new to the field. And I would recommend reaching out to them!
                                    <ul>
                                        <li id="block545">For example, Josh Engels, a new starter on my team, said he would happily receive more cold emails (as of early Sept 2025).</li>
                                        <li id="block546">As a general heuristic, email first authors of papers, not fancy last authors.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block547">
                            <strong>Start small</strong>
                            : Don &#x27;t email someone you &#x27;ve never interacted with before asking if they want to kind of officially mentor you on some project. That &#x27;s a big commitment.
                            <ul>
                                <li id="block548">It &#x27;s much better to be like, I &#x27;d be interested in having a chat about your paper or my work building on your paper.</li>
                                <li id="block549">Or just asking if they &#x27;re down to have a chat giving you feedback on some project ideas, etc.</li>
                                <li id="block550">And if this goes well, it may organically turn into a more long-term mentoring relationship!</li>
                            </ul>
                        </li>
                        <li id="block551">
                            <strong>Proof of work</strong>
                            : Demonstrate that you are actually interested in this person specifically, not just spamming tons of people.
                            <ul>
                                <li id="block552">
                                    Show that you &#x27;ve engaged with their work, say something intelligent about it, have some questions.
                                    <ul>
                                        <li id="block553">In the era of LLMs, this is less of a costly signal that you &#x27;ve actually taken an interest in this person specifically than it used to be, admittedly</li>
                                        <li id="block554">But linking to some research you did building on their work I think is still reasonably costly, and very flattering to people.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li id="block555">
                            <strong>Prioritize aggressively</strong>
                            . Assume the reader will stop reading at any moment, so put your most critical and impressive information first.
                        </li>
                        <li id="block556">
                            <strong>Explain who you are</strong>
                            : If you &#x27;re emailing someone who gets more emails than they have capacity to respond to, they &#x27;re going to be prioritizing. A key input into this is just who you are, what have you done, have you done something interesting that shows promise, do you have relevant credentials, etc. I personally find it very helpful if people just say the most impressive things about them in the first sentence or two.
                            <ul>
                                <li id="block557">To do this without seeming arrogant, you could try: &quot;I &#x27;m sure you must get many of these emails. So to help you prioritise, here &#x27;s some key info about me &quot;</li>
                            </ul>
                        </li>
                        <li id="block558">
                            Use <strong>bolding</strong>
                            for key phrases to make your email easily skimmable.
                        </li>
                        <li id="block559">
                            <strong>Be concise</strong>
                            . One thing I would often appreciate is a short blurb summarizing your request with a link to a longer document for details if I &#x27;m interested.
                        </li>
                        <li id="block560">
                            <strong>Quick requests</strong>
                            : Generally, my flow when reading emails is that I will either immediately respond or never look at it again. I &#x27;m a lot more likely to immediately respond if I can do so quickly. If you do want to email a busy person, have a clear, concrete question up front that they might be able to help with.
                        </li>
                    </ul>
                    <h3 id="Community___collaborators" data-internal-id="Community___collaborators">
                        <span id="Community___collaborators">Community &amp;collaborators</span>
                    </h3>
                    <p id="block561">Much easier than finding a mentor is finding collaborators, other people to work on the same project with, or just other people also trying to learn more about mech interp, who you can chat with and give each other feedback:</p>
                    <ul>
                        <li id="block562">
                            <strong>In-Person:</strong>
                            Local AI Safety hubs (London, Bay Area, etc.), University groups, ML conferences (e.g., the
                            <span>
                                <span>
                                    <a href="http://mechinterpworkshop.com/">NeurIPS Mech Interp workshop</a>
                                </span>
                            </span>
                            I co-organize), EAG/EAGx conferences.
                            <ul>
                                <li id="block563">If you’re a student, see if there’s a lab at your university that has some people interested in interpretability. There may be interested PhD students even if no professor works on it</li>
                            </ul>
                        </li>
                        <li id="block564">
                            <strong>Online</strong>
                            : These are also good places to meet people! I recommend sharing work for feedback, or just asking about who’s interested in what you’re interested in, and trying to DM the people who engage/seem interested, and seeing what happens
                            <ul>
                                <li id="block565">
                                    <span>
                                        <span>
                                            <a href="https://www.neelnanda.io/osmi-slack-invite">Open Source Mechanistic Interpretability Slack</a>
                                        </span>
                                    </span>
                                </li>
                                <li id="block566">
                                    <span>
                                        <span>
                                            <a href="https://discord.gg/nHS4YxmfeM">Eleuther Discord</a>
                                        </span>
                                    </span>
                                    (interpretability-general)
                                </li>
                                <li id="block567">
                                    <span>
                                        <span>
                                            <a href="https://discord.gg/ysVfhCfCKw">Mech Interp Discord</a>
                                        </span>
                                    </span>
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p id="block568">
                        <strong>Staying up to date</strong>
                        : Another common question is how to stay up to date with the field. Honestly, I think that people new to the field should not worry that much about this. Most new papers are irrelevant, including the ones that there is hype around. But it &#x27;s good to stay a little bit in the loop. Note that the community has substantial parts both in academia and outside, which are often best kept up with in different ways.
                    </p>
                    <ul>
                        <li id="block569">LessWrong and the AlignmentForum are a reasonable place to keep up to date with the less academic half</li>
                        <li id="block570">
                            Twitter is a confusing, chaotic place that is an okay place to keep up with both. It &#x27;s a bit unclear who the right people to follow.
                            <ul>
                                <li id="block571">
                                    <span>
                                        <span>
                                            <a href="http://x.com/ch402">Chris Olah</a>
                                        </span>
                                    </span>
                                    doesn &#x27;t tweet much, but it &#x27;s high quality when he does.
                                </li>
                                <li id="block572">
                                    <span>
                                        <span>
                                            <a href="http://x.com/neelnanda5">I will tweet</a>
                                        </span>
                                    </span>
                                    about all of my interpretability work and sometimes others.
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <h2 id="Careers" data-internal-id="Careers">
                        <span id="Careers">Careers</span>
                    </h2>
                    <h3 id="Where_to_apply" data-internal-id="Where_to_apply">
                        <span id="Where_to_apply">Where to apply</span>
                    </h3>
                    <ul>
                        <li id="block573">
                            Anthropic’s interpretability team roles: 
                            <span>
                                <span>
                                    <a href="https://job-boards.greenhouse.io/anthropic/jobs/4020159008">research scientist</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://job-boards.greenhouse.io/anthropic/jobs/4020305008">research engineer</a>
                                </span>
                            </span>
                            , 
                            <span>
                                <span>
                                    <a href="https://job-boards.greenhouse.io/anthropic/jobs/4009173008">research manager</a>
                                </span>
                            </span>
                        </li>
                        <li id="block574">
                            <span>
                                <span>
                                    <a href="https://openai.com/careers/research-engineer-scientist-interpretability">OpenAI &#x27;s interpretability team roles</a>
                                </span>
                            </span>
                        </li>
                        <li id="block575">
                            My team at Google DeepMind will hopefully be 
                            <span>
                                <span>
                                    <a href="https://deepmind.google/about/careers/#open-roles">hiring in early 2026</a>
                                </span>
                            </span>
                            ! Watch this space
                        </li>
                        <li id="block576">
                            <span>
                                <span>
                                    <a href="https://transluce.org/">Transluce</a>
                                </span>
                            </span>
                            -- a nonprofit research lab
                        </li>
                        <li id="block577">
                            <span>
                                <span>
                                    <a href="https://www.goodfire.ai/">Goodfire</a>
                                </span>
                            </span>
                            -- a mech interp startup that are 
                            <span>
                                <span>
                                    <a href="https://www.goodfire.ai/careers">hiring a bunch</a>
                                </span>
                            </span>
                            .
                            <ul>
                                <li id="block578">
                                    They 
                                    <span>
                                        <span>
                                            <a href="https://www.goodfire.ai/blog/announcing-our-50m-series-a">recently raised a $50 million Series A</a>
                                        </span>
                                    </span>
                                    and as of the time of writing are trying to both have people focused on products, and people focused on more fundamental research
                                </li>
                            </ul>
                        </li>
                        <li id="block579">
                            The UK government &#x27;s AI Security Institute &#x27;s interpretability team (
                            <span>
                                <span>
                                    <a href="https://www.aisi.gov.uk/careers#open-roles">not currently hiring</a>
                                </span>
                            </span>
                            )
                        </li>
                    </ul>
                    <p data-internal-id="Applying_for_grants" id="block580">
                        <span id="Applying_for_grants">Applying for grants</span>
                    </p>
                    <p id="block581">
                        For people trying to get into mech interp via the safety community, there are some funders around open to giving career transition grants to people trying to upskill in a new field like mech interp. Probably the best place I know of is 
                        <span>
                            <span>
                                <a href="https://www.openphilanthropy.org/career-development-and-transition-funding/">Open Philanthropy &#x27;s Early Career Funding.</a>
                            </span>
                        </span>
                    </p>
                    <p data-internal-id="Explore_Other_AI_Safety_Areas" id="block582">
                        <span id="Explore_Other_AI_Safety_Areas">Explore Other AI Safety Areas</span>
                    </p>
                    <p id="block583">
                        Mech interp isn &#x27;t the only game in town! There’s other important areas of safety like Evals, AI Control, and Scalable Oversight, the latter two in particular seem neglected compared to mech interp. The
                        <span>
                            <span>
                                <a href="https://arxiv.org/pdf/2504.01849">GDM AGI Safety Approach</a>
                            </span>
                        </span>
                        gives an overview of different parts of the field. If you’re doing this for safety reasons, I’d check if there’s other, more neglected subfields, that also appeal to you!
                    </p>
                    <h3 id="What_do_hiring_managers_look_for" data-internal-id="What_do_hiring_managers_look_for">
                        <span id="What_do_hiring_managers_look_for">What do hiring managers look for</span>
                    </h3>
                    <p id="block584">Leaving aside things that apply to basically all roles, like whether this person has a good personality fit (which often just means looking out for red flags), here’s my sense of what hiring managers in interpretability are often looking for.</p>
                    <p id="block585">A useful mental model is that from a hiring manager &#x27;s perspective, they &#x27;re making an uncertain bet with little information in a somewhat adversarial environment. Each applicant wants to present themselves as the perfect fit. This means managers need to rely on signals that are hard to fake. But it’s quite difficult to get that much info on a person before you actually go and work with them a bunch.</p>
                    <p id="block586">Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly. If your research track record is good enough, interviews may just act as a check for red flags and to verify that you can actually write code and run experiments well.</p>
                    <p id="block587">Key skills:</p>
                    <ul>
                        <li id="block588">
                            <strong>Research Skills:</strong>
                            A track record of completing end-to-end projects is the best signal. Papers are a great way to show this.
                            <ul>
                                <li id="block589">
                                    <strong>Research taste</strong>
                                    : The ability to come up with great research ideas <i>and</i>
                                    drive them to completion is rare and very valuable.
                                </li>
                                <li id="block590">
                                    <strong>Experiment design</strong>
                                    : Can they design good experiments and make their research ideas concrete and convert them into actions?
                                </li>
                            </ul>
                        </li>
                        <li id="block591">
                            <strong>Conceptual Understanding of Mech Interp:</strong>
                            Do you get the key ideas and know the literature?
                        </li>
                        <li id="block592">
                            <p data-internal-id="ftnt_ref36" id="block593">
                                <span id="ftnt_ref36">
                                    <strong>Productivity and Conscientiousness:</strong>
                                    This is a very hard one to interview for, but incredibly important. A public track record of doing interesting things is a good signal, as are strong references from trusted sources
                                    <span data-footnote-reference="" data-footnote-index="36" data-footnote-id="slnwemz4grq" role="doc-noteref" id="fnrefslnwemz4grq" class="footnote-reference">
                                        <sup>
                                            <span>
                                                <a href="#fnslnwemz4grq" class="">[36]</a>
                                            </span>
                                        </sup>
                                    </span>
                                    .
                                </span>
                            </p>
                        </li>
                        <li id="block594">
                            <strong>Engineering Skills:</strong>
                            Can you work fluently in a Python notebook? Can you write experiment code fast and well? Can you get things done? Do you understand the standard gotchas?
                        </li>
                        <li id="block595">
                            <strong>Deep engineering skill</strong>
                            : Beyond hacking together experiments, can you navigate large, complex codebases, write maintainable code, design complex software projects, etc?
                            <ul>
                                <li id="block596">This is much more important if doing research inside a larger lab or tech company than as an independent researcher or academic.</li>
                                <li id="block597">One of the most common reasons we don &#x27;t hire seemingly promising researchers onto my team is because they lack sufficiently strong engineering skills.</li>
                                <li id="block598">Obviously, LLMs are substantially changing the game when it comes to engineering skills, but I think deep engineering skills will be much harder to automate than shallow ones, unfortunately.</li>
                                <li id="block599">Unfortunately, I don’t have great advice on how to gain these other than working in larger and more complex codebases and learning how to cope. Pair programming with more experienced programmers can be a great way to transfer tacit knowledge</li>
                            </ul>
                        </li>
                        <li id="block600">
                            <strong>Skepticism</strong>
                            : Can you constructively engage with research and critically evaluate it? In particular, can you do this to your own research? Good researchers need to be able to do work that is true.
                        </li>
                    </ul>
                    <h3 id="Should_you_do_a_PhD_" data-internal-id="Should_you_do_a_PhD_">
                        <span id="Should_you_do_a_PhD_">Should you do a PhD?</span>
                    </h3>
                    <p id="block601">I don &#x27;t have a PhD (and think I would have had a far less successful career if I had tried to get one) so I &#x27;m somewhat biased. But it &#x27;s a common question. Here are the strongest arguments I’ve heard in favour:</p>
                    <ul>
                        <li id="block602">
                            You get extremely high <strong>autonomy</strong>
                            . If you want to spend years going deep on a niche topic that no industry lab would fund, a PhD is one of the only ways to do it.
                        </li>
                        <li id="block603">
                            It &#x27;s a great environment to cultivate the ability to <strong>set your own research agenda</strong>
                            . This is a crucial and difficult skill that is harder to learn in industry, where agendas are often set from the top down (though this varies a lot between team).
                        </li>
                    </ul>
                    <p id="block604">And here are the reasons I think it &#x27;s often a bad idea:</p>
                    <ul>
                        <li id="block605">The opportunity cost is immense. You could spend 4-6 years gaining direct, relevant experience in an industry lab.</li>
                        <li id="block606">Academic incentives can be misaligned with doing impactful research, e.g. pressure to publish meaning you’re discouraged from admitting to the limitations of your work.</li>
                        <li id="block607">The quality of supervision varies wildly, and a bad supervisor can make your life miserable.</li>
                        <li id="block608">Quality of life: The pay is generally terrible, which may or may not matter to you, and you may only get places in a different city/country than you’d prefer.</li>
                    </ul>
                    <p id="block609">But with all those caveats in mind, it’s definitely the right option for some! My overall take:</p>
                    <ul>
                        <li id="block610">
                            The key thing that matters is mentorship, being in an environment where you are working with a better researcher, and learning from them.
                            <ul>
                                <li id="block611">PhDs are often a good way of getting this. But if you can gain this by another way, plausibly you should go to that instead. PhDs have a lot of downsides too.</li>
                            </ul>
                        </li>
                        <li id="block612">
                            Generally, the variance between supervisors and between managers in industry will dominate the academia versus industry differences, and thus you should pay a lot of attention to who exactly would be managing you.
                            <ul>
                                <li id="block613">For a PhD, try to speak to your potential supervisor’s students in a private setting. If they say pretty bad things, that &#x27;s a good reason not to go for the supervisor.</li>
                                <li id="block614">A common mistake is optimising for the most prestigious and famous supervisor when you often want to go for the ones who will have the most time for you, which anti-correlates.</li>
                            </ul>
                        </li>
                        <li id="block615">
                            A common mistake is people feeling they need to <i>finish</i>
                            PhDs. But if you sincerely believe that the point of a PhD is to be a learning environment, then why would the formal end of the PhD be the optimal time to leave? It &#x27;s all kind of arbitrary.
                            <ul>
                                <li id="block616">
                                    IMO, at least every six months, you should seriously evaluate what other opportunities you have, try applying for some things and be emotionally willing leave if a better opportunity comes along (taking into account switching costs).
                                    <ul>
                                        <li id="block617">Note that often you can just take a year &#x27;s leave of absence and resume at will.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p data-internal-id="Relevant_Academic_Labs" id="block618">
                        <span id="Relevant_Academic_Labs">Relevant Academic Labs</span>
                    </p>
                    <p id="block619">I’m a big fan of the work coming out of these two, they seem like great places to work:</p>
                    <ul>
                        <li id="block620">David Bau (Northeastern)</li>
                        <li id="block621">Martin Wattenberg &amp;Fernanda Viegas (Harvard)</li>
                    </ul>
                    <p id="block622">Other labs that seem like good places to do interpretability research (note that this is not trying to be a comprehensive list!):</p>
                    <ul>
                        <li id="block623">Yonatan Belinkov (Technion)</li>
                        <li id="block624">Jacob Andreas (MIT)</li>
                        <li id="block625">Jacob Steinhardt (Berkeley)</li>
                        <li id="block626">Ellie Pavlick (Brown)</li>
                        <li id="block627">Victor Veitch (UChicago)</li>
                        <li id="block628">Robert West (EPFL)</li>
                        <li id="block629">Roger Grosse (Toronto)</li>
                        <li id="block630">Mor Geva (Tel Aviv)</li>
                        <li id="block631">Sarah Wiegreffe (Maryland)</li>
                        <li id="block632">Aaron Mueller (Boston University)</li>
                    </ul>
                    <p id="block633">
                        <i>Thanks a lot to Arthur Conmy, Paul Bogdan, Bilal Chughtai, Julian Minder, Callum McDougall, Josh Engels, Clement Dumas, Bart Bussmann for valuable feedback</i>
                    </p>
                    <ol data-footnote-section="" role="doc-endnotes" class="footnote-section footnotes">
                        <li data-footnote-item="" data-footnote-index="1" data-footnote-id="nifk1wb1jum" role="doc-endnote" id="fnnifk1wb1jum" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="nifk1wb1jum" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefnifk1wb1jum">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block635">Note that I mean a full working month here. So something like 200 working hours. If you &#x27;re only able to do this part-time, it &#x27;s fine to take longer. If you &#x27;re really focused on it, or have a head-start, then move on faster.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="2" data-footnote-id="ue9pdw6v8rj" role="doc-endnote" id="fnue9pdw6v8rj" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="ue9pdw6v8rj" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefue9pdw6v8rj">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block637">If you want something even more approachable, one of my past MATS scholars recommends getting GPT-5 thinking to produce coding exercises (eg a Python script with empty functions, and good tests), for an easier way in.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="3" data-footnote-id="hh6mwdeo4zm" role="doc-endnote" id="fnhh6mwdeo4zm" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="hh6mwdeo4zm" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefhh6mwdeo4zm">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block639">It’s fine for this coding to need a bunch of LLM help and documentation/tutorial looking up, this isn’t a memory test. The key thing is being able to correctly explain the core of each technique to a friend/LLM.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="4" data-footnote-id="sxyjce3nii" role="doc-endnote" id="fnsxyjce3nii" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="sxyjce3nii" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefsxyjce3nii">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block641">
                                    Note: This curriculum aims to get you started on <i>independent research</i>
                                    . This is often good enough for academic labs, but the engineering bar for most industry labs is significantly higher, as you’ll need to work in a large complex codebase with hundreds of other researchers. But those skills take much longer to gain.
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="5" data-footnote-id="kte6u8splw" role="doc-endnote" id="fnkte6u8splw" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="kte6u8splw" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefkte6u8splw">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block643">You want to exclude the first token of the prompt when collecting activations, it’s a weird attention sink and often has high norm/is anomalous in many ways</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="6" data-footnote-id="2ob115pcmet" role="doc-endnote" id="fn2ob115pcmet" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="2ob115pcmet" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref2ob115pcmet">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block645">Gotcha: Remember to try a bunch of coefficients for the vector when adding it. This is a crucial hyper-parameter and steered model behaviour varies a lot depending on its value</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="7" data-footnote-id="1b9r0ass7sd" role="doc-endnote" id="fn1b9r0ass7sd" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="1b9r0ass7sd" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref1b9r0ass7sd">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block647">Mixture of expert models, where there are many parameters, and only a fraction light up for each token, are a pain for interpretability research. Larger models means you &#x27;ll need to get more/larger GPUs which is expensive and unwieldy. Favor working with dense models where possible.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="8" data-footnote-id="bzop9pji3nl" role="doc-endnote" id="fnbzop9pji3nl" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="bzop9pji3nl" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefbzop9pji3nl">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block649">You can download then upload the PDF to the model, or just select all and copy and paste from the PDF to the chat window. No need to correct the formatting issues, LLMs are great at ignoring weird formatting artifacts</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="9" data-footnote-id="207k0k5nobb" role="doc-endnote" id="fn207k0k5nobb" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="207k0k5nobb" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref207k0k5nobb">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block651">
                                    <span>
                                        <span>
                                            <a href="http://repo2txt.com">repo2txt.com</a>
                                        </span>
                                    </span>
                                    is a useful tool for concatenating a Github repo into a single txt file
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="10" data-footnote-id="979wnkvgpa4" role="doc-endnote" id="fn979wnkvgpa4" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="979wnkvgpa4" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref979wnkvgpa4">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block653">
                                    If you would like other perspectives, check out 
                                    <span>
                                        <span>
                                            <a href="https://arxiv.org/abs/2501.16496">Open Problems in Mechanistic Interpretability</a>
                                        </span>
                                    </span>
                                    (broad lit review from many leading researchers, recent), or 
                                    <span>
                                        <span>
                                            <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">Interpretability Dreams</a>
                                        </span>
                                    </span>
                                    (from Anthropic, 2 years old)
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="11" data-footnote-id="3zw26zes9dx" role="doc-endnote" id="fn3zw26zes9dx" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="3zw26zes9dx" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref3zw26zes9dx">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block655">And for reasons we’ll discuss later, now feel much more pessimistic about the ambitious reverse engineering direction</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="12" data-footnote-id="7cxhc64szn8" role="doc-endnote" id="fn7cxhc64szn8" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="7cxhc64szn8" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref7cxhc64szn8">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block657">Even if you already have a research background in another field, mechanistic interpretability is sufficiently different that you should expect to need to relearn at least some of your instincts. This stage remains very relevant to you, though you can hopefully learn faster.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="13" data-footnote-id="9wj0u0qz3q" role="doc-endnote" id="fn9wj0u0qz3q" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="9wj0u0qz3q" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref9wj0u0qz3q">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block659">The rest of this piece will be framed around approaching learning research like this and why I think it is a reasonable process. Obviously, there is not one true correct way to learn research! When I e.g. critique something as a “mistake”, interpret this as “I often see people do this and think it’s suboptimal for them”, not “there does not exist a way of learning research where this is a good idea</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="14" data-footnote-id="xw1ra5pqnd" role="doc-endnote" id="fnxw1ra5pqnd" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="xw1ra5pqnd" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefxw1ra5pqnd">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block661">My term for associated knowledge, understanding, intuition, etc.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="15" data-footnote-id="tq4gws0zq69" role="doc-endnote" id="fntq4gws0zq69" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="tq4gws0zq69" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnreftq4gws0zq69">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block663">
                                    Read <a class="LinkStyles-link" href="/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft">my thoughts on SAEs here</a>
                                    . There’s still useful work to be done, but it’s an oversubscribed area, and our bar should be higher. They are a useful tool, but not as promising as I once hoped.
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="16" data-footnote-id="cdmsagzbqkp" role="doc-endnote" id="fncdmsagzbqkp" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="cdmsagzbqkp" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefcdmsagzbqkp">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block665">This was using a technique called synthetic document fine-tuning (and some other creativity on top), which basically lets you insert false beliefs into a model by generating a bunch of fictional documents where those beliefs are true and fine-tuning the model on them.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="17" data-footnote-id="p0f0m03b55r" role="doc-endnote" id="fnp0f0m03b55r" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="p0f0m03b55r" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefp0f0m03b55r">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block667">We chose problems we’re excited to see worked on, while trying to avoid fad-like dynamics</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="18" data-footnote-id="g12d8d1lqu" role="doc-endnote" id="fng12d8d1lqu" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="g12d8d1lqu" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefg12d8d1lqu">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block669">Latents refer to the hidden units of the SAE. These were originally termed “features”, but that term is also used to mean “the interpretable concept the latent refers to”, so I use a different term to minimise confusion.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="19" data-footnote-id="0td6a2gxwht" role="doc-endnote" id="fn0td6a2gxwht" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="0td6a2gxwht" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref0td6a2gxwht">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block671">One of my MATS scholars make a working GPT-5 model diffing agent in a day</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="20" data-footnote-id="5bdglmkdzr" role="doc-endnote" id="fn5bdglmkdzr" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="5bdglmkdzr" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref5bdglmkdzr">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block673">
                                    This is the one line in the post <i>without </i>
                                    a “as of early Sept 2025” disclaimer, this feels pretty evergreen
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="21" data-footnote-id="wuxdh4f7kh" role="doc-endnote" id="fnwuxdh4f7kh" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="wuxdh4f7kh" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefwuxdh4f7kh">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block675">Note: &quot;think &quot;or &quot;chain of thought &quot;are terrible terms. It &#x27;s far more useful to think of the chain of thought as a scratchpad that a model with very limited short-term memory can choose to use or ignore.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="22" data-footnote-id="3qxoen8tddk" role="doc-endnote" id="fn3qxoen8tddk" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="3qxoen8tddk" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref3qxoen8tddk">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block677">Reasoning models break a lot of standard interpretability techniques because now the computational graph goes through the discrete, non-differentiable, and random operation of sampling thousands of times. Most interpretability techniques focus on studying a single forward pass.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="23" data-footnote-id="lm5ixkfuzk" role="doc-endnote" id="fnlm5ixkfuzk" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="lm5ixkfuzk" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnreflm5ixkfuzk">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block679">Not just, e.g., ones you can publish on.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="24" data-footnote-id="idab8074tka" role="doc-endnote" id="fnidab8074tka" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="idab8074tka" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefidab8074tka">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block681">I called this moving fast in the blog post, but I think that may have confused some people.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="25" data-footnote-id="wpekmwudkpd" role="doc-endnote" id="fnwpekmwudkpd" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="wpekmwudkpd" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefwpekmwudkpd">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block683">Though often this is done well with just a good introduction</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="26" data-footnote-id="1bau7vsh9tk" role="doc-endnote" id="fn1bau7vsh9tk" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="1bau7vsh9tk" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref1bau7vsh9tk">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block685">And having a well-known researcher as co-author is not sufficient evidence to avoid this, alas. I’m sure at least one paper I’ve co-authored in the past year or two is substantially false</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="27" data-footnote-id="adytzr5d7y" role="doc-endnote" id="fnadytzr5d7y" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="adytzr5d7y" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefadytzr5d7y">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block687">It &#x27;s strongly in your interests for people to build on your work because that makes your original work look better, in addition to being just pretty cool to see people engage deeply with your stuff.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="28" data-footnote-id="va7mhfkrhm" role="doc-endnote" id="fnva7mhfkrhm" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="va7mhfkrhm" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefva7mhfkrhm">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block689">
                                    Note that deliberately reproducing work, or trying to demonstrate the past work is shoddy, is completely reasonable. You just need to not <i>accidentally</i>
                                    reinvent the wheel.
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="29" data-footnote-id="tt0owz8koks" role="doc-endnote" id="fntt0owz8koks" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="tt0owz8koks" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnreftt0owz8koks">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block691">This is generally a good thing to do regardless of whether you’re focused on research taste or not!</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="30" data-footnote-id="e3252d8idmr" role="doc-endnote" id="fne3252d8idmr" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="e3252d8idmr" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefe3252d8idmr">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block693">And, nowadays, LLM knowledge too I guess?</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="31" data-footnote-id="8354hd0flji" role="doc-endnote" id="fn8354hd0flji" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="8354hd0flji" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref8354hd0flji">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block695">Note that you’ll need someone who’s written several Arxiv papers to endorse you. cs.LG is the typical category for ML papers.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="32" data-footnote-id="9oppcf0ftrh" role="doc-endnote" id="fn9oppcf0ftrh" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="9oppcf0ftrh" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref9oppcf0ftrh">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block697">
                                    Note that you can submit something to a workshop <i>and </i>
                                    to a conference, so long as the workshop is “non-archival”
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="33" data-footnote-id="fmh579omuc6" role="doc-endnote" id="fnfmh579omuc6" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="fmh579omuc6" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnreffmh579omuc6">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block699">A conference paper is a fair bit more effort, and you generally want to be working with someone who understands the academic conventions and shibboleths and the various hoops you should be jumping through. But I think this can be a nice thing to aim for, especially if you &#x27;re starting out and need credentials, though mech interp cares less about peer review than most academic subfields.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="34" data-footnote-id="f09vsa4w37e" role="doc-endnote" id="fnf09vsa4w37e" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="f09vsa4w37e" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnreff09vsa4w37e">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block701">
                                    See 
                                    <span>
                                        <span>
                                            <a href="https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/">this NeurIPS experiment</a>
                                        </span>
                                    </span>
                                    showing that half the spotlight papers would be rejected by an independent reviewing council
                                </p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="35" data-footnote-id="7ruxx269r2s" role="doc-endnote" id="fn7ruxx269r2s" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="7ruxx269r2s" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnref7ruxx269r2s">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block703">This is one of the most valuable things I do for my MATS scholars, IMO.</p>
                            </div>
                        </li>
                        <li data-footnote-item="" data-footnote-index="36" data-footnote-id="slnwemz4grq" role="doc-endnote" id="fnslnwemz4grq" class="footnote-item">
                            <span data-footnote-back-link="" data-footnote-id="slnwemz4grq" class="footnote-back-link">
                                <sup>
                                    <strong>
                                        <a href="#fnrefslnwemz4grq">^</a>
                                    </strong>
                                </sup>
                            </span>
                            <div data-footnote-content="" class="footnote-content">
                                <p id="block705">Unfortunately, standard reference culture, especially in the US, is to basically lie, and the amount of lying varies between contexts, rendering references mostly useless unless from a cultural context the hiring manager understands or ideally from people they know and trust. This is one of the reasons that doing AI safety mentoring programs like MATS can be extremely valuable, because often your mentor will know people who might then go on to hire you, which makes you a lower risk hire from their perspective.</p>
                            </div>
                        </li>
                    </ol>
                </div>
            </div>
        </div>
        <script>
            $RS("S:3", "P:3")
        </script>
        <script>
            $RC("B:1", "S:1")
        </script>
    </body>
    <script>
        window.ssrMetadata = {
            "renderedAt": "2025-09-04T18:33:35.223Z",
            "cacheFriendly": false,
            "timezone": "America/Los_Angeles"
        }
    </script>
    <script>
        window.__APOLLO_STATE__ = {
            "ROOT_QUERY": {
                "__typename": "Query",
                "currentUser": null,
                "post({\"input\":{\"selector\":{\"documentId\":\"jP9KDyMkchuv6tHwm\"}}})": {
                    "__typename": "SinglePostOutput",
                    "result": {
                        "__ref": "Post:jP9KDyMkchuv6tHwm"
                    }
                },
                "comments({\"enableTotal\":true,\"limit\":1000,\"selector\":{\"afPostCommentsTop\":{\"postId\":\"jP9KDyMkchuv6tHwm\"}}})": {
                    "__typename": "MultiCommentOutput",
                    "results": [],
                    "totalCount": 0
                },
                "comments({\"enableTotal\":false,\"limit\":5,\"selector\":{\"reviews\":{\"minimumKarma\":5,\"postId\":\"jP9KDyMkchuv6tHwm\"}}})": {
                    "__typename": "MultiCommentOutput",
                    "results": [],
                    "totalCount": null
                },
                "comments({\"enableTotal\":false,\"limit\":10,\"selector\":{\"alignmentSuggestedComments\":{\"postId\":\"jP9KDyMkchuv6tHwm\"}}})": {
                    "__typename": "MultiCommentOutput",
                    "results": [],
                    "totalCount": null
                },
                "posts({\"enableTotal\":true,\"limit\":5,\"selector\":{\"pingbackPosts\":{\"postId\":\"jP9KDyMkchuv6tHwm\"}}})": {
                    "__typename": "MultiPostOutput",
                    "results": [],
                    "totalCount": 0
                }
            },
            "Post:jP9KDyMkchuv6tHwm": {
                "__typename": "Post",
                "_id": "jP9KDyMkchuv6tHwm",
                "slug": "how-to-become-a-mechanistic-interpretability-researcher",
                "title": "How To Become A Mechanistic Interpretability Researcher",
                "draft": null,
                "shortform": false,
                "hideCommentKarma": false,
                "af": true,
                "currentUserReviewVote": null,
                "userId": "KCExMGwS2ETzN3Ksr",
                "coauthorStatuses": null,
                "hasCoauthorPermission": true,
                "rejected": false,
                "debate": false,
                "collabEditorDialogue": false,
                "url": null,
                "postedAt": "2025-09-02T23:38:43.780Z",
                "createdAt": null,
                "sticky": false,
                "metaSticky": false,
                "stickyPriority": 2,
                "status": 2,
                "frontpageDate": "2025-09-02T23:41:53.215Z",
                "meta": false,
                "deletedDraft": false,
                "postCategory": "post",
                "tagRelevance": {
                    "4kQXps8dYsKJgaayN": 2,
                    "56yXXrcxRjrQs6z9R": 2,
                    "fF9GEdWXKJ3z73TmB": 2,
                    "fkABsGCJZ6y9qConW": 2,
                    "sYm3HiWcfZvrGu3ui": 1
                },
                "shareWithUsers": [],
                "sharingSettings": null,
                "linkSharingKey": null,
                "contents_latest": "E8uoWmvStdScjxJnC",
                "commentCount": 10,
                "voteCount": 40,
                "baseScore": 75,
                "extendedScore": {
                    "reacts": {},
                    "agreement": 0,
                    "approvalVoteCount": 40,
                    "agreementVoteCount": 0
                },
                "emojiReactors": {},
                "unlisted": false,
                "score": 1.0715864896774292,
                "lastVisitedAt": null,
                "isFuture": false,
                "isRead": false,
                "lastCommentedAt": "2025-09-04T09:58:25.237Z",
                "lastCommentPromotedAt": null,
                "canonicalCollectionSlug": null,
                "curatedDate": null,
                "commentsLocked": null,
                "commentsLockedToAccountsCreatedAfter": null,
                "question": false,
                "hiddenRelatedQuestion": false,
                "originalPostRelationSourceId": null,
                "location": null,
                "googleLocation": null,
                "onlineEvent": false,
                "globalEvent": false,
                "startTime": null,
                "endTime": null,
                "localStartTime": null,
                "localEndTime": null,
                "eventRegistrationLink": null,
                "joinEventLink": null,
                "facebookLink": null,
                "meetupLink": null,
                "website": null,
                "contactInfo": null,
                "isEvent": false,
                "eventImageId": null,
                "eventType": null,
                "types": [],
                "groupId": null,
                "reviewedByUserId": "55XxDBpfKkkBPm9H8",
                "suggestForCuratedUserIds": null,
                "suggestForCuratedUsernames": null,
                "reviewForCuratedUserId": null,
                "authorIsUnreviewed": false,
                "afDate": null,
                "suggestForAlignmentUserIds": [],
                "reviewForAlignmentUserId": null,
                "afBaseScore": 29,
                "afExtendedScore": {
                    "reacts": {},
                    "agreement": 0,
                    "approvalVoteCount": 17,
                    "agreementVoteCount": 0
                },
                "afCommentCount": 0,
                "afLastCommentedAt": "2025-09-02T23:38:43.780Z",
                "afSticky": false,
                "hideAuthor": false,
                "moderationStyle": null,
                "ignoreRateLimits": null,
                "submitToFrontpage": true,
                "onlyVisibleToLoggedIn": false,
                "onlyVisibleToEstablishedAccounts": false,
                "reviewCount": 0,
                "reviewVoteCount": 0,
                "positiveReviewVoteCount": 0,
                "manifoldReviewMarketId": null,
                "annualReviewMarketProbability": null,
                "annualReviewMarketIsResolved": null,
                "annualReviewMarketYear": null,
                "annualReviewMarketUrl": null,
                "group": null,
                "rsvpCounts": {},
                "podcastEpisodeId": null,
                "forceAllowType3Audio": false,
                "nominationCount2019": 0,
                "reviewCount2019": 0,
                "votingSystem": "namesAttachedReactions",
                "disableRecommendation": false,
                "user": {
                    "__ref": "User:KCExMGwS2ETzN3Ksr"
                },
                "coauthors": [],
                "readTimeMinutes": 66,
                "rejectedReason": null,
                "customHighlight": null,
                "lastPromotedComment": null,
                "bestAnswer": null,
                "tags": [{
                    "__ref": "Tag:4kQXps8dYsKJgaayN"
                }, {
                    "__ref": "Tag:56yXXrcxRjrQs6z9R"
                }, {
                    "__ref": "Tag:fF9GEdWXKJ3z73TmB"
                }, {
                    "__ref": "Tag:fkABsGCJZ6y9qConW"
                }, {
                    "__ref": "Tag:sYm3HiWcfZvrGu3ui"
                }],
                "socialPreviewData": {
                    "__ref": "SocialPreviewType:jP9KDyMkchuv6tHwm"
                },
                "feedId": null,
                "totalDialogueResponseCount": 0,
                "unreadDebateResponseCount": 0,
                "dialogTooltipPreview": null,
                "disableSidenotes": false,
                "canonicalSource": null,
                "noIndex": false,
                "viewCount": null,
                "commentSortOrder": null,
                "sideCommentVisibility": null,
                "collectionTitle": null,
                "canonicalPrevPostSlug": null,
                "canonicalNextPostSlug": null,
                "canonicalSequenceId": null,
                "canonicalBookId": null,
                "canonicalSequence": null,
                "canonicalBook": null,
                "canonicalCollection": null,
                "podcastEpisode": null,
                "bannedUserIds": null,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "feedLink": null,
                "feed": null,
                "sourcePostRelations": [],
                "targetPostRelations": [],
                "rsvps": null,
                "activateRSVPs": true,
                "fmCrosspost": {
                    "__typename": "CrosspostOutput",
                    "isCrosspost": true,
                    "hostedHere": true,
                    "foreignPostId": "7mDeuQzJ56wozDAah"
                },
                "glossary": [{
                    "__ref": "JargonTerm:Mw2qopyZuMv2ZFwux"
                }, {
                    "__ref": "JargonTerm:aucuuEYs2bg9n9Whq"
                }, {
                    "__ref": "JargonTerm:DvQiba5yzfRnGBTK5"
                }, {
                    "__ref": "JargonTerm:nxwqkKatrCCcfBvxg"
                }, {
                    "__ref": "JargonTerm:bYopjJdkLcudMRAZd"
                }, {
                    "__ref": "JargonTerm:mZDFjPJgNkwy6BrFN"
                }, {
                    "__ref": "JargonTerm:FbELb3r2TFxMAfryQ"
                }, {
                    "__ref": "JargonTerm:gCiJ8wX9Gsm25gHRf"
                }, {
                    "__ref": "JargonTerm:C6rE5KrJAjcwzzvJH"
                }, {
                    "__ref": "JargonTerm:obQBqxasnYMCELd7t"
                }, {
                    "__ref": "JargonTerm:ebqfGKjxzEyC6afjm"
                }, {
                    "__ref": "JargonTerm:u4gFFLuZudP3fkd7Z"
                }, {
                    "__ref": "JargonTerm:p9AH5LJCobPQsrWn3"
                }, {
                    "__ref": "JargonTerm:wBYHsMiWjywAbxyxy"
                }, {
                    "__ref": "JargonTerm:g9WMnKwfyqKgACJdK"
                }, {
                    "__ref": "JargonTerm:k69d5jT4J6abhyP9i"
                }, {
                    "__ref": "JargonTerm:huCiBPpE3tgsHgDAG"
                }, {
                    "__ref": "JargonTerm:wxHNv6zijgrbTxu6N"
                }, {
                    "__ref": "JargonTerm:CDZedjFDGhR6Dptr6"
                }, {
                    "__ref": "JargonTerm:pfjnKhCdZ4YqtZEBc"
                }, {
                    "__ref": "JargonTerm:eTaF5GuyjrPDDAAco"
                }, {
                    "__ref": "JargonTerm:NqFaZrhFuNgwaT4Jn"
                }, {
                    "__ref": "JargonTerm:kYkGtMAFwDqRFpD2N"
                }],
                "version": "1.4.0",
                "contents": {
                    "__ref": "Revision:E8uoWmvStdScjxJnC"
                },
                "myEditorAccess": "none",
                "sequence({\"sequenceId\":null})": null,
                "prevPost({\"sequenceId\":null})": null,
                "nextPost({\"sequenceId\":null})": null,
                "tableOfContents": {
                    "html": "<p><strong>Note<\/strong>: If you’ll forgive the shameless self-promotion, <strong>applications for <\/strong><a href=\"http://tinyurl.com/neel-mats-app\"><strong>my MATS stream<\/strong><\/a><strong>&#160;are open until<\/strong>&#160;<strong>Sept 12<\/strong>. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.<\/p><p><i>Last updated Sept 2 2025<\/i><\/p><h2 id=\"TL_DR\" data-internal-id=\"TL_DR\">TL;DR<\/h2><ul><li>This post is about the mindset and process I recommend if you want to <i>do<\/i>&#160;mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.<ul><li>Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.<\/li><li><strong>Learn the minimum viable basics, then do research.<\/strong>&#160;Mech interp is an empirical science<\/li><\/ul><\/li><li>Three stages:<ul><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learn the ropes<\/strong><\/a><strong>&#160;(≤1 month)<\/strong>&#160;learn the essentials, go breadth-first;<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Learn with research mini-projects<\/strong><\/a>&#160;practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full projects<\/strong><\/a>, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.<\/li><\/ul><\/li><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Stage 1:<\/strong><\/a><strong>&#160;Learning the Ropes<\/strong><ul><li><strong>Breadth over depth; get a good baseline not perfection<\/strong><\/li><li><strong>Learn the basics<\/strong>: <a href=\"#Machine_Learning___Transformer_Basics\">Code a transformer from scratch<\/a>, <a href=\"#Mechanistic_Interpretability_Techniques\">key mech interp techniques<\/a>, <a href=\"#Using_LLMs_for_Learning\">the landscape of the field<\/a>, <a href=\"#Machine_Learning___Transformer_Basics\">linear algebra intuitions<\/a>, <a href=\"#Mechanistic_Interpretability_Coding___Tooling\">how to write mech interp code<\/a>&#160;(<a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">ARENA is your friend<\/a>)<\/li><li><strong>Get your hands dirty<\/strong>: Do <i>not<\/i>&#160;just read things. Mech interp is a fundamentally empirical science<\/li><li><strong>Move on after a month<\/strong>. Don’t expect to feel “done” or to have covered <i>all <\/i>of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real<\/li><li><a href=\"#Using_LLMs_for_Learning\"><strong>Use LLMs extensively<\/strong><\/a>&#160;- they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)<\/li><\/ul><\/li><li><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><strong>Unpacking the research process<\/strong><\/a>:<ul><li><a href=\"#Unpacking_the_Research_Process\">Many skills<\/a>, categorise them by the feedback loops.<ul><li>Fast skills (minutes-hours) like write/run/debug experiments<\/li><li>Slow (weeks) like how to prioritise and when to pivot<\/li><li>Very slow (months) like generating good research ideas<\/li><\/ul><\/li><li><strong>Do <\/strong><i><strong>not<\/strong><\/i><strong>&#160;try to learn all skills at once<\/strong>. Focus on fast/medium skills first, then slowly expand<\/li><li><a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">4 phases of research<\/a>: finding an idea (<strong>ideation<\/strong>) -&gt; building intuition and hunches (<strong>exploration<\/strong>) -&gt; testing hypotheses (<strong>understanding<\/strong>) -&gt; refining and writing up (<strong>distillation<\/strong>)<\/li><\/ul><\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Stage 2:<\/strong><\/a><strong>&#160;Mini projects<\/strong>&#160;(1-5 days each for 2-4 weeks)<ul><li><a href=\"#Practicing_Exploration\">Exploration mindset<\/a>: <strong>Maximise information gain per unit time<\/strong>, learn how to get unstuck. You don't need a plan, so long as you're learning<\/li><li><a href=\"#Practicing_Understanding\">Understanding mindset<\/a>: <strong>Every research result is false until proven otherwise<\/strong>. The more exciting a result is, the more likely it is to be false. Be your own greatest critic<\/li><li>Idea quality (ideation) and write-ups (distillation) aren't the priority yet; <strong>taste and prioritization are learned by doing things<\/strong>.<\/li><li>Having good research ideas takes forever to learn, <strong>to choose early projects, cheat<\/strong>! <a href=\"#Choose_A_Project\">Pick well scoped projects<\/a>, eg extending a paper (ideas)<\/li><li><a href=\"#Using_LLMs_for_Research_Code\"><strong>Use LLMs extensively<\/strong><\/a><strong>&#160;<\/strong>- they should speed up your research/coding a <i>lot<\/i>&#160;(if you know how to use them properly!)<\/li><\/ul><\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Stage 3:<\/strong><\/a><strong>&#160;Towards full projects<\/strong><ul><li><strong>Work in 1-2 week sprints<\/strong>, post-mortem after each, pivot to another project unless it's going <i>great<\/i><\/li><li><a href=\"#Deepening_Your_Skills\"><strong>Slower skills<\/strong><\/a><strong>&#160;and <\/strong><a href=\"#Key_Research_Mindsets\"><strong>key mindsets<\/strong><\/a>: careful skepticism, awareness of the literature, prioritization, high productivity<\/li><li><a href=\"#Doing_Good_Science\"><strong>Do good science<\/strong><\/a><strong>, not flashy science<\/strong>&#160;- be honest about limitations, give proof you're not cherry picking, read your data, do the simple things that work, use real baselines.<\/li><li><a href=\"#Write_up_your_work_\"><strong>Write-up<\/strong><\/a><strong>&#160;your work<\/strong>! Distill it into a narrative, then iteratively expand it to a write-up<ul><li><strong>Good public work is <\/strong><a href=\"#Why_aim_for_public_output_\"><strong>your best credential<\/strong><\/a>&#160;- for careers, PhDs, finding mentors, etc<\/li><li><strong>Writing is not an afterthought<\/strong>&#160;- make time for it. <a href=\"#Common_mistakes\">The reader will understand less than you think<\/a><\/li><\/ul><\/li><li><strong>Practice <\/strong><a href=\"#Practicing_Ideation\"><strong>generating research ideas<\/strong><\/a>. If possible, try to imitation learn <a href=\"#Research_Taste_Exercises\">a mentor's research taste.<\/a><ul><li><a href=\"#Avoiding_Fads\">Avoid fads<\/a>, and think about <a href=\"#What_s_New_In_Mech_Interp_\">what’s new and exciting in mech interp<\/a><\/li><\/ul><\/li><\/ul><\/li><li><a href=\"#Advice_on_finding_a_mentor\"><strong>Proactively reach out to mentors<\/strong><\/a>&#160;Everything is <i>much<\/i>&#160;easier with a good mentor. Cold email, apply for mentoring programs, etc.<ul><li>Reach out to researchers who'll have time, not the most famous<\/li><\/ul><\/li><li><strong>Careers:<\/strong>&#160;If you want to work in the field, apply for things! <a href=\"#Where_to_apply\">Jobs<\/a>, <a href=\"#Mentoring_programs\">mentoring programs<\/a>, <a href=\"#Applying_for_grants\">funding<\/a>, <a href=\"#Relevant_Academic_Labs\">academic labs<\/a>.<ul><li>Bonus thoughts: <a href=\"#What_do_hiring_managers_look_for\">what do hiring managers look for<\/a>, <a href=\"#So_what_does_a_research_mentor_actually_do_\">what does a good research mentor actually do<\/a>, and <a href=\"#Should_you_do_a_PhD_\">should you do a PhD<\/a>?<\/li><\/ul><\/li><li>I also give various thoughts on how I'm thinking about the field nowadays, and what I’ve changed my mind about. I separate these from the practical advice, so you can take it or leave it.<ul><li>Covering: <a href=\"#Interlude__What_is_mech_interp_\">how I currently define the field<\/a>, why I'm <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">pessimistic on ambitious reverse engineering, and excited about more pragmatic approaches<\/a>, <a href=\"#What_s_New_In_Mech_Interp_\">what recent work I am<i>&#160;<\/i>excited about<\/a>&#160;and recommend building on.<\/li><li>And if any of that worldview appeals, you may want to apply to work with me via <a href=\"http://tinyurl.com/neel-mats-app\">MATS, due Sept 12<\/a>!<\/li><\/ul><\/li><\/ul><h2 id=\"Introduction\" data-internal-id=\"Introduction\">Introduction<\/h2><p>Mechanistic interpretability (mech interp) is, in my incredibly biased opinion, one of the most exciting research areas out there. We have these incredibly complex AI models that we don't understand, yet there are tantalizing signs of real structure inside them. Even partial understanding of this structure opens up a world of possibilities, yet is neglected by 99% of machine learning researchers. There’s so much to do!<\/p><p>I think mech interp is an unusually easy field to learn about on your own: there’s a lot of educational materials, you don’t need too much compute, and there’s short feedback loops. But if you're new, it can feel pretty intimidating to get started. This is my updated guide on how to skill up, get involved, reach the point where you can do actual research, and some advice on how to go from there to a career/academic role in the field.<\/p><p>This guide is deliberately highly opinionated. My goal is to convey a productive mindset and concrete steps that I think will work well, and give a sense of direction, rather than trying to give a fully broad overview or perfect advice. (And many of the links are to my own work because that's what I know best. Sorry!)<\/p><h3 id=\"High_Level_Framing\" data-internal-id=\"High_Level_Framing\">High-Level Framing<\/h3><p>My core philosophy for getting into mech interp is this: learn the absolute minimal basics as quickly as possible, and then immediately transition to learning by doing research.<\/p><p>The goal is not to read every paper before you touch research. When doing research you'll notice gaps and go back to learn more. But being grounded in a project will give you vastly more direction to guide your learning, and contextualise why anything you’re learning actually matters. You just want enough grounding to start a project with some understanding of what you’re doing.<\/p><p>Don't stress about the research quality at first, or having the perfect project idea. Key skills, like <a href=\"https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/Ldrss6o3tiKT6NdMm\">research taste<\/a>&#160;and the ability to prioritize, take time to develop. Gaining experience—even messy experience—will teach you the basics like how to run and interpret experiments, which in turn help you learn the high-level skills.<\/p><p>I break this down into three stages:<\/p><ol><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learning the ropes<\/strong><\/a>, where you work through the basics breadth first, and after at most a month, move on to stage 2<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Practicing research with mini-projects<\/strong><\/a>. Work on throwaway, 1-5 day research projects. Focus on practicing the basic research skills with the fastest feedback loops, don’t stress about having the best ideas, or writing them up. After 2-4 weeks, move on to stage 3<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full-projects<\/strong><\/a>: work in 1-2 week sprints. After each, do a post-mortem and pivot to something else, <i>unless <\/i>it was going great and has momentum. Eventually, you should end up working on something longer-term. Start thinking about the deeper skills and research mindsets, practice having good ideas, and prioritize making good public write-ups of sprints that went well<\/li><\/ol><h2 id=\"Stage_1__Learning_the_Ropes\" data-internal-id=\"Stage_1__Learning_the_Ropes\">Stage 1: Learning the Ropes<\/h2><p>Your goal here is learning the basics: how to write experiments with a mech interp library, understanding the key concepts, getting the lay of the land.<\/p><p data-internal-id=\"ftnt_ref1\">Your aim is learning enough that the rest of your learning can be done via doing research, <i>not<\/i>&#160;finishing learning. Prioritize ruthlessly. <strong>After max 1 month<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-noteref\" id=\"fnrefnifk1wb1jum\"><sup><a href=\"#fnnifk1wb1jum\">[1]<\/a><\/sup><\/span><strong>, move on to stage 2<\/strong>. I’ve flagged which parts of this I think are essential, vs just nice to have.<\/p><p><strong>Do not just read papers <\/strong>- a common mistake among academic types is to spend months reading as many papers as they can get their hands on before writing code. Don’t do it. Mech interp is an empirical science, getting your hands dirty gives key context for your learning. Intersperse reading papers with doing coding tutorials or small research explorations. See <a href=\"https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">my research walkthroughs<\/a>&#160;for an idea of what tiny exploratory projects can look like.<\/p><p>LLMs are a key tool - see <a href=\"#h.ab01gbohcxm5\">the section below<\/a>&#160;for advice on using them well<\/p><h3 id=\"Machine_Learning___Transformer_Basics\" data-internal-id=\"Machine_Learning___Transformer_Basics\"><strong>Machine Learning &amp; Transformer Basics<\/strong><\/h3><p><i>Assuming you already know basic Python and introductory ML concepts.<\/i><\/p><ul><li><strong>Maths:<\/strong><ul><li><strong>Linear Algebra is King (Essential):<\/strong>&#160;You need to think in vectors and matrices fluently. This is by far the highest value set of generic math you should learn to do mech interp or ML research.<ul><li><i>Resource:<\/i>&#160;3Blue1Brown's<a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\">&#160;Essence of Linear Algebra<\/a>.<\/li><li><strong>Highly recommended<\/strong>: Put <a href=\"https://transformer-circuits.pub/2021/framework/index.html\">A Mathematical Framework For Transformer Circuits<\/a>&#160;in the context window and have the LLM generate exercises to test your intuitions about transformer internals.<\/li><li>LLMs are great for checking whether linear algebra actually clicks. Try summarizing what you've learned and the links between different concepts and ask an LLM whether you are correct. For example:<ul><li>Ensure you understand SVD and why it works<\/li><li>What does changing basis mean and why does it matter<\/li><li>Key ways a low rank and full rank matrix differ<\/li><\/ul><\/li><\/ul><\/li><li><strong>Other Bits:<\/strong>&#160;Basic probability, info theory, optimization, vector calculus.<ul><li>Use an LLM tutor to quiz your understanding on the parts most relevant to transformers<\/li><\/ul><\/li><li>Generally don’t bother learning other areas of maths (unless doing it for fun!)<\/li><\/ul><\/li><li><strong>Practical ML with PyTorch: (Essential)<\/strong><ul><li><p data-internal-id=\"ftnt_ref2\">Code a simple Transformer (like GPT-2) from scratch. ARENA Chapter 1.1 is a great coding tutorial<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-noteref\" id=\"fnrefue9pdw6v8rj\"><sup><a href=\"#fnue9pdw6v8rj\">[2]<\/a><\/sup><\/span><\/p><ul><li><p data-internal-id=\"ftnt_ref2\">This builds intuitions for mech interp <i>and <\/i>on using PyTorch.<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">I have two video tutorials on this, starting from the basics - <a href=\"https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz\">start here<\/a>&#160;if you’re not sure what to do!<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">And use LLMs to fill in any background things you’re missing, like PyTorch basics<\/p><\/li><\/ul><\/li><\/ul><\/li><li><strong>Cloud GPUs:<\/strong><ul><li>You’ll need to be able to run language models, which (typically) needs a GPU<\/li><li>You can start with Google Colab to get started fast, but it’ll be very constraining to use long-term. Learn to rent and use a cloud GPU.<ul><li>Newer Macbook Pros, or computers with powerful gaming GPUs may also be able to run LLMs locally<\/li><\/ul><\/li><li><i>Resource:<\/i>&#160;ARENA has a<a href=\"https://arena-appendix.streamlit.app/cloud-gpus\">&#160;<\/a><a href=\"https://arena-chapter0-fundamentals.streamlit.app/#vm-setup-instructions\">guide<\/a>. I like<a href=\"http://runpod.io/\">&#160;<\/a><a href=\"http://runpod.io\">runpod.io<\/a>&#160;as a provider;<a href=\"http://vast.ai/\">&#160;vast.ai<\/a>&#160;is cheaper.<\/li><li>nnsight also lets you do some <a href=\"https://nnsight.net/notebooks/tutorials/get_started/start_remote_access/\">interpretability on certain models they host themselves<\/a>, including LLaMA 3 405B, which can be a great way to work with larger models.<\/li><\/ul><\/li><\/ul><h3 id=\"Mechanistic_Interpretability_Techniques\" data-internal-id=\"Mechanistic_Interpretability_Techniques\">Mechanistic Interpretability Techniques<\/h3><p>A lot of mech interp research looks like knowing the right technique to apply and in what context. This is a key thing to prioritise getting your head around when starting out. You’ll learn this with a mix of reading educational materials and doing coding tutorials like ARENA (discussed in next sub-section).<\/p><ul><li><a href=\"https://arxiv.org/abs/2405.00208\">Ferrando et al<\/a>&#160;is a good <strong>overview<\/strong>&#160;of the key techniques - it’s long enough that you shouldn’t prioritise reading it in full, but it’s a great reference<ul><li>Put it in a LLM context window and ask questions, or to write you exercises<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Essential<\/strong>: Make sure you understand these <strong>core techniques<\/strong>, well enough that you can code it up yourself on a simple model like GPT-2 Small<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-noteref\" id=\"fnrefhh6mwdeo4zm\"><sup><a href=\"#fnhh6mwdeo4zm\">[3]<\/a><\/sup><\/span>:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Activation Patching<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Linear Probes<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Using Sparse Autoencoders (SAEs) (you only need to write code that uses an SAE, not trains one)<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Max Activating Dataset Examples<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Nice-to-have:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Steering Vectors<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Direct Logit Attribution (DLA) (a simpler version is called logit lens)<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Key exercise<\/strong>: Describe each technique to an LLM with Ferrando et al in the context window and ask for feedback. Iterate until you get it all right.<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Use an anti-sycophancy prompt to get real feedback, by pretending someone else wrote your answer, e.g. “I saw someone claim this, it seems pretty off to me, can you help me give them direct but constructive feedback on what they missed? [insert your description]”<\/p><\/li><\/ul><\/li><\/ul><\/li><li>Remember that there’s a bunch of valuable <strong>black-box interpretability <\/strong>techniques! (ie that don’t use the model’s internals) You can often correctly guess a model’s algorithm by reading its chain of thought. Careful variation of the prompt is a powerful way to causally test hypotheses.<ul><li>They’re an additional tool. Often the correct first step in an investigation is just talking to the model and bunch and observing its behaviour. Don’t be a purist and dismiss them as “not rigorous” - they have uses and flaws, just like any other technique.<ul><li>One <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">project I supervised<\/a>&#160;on interpreting “self-preservation” in frontier models started with simple black-box techniques, and it just worked, we never needed anything fancier.<\/li><\/ul><\/li><li>Understand fancier black-box techniques like <a href=\"https://arxiv.org/abs/2312.12321\">token forcing<\/a>&#160;(aka prefill attacks) where you put words in a model’s mouth.<\/li><\/ul><\/li><\/ul><h3 id=\"Mechanistic_Interpretability_Coding___Tooling\" data-internal-id=\"Mechanistic_Interpretability_Coding___Tooling\">Mechanistic Interpretability Coding &amp; Tooling<\/h3><ul><li><p data-internal-id=\"ftnt_ref4\"><strong>Goal:<\/strong>&#160;Get comfortable running experiments and \"playing\" with model internals. Get the engineering basics down<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-noteref\" id=\"fnrefsxyjce3nii\"><sup><a href=\"#fnsxyjce3nii\">[4]<\/a><\/sup><\/span>. Get your hands dirty<strong>.<\/strong><\/p><\/li><li><strong>ARENA<\/strong>: ARENA has <a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">a set of fantastic coding tutorials by Callum McDougall<\/a>, you should just go do these. But there’s tons, so <strong>prioritize ruthlessly<\/strong>.<ul><li><strong>Essential<\/strong><i><strong>:<\/strong><\/i><strong>&#160;Chapter 1.2<\/strong>&#160;(Interpretability Basics – prioritize the first 3 sections on tooling, direct observation, and patching).<\/li><li><i>Recommended: <\/i>1.4.1 (Causal Interventions &amp; Activation Patching – this is a core technique).<\/li><li><i>Worthwhile<\/i>: 1.3.2 (Sparse Autoencoders (SAEs) – Skim or Skip section 1, the key thing to get from the rest is an intuition for what SAEs are, strengths and weaknesses, and how to use an open source SAE. Don’t worry about training them).<\/li><\/ul><\/li><li><strong>Tooling <\/strong>(<strong>Essential<\/strong>)<strong>:<\/strong>&#160;Get proficient with at least one mech interp library, this is what you’ll use to run experiments.<ul><li><a href=\"https://github.com/TransformerLensOrg/TransformerLens\">TransformerLens<\/a>: best for small models &lt;=9B where you want to write more complex interpretability experiments, or work with many models at once.<ul><li>As of early Sept 2025, TransformerLens <a href=\"https://github.com/TransformerLensOrg/TransformerLens/releases/tag/v3.0.0a5\">v3<\/a>&#160;is in alpha, works well with large models and is far more flexible.<\/li><\/ul><\/li><li><a href=\"http://nnsight.net/\">nnsight<\/a>: More performant, works well on larger models, it’s just a wrapper around standard LLM libraries like HuggingFace transformers<\/li><\/ul><\/li><li><strong>LLM APIs<\/strong>: Learn how to use an LLM API to call an LLM programmatically. This is super useful for measuring qualitative things about some data, and for generating synthetic datasets<ul><li>I like <a href=\"http://openrouter.ai\">openrouter.ai<\/a>&#160;which lets you access almost all the important LLMs from a single place. GPT5 and Gemini are reasonably priced and good defaults, they have a range of sizes<ul><li>Cerebras and Groq have <i>way <\/i>higher throughput than normal providers, and serve a handful of open source models, they may be worth checking out.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref6\">Exercise: Make a happiness steering vector (for e.g. GPT-2 Small) by having an LLM via an API generate 32 happy prompts and 32 sad prompts, and taking the difference in mean activations<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-noteref\" id=\"fnrefkte6u8splw\"><sup><a href=\"#fnkte6u8splw\">[5]<\/a><\/sup><\/span>&#160;(e.g. the residual stream at the middle layer). Add this vector to the model’s residual stream<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-noteref\" id=\"fnref2ob115pcmet\"><sup><a href=\"#fn2ob115pcmet\">[6]<\/a><\/sup><\/span>&#160;while generating responses to some example prompts, and use an LLM API to rate how happy they seem, and see this score go up when steering.<\/p><\/li><\/ul><\/li><li><strong>Open source LLMs<\/strong>: You’ll want to work a lot with open source LLMs, as the thing you’re trying to interpret. The best open source LLM changes a lot<ul><li><p data-internal-id=\"ftnt_ref7\">As of early Sept 2025, Qwen3 is a good default model family. Each model has reasoning and non-reasoning mode, there’s a good range of sizes, and most are dense<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-noteref\" id=\"fnref1b9r0ass7sd\"><sup><a href=\"#fn1b9r0ass7sd\">[7]<\/a><\/sup><\/span>&#160;<\/p><ul><li><p data-internal-id=\"ftnt_ref7\">Gemma 3 and LLaMA 3.3 are decent non-reasoning models. I’ve heard bad things about gpt-oss and LLaMA 4<\/p><\/li><\/ul><\/li><li><i>Gotcha: <\/i>The different open source LLMs often have different tokenizations and formats for chat or reasoning tokens. Using the wrong token format can only somewhat degrade performance and may be hard to notice while corrupting your results - keep an eye out, try hard to find where this might be documented, and sanity check by e.g. comparing to official evals<\/li><\/ul><\/li><\/ul><h3 id=\"Understanding_the_literature\" data-internal-id=\"Understanding_the_literature\">Understanding the literature<\/h3><p>Your priority is to understand the concepts and the basics, but you want a sense for the landscape of the field, so you should practice reading at least some papers.<\/p><ul><li>Remember, <strong>breadth over depth<\/strong>. Skim things, get a sense of what's out there, and only dive into the things that are most interesting.<ul><li>You should be heavily using <strong>LLMs<\/strong>&#160;here. Give them something you're considering reading and get a summary, ask questions about the work, summarise your understanding to it and ask for feedback (with an anti-sycophancy prompt).<ul><li>If you aren't able to verify yourself, cross-reference by asking multiple LLMs and making sure they all say consistent things.<\/li><\/ul><\/li><\/ul><\/li><li>Here’s <a href=\"https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite\">a list of my favourite papers<\/a>&#160;(as of mid 2024) with summaries and opinions<ul><li>Do <i>not <\/i>try to read all of these in full. Skim summaries, skim abstracts, pick a few to explore deeper with an LLM, <i>then<\/i>&#160;decide if you want to read the full paper.<\/li><li><a href=\"https://www.youtube.com/@neelnanda2469\">My YouTube Channel<\/a>:<a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T&index=1\">&#160;<\/a><a href=\"https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&pp=gAQB\">Paper walkthroughs<\/a>, <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">recordings of myself doing research<\/a>, and talks.<\/li><\/ul><\/li><li><a href=\"https://arxiv.org/abs/2501.16496\">Open Problems In Mechanistic Interpretability<\/a>&#160;is a decent recent literature review, that a lot of top mech interp people were involved in<ul><li>Be warned that the paper basically consists of a bunch of opinionated and disagreeable researchers writing their own sections and often having strong takes. Don’t defer to it too much, but it's a good way to quickly assess what's out there.<\/li><\/ul><\/li><li><strong>Deep dives<\/strong>: You should read at least one paper carefully and in full. This is a useful skill that you will use in research projects where there’s a handful of extremely relevant papers to your project<ul><li>This is much more than just reading the words! You should write out a summary, try to understand the surrounding context with LLM help, be able to describe why the paper exists, the motivation, the problem it's trying to solve, etc.<\/li><li>Aim for a barbell strategy: put minimal effort into most papers and a lot of effort into a few.<\/li><\/ul><\/li><li><strong>LLMs<\/strong>: LLMs are a super useful tool for exploring the literature, but easy to shoot yourself in the foot with.<ul><li>As a search engine over the literature (especially with some lit reviews in context, or a starting paper), basically doing a lit review, finding relevant work for a question you have, etc.<ul><li><p data-internal-id=\"ftnt_ref8\">As a tool to help you skim a paper - put the paper in the context window<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-noteref\" id=\"fnrefbzop9pji3nl\"><sup><a href=\"#fnbzop9pji3nl\">[8]<\/a><\/sup><\/span>&#160;then get a summary, ask it questions, etc<\/p><\/li><li>If you’re concerned about hallucinations, you can&#160;ask it to support answers with quotes (and verify these are real and make sense), or give its answer to another LLM and ask for harsh critique of all the inaccuracies. Honestly, I often don’t bother though, frontier reasoning models are pretty good now.<\/li><\/ul><\/li><li>As a tool to help with deep dives - you need to actually read the paper, but I recommend having the LLM chat open as you read with the paper in the context and asking it questions, for context, etc every time you get confused.<\/li><\/ul><\/li><\/ul><h3 id=\"Using_LLMs_for_Learning\" data-internal-id=\"Using_LLMs_for_Learning\">Using LLMs for Learning<\/h3><p><i>Note: I expect this section to go out of date fast! Written early Sept 2025<\/i><\/p><p>LLMs are a super useful tool for learning, especially in a new field. While they struggle to beat experts, they often beat novices. If you aren’t using them regularly throughout this process, I’d guess you’re leaving a bunch of value on the table.<\/p><p>But LLMs have weird flaws and strengths, and it’s worth being intentional about how you use them:<\/p><ul><li><strong>Use a good model<\/strong>:&#160;The best paid models are way better than e.g. free ChatGPT. Don't be a cheapskate; if you can, get a $20/month subscription, it makes a big difference. Gemini 2.5 Pro, Claude 4.1 Opus with extended thinking, and GPT-5 Thinking are all reasonable. (do <i>not <\/i>use non-thinking GPT-5 or anything older like GPT-4o, reasoning models are a big upgrade)<ul><li>If you can’t get a subscription, Gemini 2.5 Pro is also available for free, and is the best.<\/li><li>Use Gemini 2.5 Pro via <a href=\"https://aistudio.google.com/prompts/new_chat\">AI Studio<\/a>, it’s way better than the main Gemini interface&#160;and has much nicer rate limits for free users. Always use compare mode (the button in the header with two arrows) to see two responses in parallel from Pro<\/li><li>See <a href=\"https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher?commentId=jDzbZGnjWDMsNjDPQ\">thoughts<\/a> from my MATS alum Paul Bogdan comparing different LLMs for learning, and why he currently prefers Gemini<\/li><\/ul><\/li><li><strong>System Prompts:<\/strong>&#160;System prompts make a big difference - be concrete and specific about what you want, and how you want it done.<ul><li>LLMs are good at this: I'll just ramble at one about what the task is, my criteria, the failure modes I don't want, and then it’ll just write the prompt for me<\/li><li>If the prompt doesn’t work, tell the LLM what it did wrong, and see if it can rewrite the prompt for you.<\/li><\/ul><\/li><li><strong>Merge perspectives<\/strong>:<ul><li>Ask a Q to multiple different frontier LLMs, give LLM B’s response to LLM A and ask it to assess the strengths and weaknesses then merge.<ul><li>If a point is in both original responses, it’s probably not a hallucination<\/li><\/ul><\/li><li>If you want to fact check an LLM’s answer, give it to another LLM with an anti-sycophancy prompt<\/li><\/ul><\/li><li><strong>Anti-Sycophancy Prompts:<\/strong>&#160;LLMs are bad at giving critical feedback. Frame your request so the sycophantic thing to do is to be critical, by pretending someone else wrote the thing you want feedback on.<ul><li><i>\"A friend wrote this explanation and asked for brutally honest feedback. They'll be offended if I hold back. Please help me give them the most useful feedback.\"<\/i><\/li><li><i>\"I saw someone claiming this, but it seems pretty dumb to me. What do you think?\"<\/i><\/li><li><i>“Some moron wrote this thing, and I find this really annoying. Please write me a brutal but truthful response”<\/i><\/li><\/ul><\/li><li><strong>Learn actively, not passively:<\/strong><ul><li><strong>Summarize <\/strong>your understanding back to the LLM in your own words and ask for critical feedback. Do this every time you read a paper or learn about a new concept<\/li><li>Try having it teach you <strong>socratically<\/strong>. Note: you can probably design a better system prompt than the official “study mode”<\/li><li>Ask the LLM to <strong>generate exercises<\/strong>&#160;to test your understanding, including maths and coding exercises as appropriate.<ul><li>Gemini can make multiple choice quizzes, which some enjoy<\/li><li>Coding exercises can be requested with accompanying tests, and template code with blank functions for you to fill out, a la the ARENA tutorials.<\/li><\/ul><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref9\"><strong>Context engineering:<\/strong>&#160;Modern LLMs are much more useful with relevant info in context. If you give them the paper in question, or source code of the relevant library<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-noteref\" id=\"fnref207k0k5nobb\"><sup><a href=\"#fn207k0k5nobb\">[9]<\/a><\/sup><\/span>, they’ll be far more helpful.<\/p><ul><li><p data-internal-id=\"ftnt_ref9\">See <a href=\"https://drive.google.com/drive/u/0/folders/1GfrgKJwndk-twnJ8K7Ba-TE9i_8wBWAU\">this folder<\/a>&#160;for a bunch of saved context files for mech interp queries. If you don’t know what you need, just use <a href=\"https://drive.google.com/file/d/18cF3lkU17_elUSv0zk8KSVejM1jGfNnz/view?usp=drive_link\">this default file<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref9\">I recommend Gemini 2.5 Pro (1M context window) via<a href=\"http://aistudio.google.com/\">&#160;aistudio.google.com<\/a>; the UI is better. Always turn compare mode on, you get two answers in parallel<\/p><\/li><\/ul><\/li><li><strong>Voice dictation<\/strong>: If you dictate to your LLM, via free speech-to-text software, and run it with no editing, it’ll understand fine. I personally find this much easier, especially when brain-dumping.<ul><li><a href=\"http://superwhisper.com\">Superwhisper<\/a>&#160;on Mac is great; Superwhisper is not currently available on Windows, but Windows users can use <a href=\"https://wisprflow.ai/\">Whispr Flow<\/a>.<\/li><\/ul><\/li><li><strong>Coding<\/strong>: LLM tools like Cursor are great for coding, but <i>not <\/i>if your goal is to learn. For things like ARENA, only let yourself use browser-based LLMs, and only use them as a tutor. Don’t copy and paste code, your goal is to learn not complete exercises.<\/li><\/ul><h2 id=\"Interlude__What_is_mech_interp_\" data-internal-id=\"Interlude__What_is_mech_interp_\">Interlude: What is mech interp?<\/h2><p><i>Feel free to skip to the<\/i>&#160;<i>“<\/i><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p data-internal-id=\"ftnt_ref10\">At this point it’s worth reflecting on what mech interp actually <i>is<\/i>. What are we even doing here? There isn't a consensus definition on how exactly to define mechanistic interpretability, and different researchers will give very different takes. But <i>my<\/i>&#160;working definition is as follows<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-noteref\" id=\"fnref979wnkvgpa4\"><sup><a href=\"#fn979wnkvgpa4\">[10]<\/a><\/sup><\/span>.<\/p><ul><li><strong>Interpretability<\/strong>&#160;is the study of understanding models, gaining insight into their behavior, the cognition inside of them, why and how they work, etc. This is the important part and the heart of the field.<\/li><li><strong>Mechanistic<\/strong>&#160;means using the internals of the model, the weights and activations<\/li><li>So <strong>mechanistic interpretability <\/strong>is any approach to understanding the model that uses its internals.<ul><li>This is distinct from some other worthwhile directions, like <strong>black box interpretability<\/strong>, understanding models without using the internals, and <strong>model internals<\/strong>, using the internals of the model for other things like steering vectors.<\/li><\/ul><\/li><\/ul><p><strong>Why this definition?<\/strong>&#160;To do impactful research, it's often good to find the directions that other people are missing. I think of most of machine learning as non-mechanistic non-interpretability. 99% of ML research just looks at the inputs and outputs to models, and treats its north star as controlling their behavior. Progress is defined by making a number go up, not to explain why it works. This has been very successful, but IMO leaves a lot of value on the table. Mechanistic interpretability is about doing better than this, and has achieved a bunch of cool stuff, like <a href=\"https://arxiv.org/abs/2310.16410\">teaching grandmasters how to play chess better by interpreting AlphaZero<\/a>.<\/p><p><strong>Why care?<\/strong>&#160;Obviously, our goal is not “do things if and only if they fit the above definition”, but I find it a useful one. To discuss this, let’s first consider our actual goals here. To me, <strong>the ultimate goal is to make human-level AI systems (or beyond) safer<\/strong>. I do mech interp because I think we’ll find enough understanding of what happens inside a model to be pragmatically useful here (also, because mech interp is fun!): to better understand how they work, detect if they're lying to us, detect and diagnose unexpected failure modes, etc. But people’s goals vary, e.g. real-world usefulness today, aesthetic beauty, or scientific insight. It’s worth thinking about what yours are.<\/p><p>Some implications of this framing worth laying out:<\/p><ul><li>My ultimate <strong>north star is pragmatism<\/strong>&#160;- achieve enough understanding to be (reliably) useful. Subgoals like “completely reverse engineer the model” are just means to an end.<ul><li>One of my big shifts in research prioritization in recent years is concluding that <strong>reverse engineering is not the right aim<\/strong>. Instead, I think we should just more directly try to do pragmatic work that enables us to do useful things using internals. I discuss this shift more <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">later on<\/a>.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref11\">This is a <strong>broad definition<\/strong>. Historically, the field has focused on more specific agendas, like ambitious reverse engineering of models. But I think we shouldn’t limit ourselves, there’s many other important and neglected directions and the field is large enough to cover a lot of ground<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-noteref\" id=\"fnref3zw26zes9dx\"><sup><a href=\"#fn3zw26zes9dx\">[11]<\/a><\/sup><\/span><\/p><\/li><li>It’s about <strong>understanding<\/strong>, not just using internals - model internals methods like steering vectors can be useful for shaping a model’s behaviour, but compete with many powerful methods like prompting and fine-tuning. Very few areas of ML can achieve understanding<\/li><li><strong>Don’t be a purist<\/strong>&#160;- using internals is a means to an end. If black-box methods are the right tool, use them<\/li><\/ul><h2 id=\"The_Big_Picture__Learning_the_Craft_of_Research\" data-internal-id=\"The_Big_Picture__Learning_the_Craft_of_Research\">The Big Picture: Learning the Craft of Research<\/h2><p data-internal-id=\"ftnt_ref12\">So, you've gone through the tutorials, you understand the core concepts, and you can write some basic experimental code. Now comes the hard part: learning how to actually do mech interp research<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-noteref\" id=\"fnref7cxhc64szn8\"><sup><a href=\"#fn7cxhc64szn8\">[12]<\/a><\/sup><\/span>.<\/p><p>This is an inherently difficult thing to learn, of course. But IMO people often misunderstand what they need to do here, try to learn everything at once, or more generally make life unnecessarily hard for themselves. The key is to break the process down, understand the different skills involved, and focus on <strong>learning the pieces with the fastest feedback loops first<\/strong>.<\/p><p data-internal-id=\"ftnt_ref13\">I suggest breaking this down into two stages<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-noteref\" id=\"fnref9wj0u0qz3q\"><sup><a href=\"#fn9wj0u0qz3q\">[13]<\/a><\/sup><\/span>.<\/p><p><strong>Stage 2<\/strong>: working on a bunch of throwaway mini projects of 1-5 days each. Don't stress about choosing the best projects or producing public output. The goal is to learn the skills with the fastest feedback loops.<\/p><p><strong>Stage 3: <\/strong>After a few weeks of these, start to be more ambitious: paying more attention to how you choose your projects, gaining the subtler skills, and how to write things up. I still recommend working iteratively, in one to two week sprints, but ending up with longer-term projects if things go well.<\/p><p>Note: Unlike stage 1 to 2, the transition from stages two to three should be fairly gradual as you take on larger projects and become more ambitious. A good default would be after three to four weeks in stage two, but you don’t need to have a big formal shift.<\/p><p><strong>Mentorship<\/strong>: A good mentor is a major accelerator, and finding one should be a major priority for you. In the careers section, I provide advice on <a href=\"#Advice_on_finding_a_mentor\">how to go about finding a good mentor<\/a>, and <a href=\"#So_what_does_a_research_mentor_actually_do_\">how concretely they can add value<\/a>. In the rest of the post I'll write most of it assuming you do not have a mentor and then flag the ways to use a mentor where appropriate.<\/p><h3 id=\"Unpacking_the_Research_Process\" data-internal-id=\"Unpacking_the_Research_Process\">Unpacking the Research Process<\/h3><p>I find it helpful to think of research as a cycle of four distinct stages. Read <a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">my blog post on the research proces<\/a>&#160;for full details, but in brief:<\/p><ul><li><strong>Ideation:<\/strong>&#160;You choose a research problem or a general domain to focus on.<\/li><li><strong>Exploration:<\/strong>&#160;You may not have a specific hypothesis yet; you’re just trying to figure out the right questions to ask, and build deeper intuition for the domain. Your north star is to gain information and surface area.<\/li><li><strong>Understanding:<\/strong>&#160;This begins when you have a concrete hypothesis, and some intuitive understanding of the domain. Your north star is to convince yourself that the hypothesis is true or false.<\/li><li><strong>Distillation:<\/strong>&#160;Once you’re convinced, your north star is to compress your findings into concise, rigorous truth that you can communicate to the world - create enough experimental evidence to convince others, write it up clearly, and share it.<\/li><\/ul><p>Underpinning these stages is a host of skills, best separated by how quickly you can apply them and get feedback. We learn by doing things and getting feedback, so you’ll learn the fast ones much more quickly. I put a rough list and categorization below.<\/p><p>My general advice is <strong>to prioritize learning these in order of feedback loops<\/strong>. If it seems like you need a slow skill to get started, like the taste to choose a good research problem, find a way to cheat rather than stressing about not having that skill (e.g. doing an incremental extension to a paper, getting one from a mentor, etc).<\/p><ul><li><strong>Fast Loop (minutes-hours):<\/strong><ul><li>Planning and writing experiment code<ul><li><strong>Medium<\/strong>: Designing great experiments<\/li><li><strong>Medium<\/strong>: Knowing when to write hacky vs. quality code.<\/li><\/ul><\/li><li>Running/debugging experiments<ul><li><strong>Medium/Slow<\/strong>: Spotting and fixing subtle bugs (e.g., you got your tokenization subtly wrong, you didn’t search hyper-parameters well enough, etc)<\/li><\/ul><\/li><li>Interpreting the results of a single experiment.<ul><li><strong>Medium<\/strong>: Understanding whether your results support your conclusions<\/li><li><strong>Slow<\/strong>: Spotting subtle interpretability illusions where your results don't actually support your claims<\/li><\/ul><\/li><\/ul><\/li><li><strong>Medium Loop (days):<\/strong><ul><li>Developing a conceptual understanding of mech interp<ul><li><strong>Slow<\/strong>: Noticing and fixing your own subtle confusions<\/li><li><strong>Slow<\/strong>: Build a deep knowledge of the literature<\/li><\/ul><\/li><li>Knowing how to explore without getting stuck<\/li><li>Writing up results<ul><li><strong>Slow<\/strong>: Communicating your work in a way that’s genuinely clear to people.<\/li><li><strong>Slow<\/strong>: Communicating why your work is <i>interesting<\/i>&#160;to people<\/li><\/ul><\/li><\/ul><\/li><li><strong>Slow Loop (weeks):<\/strong><ul><li>Prioritizing which experiment to do next<\/li><li>Knowing when to continue with a research direction or pivot to another angle of attack/another project<\/li><li>Identifying bad research ideas, <i>without <\/i>doing a project on them first<\/li><\/ul><\/li><li><strong>Very Slow Loop (months):<\/strong><ul><li>Coming up with good research ideas. This is the core of \"research taste.\"<\/li><\/ul><\/li><\/ul><p>Your progression should be simple: First, focus on the fast/medium skills behind exploration and understanding with throwaway projects. Then, graduate to end-to-end projects where you can intentionally practice the deeper skills, and practice ideation and distillation too.<\/p><h3 id=\"What_is_research_taste_\" data-internal-id=\"What_is_research_taste_\">What is research taste?<\/h3><p>A particularly important and fuzzy type of skill is called research taste. I basically think of this as the bundle of intuitions you get with enough research experience that let you do things like come up with good ideas, predict if an idea is promising, have conviction in good research directions, etc. Check out <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">my post on the topic<\/a>&#160;for more thoughts.<\/p><p>I broadly think you should just ignore it for now, find ways to compensate for not having much yet, and focus on learning the fast-medium skills, and this will give you a much better base for learning it. In particular, it's much faster to learn with a mentor, so if you don't have a mentor at the start, you should prioritize other things.<\/p><p>But you want to learn it eventually, so it's good to be mindful of it throughout, and look for opportunities to practice and learn lessons. I recommend treating it as a nice-to-have but not stressing about it<\/p><p>Note, one important trap here is that having good taste can often manifest as having confidence and conviction in some research direction. But often novice researchers develop this confidence and conviction significantly <i>before <\/i>they develop the ability to not be confident in bad ideas. It’s often a good learning experience to once or twice pursue a thing you feel really convinced is going to be epic and then discover you're wrong, so it's not that bad an outcome, especially in stage 2 (mini-projects) but be warned.<\/p><h2 id=\"Stage_2__Practicing_Research_with_Mini_Projects\" data-internal-id=\"Stage_2__Practicing_Research_with_Mini_Projects\">Stage 2: Practicing Research with Mini-Projects<\/h2><p>With that big picture in mind, let's get our hands dirty. You want to do a series of ~1-5 day mini-projects, for maybe 2-4 weeks. The goal right now is to learn the craft, not to produce groundbreaking research.<\/p><p>Focus on practicing exploration and understanding and gaining the fast/medium skills, leave aside ideation and distillation for now. If you produce something cool and want to write it up, great! But that’s a nice-to-have, not a priority.<\/p><p>Once you finish a mini-project, remember to do a post-mortem. Spend at least an hour analyzing: what did you do? What did you try? What worked? What didn't? What mistakes did you make? What would you do differently if doing this again? And how can you integrate this into your research strategy going forwards?<\/p><h3 id=\"Choose_A_Project\" data-internal-id=\"Choose_A_Project\">Choose A Project<\/h3><p>Some suggested starter projects<\/p><ul><li><strong>Replicate and Extend a Paper:<\/strong>&#160;A classic for a reason. Replicate a key result, then extend it. Suggestions:<ul><li><a href=\"https://arxiv.org/abs/2406.11717\">Refusal is mediated by a single direction<\/a><ul><li>Extending papers can vary a lot in difficulty. For example, applying the method to study refusal on a new model is easy as you can reuse the same data, while applying it to a new concept is harder.<\/li><li>Skills: practicing activation patching and steering vectors.<\/li><\/ul><\/li><li><a href=\"http://thought-anchors.com\">Thought Anchors<\/a>: apply these reasoning model interpretability methods to new types of prompts, or explore some prompts using the linked interface, or see if you can improve on the methods/invent your own.<ul><li>Skills: reasoning model interpretability, using LLM APIs, and working with modern models<\/li><\/ul><\/li><li>Replicate the truth probes in <a href=\"https://arxiv.org/abs/2310.06824\">Geometry of Truth<\/a>&#160;on a more modern model and try applying them in more interesting settings. How well do they generalise? Can you break them? If so, can you fix this?<ul><li>Skills: probing, supervised learning, dataset creation<\/li><\/ul><\/li><\/ul><\/li><li><strong>Play around with something interesting:<\/strong><ul><li>Use <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia's attribution graphs<\/a>&#160;to form a hypothesis about Gemma 2B, then use other methods (e.g. prompting) to verify it.<ul><li>Skills: Attribution graphs, scientific mindset, prompting<\/li><\/ul><\/li><li>Play with <a href=\"https://huggingface.co/collections/bcywinski/gemma-2-9b-it-taboo-6826efbb186dfce0616dd174\">Bartosz Cywiński's taboo models<\/a>&#160;that have a secret word programmed in and test as many methods as you can to find it.<ul><li>If you’re feeling ambitious: train your own models with a more complex secret, and try to interpret those.<\/li><li>Skills: Logit lens, SAEs, black box methods<\/li><\/ul><\/li><li>Explore <a href=\"https://github.com/clarifying-EM/model-organisms-for-EM\">the models<\/a>&#160;from the <a href=\"https://www.emergent-misalignment.com/\">emergent<\/a>&#160;<a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">misalignment<\/a>&#160;<a href=\"https://openai.com/index/emergent-misalignment/\">papers<\/a>.<ul><li>Skills: steering vectors, SAEs, maybe fine-tuning<\/li><\/ul><\/li><li>Pick some prompts from <a href=\"https://arxiv.org/abs/2503.08679\">Chain-of-Thought Reasoning In The Wild Is Not Always Faithful<\/a>&#160;and try to gain a deeper understanding of what’s happening<ul><li>Skills: Open ended exploration, using whichever tools seem appropriate<\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Those cover two kinds of starter projects:<\/p><ul><li><strong>Understanding-heavy<\/strong>, where you take a well-known domain and try to test a hypothesis there (e.g. extending a paper you’ve read closely)<ul><li>Note that you still want to do <i>some<\/i><\/li><\/ul><\/li><li><strong>Exploration-heavy<\/strong>, where you take some phenomena (a technique, a model, a phenomena, etc) play around with it, and try to understand what’s going on.<ul><li>Exploration-heavy projects are often a less familiar style, so make sure to do some of those!<\/li><\/ul><\/li><\/ul><p>Common mistakes:<\/p><ul><li>People often get hung up on finding the “best” project. Sadly, that’s not going to happen. Instead, just do something and see what happens - better ideas and inspiration come with time.<\/li><li>Don't get too attached to your first project. It was probably badly chosen! These are throwaway projects, just move on once you’re not learning as much.<\/li><li>Conversely, don't flit between ideas so much that you never build your \"getting unstuck\" toolkit.<\/li><li>Avoid compute-heavy and/papers (e.g., training cross-layer transcoders) or highly technical papers (e.g., Sparse Feature Circuits).<\/li><\/ul><h3 id=\"Practicing_Exploration\" data-internal-id=\"Practicing_Exploration\">Practicing Exploration<\/h3><p>The idea of exploration as a phase in itself often trips up people new to mech interp. They feel like they always need to have a plan, a clear thing they're doing at any given point, etc. In my experience, you will often spend more than half of a project trying to figure out what the hell is happening and what you think your plan is. This is totally fine!<\/p><p data-internal-id=\"ftnt_ref14\">You don't need a plan. It's okay to be confused. However, this does <i>not <\/i>mean you should just screw around. Your North Star: gain information and surface area<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-noteref\" id=\"fnrefxw1ra5pqnd\"><sup><a href=\"#fnxw1ra5pqnd\">[14]<\/a><\/sup><\/span>&#160;on the problem. Your job is to take actions that maximise information gained per unit time. If you've learned nothing in 2 hours, pivot to another approach. If 2-3 approaches were dead ends, it’s fine to just pick another problem.<\/p><p>I have <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">several research walkthroughs on my YouTube channel<\/a>&#160;that I think demonstrates the mindset of exploration. What I think is an appropriate speed to be moving. E.g. I think you should aim to make a new plot every few minutes (or faster!) if experiments don't take too long to run.<\/p><p>A common difficulty is feeling “stuck” and not knowing what to do. IMO, this is largely a skill issue. Here's my recommended toolkit when this happens:<\/p><ul><li>Use \"gain surface area\" techniques, things that can surface new ideas and connections and just give you raw data to work with: look at the model's output/chain-of-thought, change the prompt, probe for a concept, look at an SAE/attribution graph, read examples from your dataset, try logit lens or steering, etc.<\/li><li>Set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5-minute timer<\/a>&#160;and brainstorm things you're curious about or directions to try.<\/li><li>If you’re confused/curious about something, set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5 minute timer<\/a>&#160;and brainstorm what could be happening.<\/li><\/ul><p>Other advice:<\/p><ul><li>Before any &gt;30 minute experiment, stop and brainstorm alternatives. Is this <i>really<\/i>&#160;the fastest way to gain information?<\/li><li>It's totally fine to pause for half a day to go learn some key background knowledge.<\/li><li>Get in the habit of keeping a research log of your findings and a \"highlights\" doc for the really cool stuff.<ul><li>If applicable, it can be cool to have your research log be a slack/discord channel<\/li><\/ul><\/li><li>Remember: when exploring and thinking through how to explain mysterious phenomena, most of your probability mass should be on \"something I haven't thought of yet.\"<\/li><li>Practice following your curiosity, but be aware that it’ll often lead you astray at first. When it does, pay attention! What can you learn from this?<\/li><\/ul><h3 id=\"Practicing_Understanding\" data-internal-id=\"Practicing_Understanding\">Practicing Understanding<\/h3><p>If exploration goes well, you'll start to form hunches about the problem. E.g. thinking that you are successfully (linearly) probing for some concept. Or that you found a direction that mediates refusal. Or that days of the week are represented as a circle in a 2D subspace.<\/p><p>Once you have this, you want to go to figure out if it's actually true. Be warned, the feeling of “being really convinced that it's true” is very different from actually being true. Part of being a good researcher is being good enough at testing and falsifying your pet hypotheses that, when you fail to falsify one, there’s a good chance that it's true. But you're probably not there yet.<\/p><p>Note: While I find it helpful to think of these as discrete stages, often you'll be flitting back and forth. A great way to explore is coming up with guesses and micro-hypotheses about what's going on, running a quick experiment to test them, and integrating the results into your understanding of the problem, going back to the drawing board.<\/p><p>Your North Star: convince yourself a hypothesis is true or false. The key mindset is skepticism. Advice:<\/p><ul><li>Before testing a hypothesis, set a five-minute timer and brainstorm, \"What are the ways this could be false?\"<\/li><li>Alternatively, write out the best possible case for your hypothesis and see where the argument feels weak.<ul><li>Try using an LLM with an anti-sycophancy prompt (\"My friend wrote this and wants brutal feedback...\") to red-team your arguments - it probably won’t work, but might be helpful<\/li><\/ul><\/li><li>Or set a 5 minute timer and brainstorm alternative explanations for your observations<\/li><\/ul><p>You then want to convert these flaws and alternative hypotheses into concrete experiments. <strong>Experiment design is a deep skill<\/strong>. Honestly, I'm not sure how to teach it other than through experience. But one recommendation is to pay close attention to the experiments in papers you admire and analyze what made them so clever and effective. I also recommend that, every time you feel like you’ve (approximately) proven or falsified a hypothesis, adding them to a running doc of “things I believe to be true” with hypotheses, experiments, and results.<\/p><h3 id=\"Using_LLMs_for_Research_Code\" data-internal-id=\"Using_LLMs_for_Research_Code\">Using LLMs for Research Code<\/h3><p>In my opinion, coding is one of the domains where LLMs are most obviously useful. It was very striking to me how much better my math scholars were six months ago than 12 months ago, and I think a good chunk of this is attributable by them having much better LLMs to use. If you are not using LLMs as a core part of your coding workflow, I think you're making a mistake.<\/p><ul><li><strong>Use<\/strong><a href=\"http://cursor.com/\"><strong>&#160;Cursor<\/strong><\/a><strong>:<\/strong>&#160;It's VS Code with fantastic AI integration. Make sure to add the docs for libraries with @&#160;so the AI has context. The $20/month plan is worth it, if possible, and there’s a <a href=\"https://cursor.com/students\">free student version<\/a>.<ul><li>Claude Code is tempting but bad for learning and iteration. I’d use it for throwaway things and first drafts - if the draft has a bunch of bugs, go read the code yourself/throw it away and start again. Cursor facilitates reading the AI’s code better than Claude code does IMO<\/li><\/ul><\/li><li><strong>A caveat:<\/strong>&#160;If learning a new library (like in ARENA), first try writing things yourself. Use the LLM when stuck, not to replace the learning process.<\/li><li>Later on, when thinking about writing up results, if key experiments were mostly vibe-coded, I recommend re-implementing them by hand to make sure no dumb LLM bugs slipped in.<\/li><\/ul><h2 id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\" data-internal-id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\">Interlude: What’s New In Mechanistic Interpretability?<\/h2><p><i>Feel free to skip to the “<\/i><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p>Things move fast in mechanistic interpretability. Newcomers to the field who've kept up from afar are often pretty out of date. Here's what I think you need to know, again, filtered through my own opinions and biases.<\/p><h3 id=\"Avoiding_Fads\" data-internal-id=\"Avoiding_Fads\">Avoiding Fads<\/h3><p>This interlude is particularly important because <strong>the field often has fads<\/strong>: lines of research that are very popular for a year or so, make some progress and find many limitations, and then the field moves on. But if you’re new, and catching up on the literature, you might not realise. I often see people new to the field working on older things, that I don’t think are too productive to work on any more. Historical fads include:<\/p><ul><li>Interpreting toy models trained on algorithmic tasks (e.g. my <a href=\"https://arxiv.org/abs/2301.05217\">grokking work<\/a>)<ul><li>I no longer recommend working on this, as I think we basically know that “sometimes models trained on algorithmic tasks are interpretable”, and they’re sufficiently artificial and divorced from real models that I am pessimistic about deeper and more specific insights generalising<\/li><\/ul><\/li><li>Circuit analysis via causal interventions on model components (e.g. the <a href=\"https://arxiv.org/abs/2211.00593\">IOI paper<\/a>)<ul><li>This is slightly more complicated. I think that's worth learning about, and techniques like activation and attribution patching are genuinely useful.<\/li><li>But the core problem is that once you got a sparse subgraph of a model responsible for a task, there wasn't really a “what next?”. This didn't tend to result in deeper insight because the nodes (eg layers or maybe attention heads) weren't monosemantic, and it was often more complicated than naive stories suggested but we didn’t have the tools to dig deeper.<\/li><li>It was pretty cool to see that this was possible at all, but there have been more than enough works in this area that the bar for a novel contribution is now much higher.<\/li><li>Simply identifying a circuit is no longer enough; you need to use that circuit to reveal a deeper, non-obvious property of the model. I recommend exploring <a href=\"https://www.neuronpedia.org/graph/info\">attribution-graph style approaches<\/a><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">We're at the tail end of a fad of incremental <a href=\"https://transformer-circuits.pub/2023/monosemantic-features\">sparse autoencoder research<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-noteref\" id=\"fnreftq4gws0zq69\"><sup><a href=\"#fntq4gws0zq69\">[15]<\/a><\/sup><\/span>&#160;(i.e. focusing on simple uses and refinements of the basic technique)<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Calling this one a fad is probably more controversial (if only because it's more recent).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">The <i>specific<\/i>&#160;thing I am critiquing is the spate of papers, including ones I was involved in, that are about incremental improvements to the sparse autoencoder architecture, or initial demonstrations that you can apply SAEs to do things, or picking some downstream task and seeing what SAEs do on it.<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I think this made some sense when it seemed like SAEs could be a total gamechanger for the field, and where we were learning things from each new such paper. I think this moment has passed; I do not think they were a gamechanger in the way that I hoped they might be. See <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">more of my thoughts here<\/a>.<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">I am <i>not<\/i>&#160;discouraging work on the following:<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Attribution graph-based circuit analysis, which I don't think has played out yet - see <a href=\"https://www.neuronpedia.org/graph/info\">a recent overview of that sub-field I co-wrote<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Trying meaningfully different approaches to dictionary learning (eg <a href=\"https://arxiv.org/abs/2506.20790\">SPD<\/a>&#160;or <a href=\"https://arxiv.org/abs/2505.17769\">ITDA<\/a>), or things targeted to fix conceptual limitations of current techniques (eg <a href=\"https://arxiv.org/abs/2503.17547\">Matryoshka<\/a>).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Using SAEs as a tool, whether as part of a broader project investigating weird phenomena in model biology, or as a baseline/approach on some downstream task. The key is that the project’s motivation should <i>not <\/i>just be “what if we used SAEs for X?” unless there’s a good argument<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I particularly recommend them for tasks where you don’t know exactly what you’re looking for, e.g. trying to explore some mysterious phenomena<\/p><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Note that I am putting this after stage 2 because I think that <strong>for initial throwaway projects you should <\/strong><i><strong>not<\/strong><\/i><strong>&#160;be stressing about novelty and avoiding fads<\/strong>&#160;- your goal is just to learn. But as we move into stage 3 you should start to be a bit more mindful about choosing more exciting/impactful projects where possible.<\/p><p>Also, take these as nudges and recommendations, not as instructions. If there's a direction you believe in that fits the things I'm critiquing, maybe I'm just wrong, maybe your thing is an exception, go wild, see what happens.<\/p><h3 id=\"What_s_New_In_Mech_Interp_\" data-internal-id=\"What_s_New_In_Mech_Interp_\">What’s New In Mech Interp?<\/h3><p>OK, so those are my hot takes on what not to do. What should you do? I think that some really cool new opportunities have opened up in mech interp over the last year, and newcomers may not have come across these. Here are some of the key themes in my favorite papers over the last year, that I’d love to see readers build on:<\/p><ol><li><strong>Downstream tasks<\/strong>. If our North Star is to be able to do useful things with interpretability, a great way to test our techniques is on real-world tasks, to see if they beat baselines or enable us to do a new thing<ol><li>Even if we can't directly test on the tasks we most care about, if models can let us do new things in <i>some<\/i>&#160;domains, that's reasonable evidence that we're learning something real in a way that is hard to fake.<\/li><li>One of my favourite examples is <strong>auditing games<\/strong>: <a href=\"https://arxiv.org/abs/2503.10965\">Marks et al<\/a>. is a fantastic paper that trained a model to have a hidden goal and then had different teams compete to find this goal, some with interpretability techniques, some without. Hypothesis: If interpretability does something real, then interp teams should win.<ol><li>This study itself was a bit ambiguous (the mech interp teams were 1st and 3rd, a black box team was 2nd), but I think this is a fantastic way to test interpretability techniques, and has a ton of potential for follow-on work evaluating interpretability in different kinds of games.<\/li><li>The general idea is just that there is a model with some known hidden property and different techniques are used to see which can elicit it. Things like having different teams, allowed to use different techniques, etc. is just one implementation.<\/li><\/ol><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref16\"><strong>Model organisms<\/strong>: The auditing games paper was made possible by the fact that they were able to make a model with a hidden goal<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-noteref\" id=\"fnrefcdmsagzbqkp\"><sup><a href=\"#fncdmsagzbqkp\">[16]<\/a><\/sup><\/span>, a model organism to study. In general, we’re collecting techniques like <a href=\"https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/\">synthetic document fine-tuning<\/a>&#160;to make really interesting model organisms.<\/p><ol><li><p data-internal-id=\"ftnt_ref16\">This kind of thing has a lot of potential! If we want to make a lie detector, a core challenge is that we don’t know how to test if it works or not. But if we can insert beliefs or deceptive behaviours into a model, many more projects become possible<\/p><\/li><li><p data-internal-id=\"ftnt_ref16\">A great intro project is playing around with open source model organisms, e.g. from <a href=\"https://arxiv.org/abs/2505.14352\">Cywinski et al<\/a><\/p><\/li><\/ol><\/li><li><strong>Practice on the real AGI Safety problems<\/strong>: Historically, interpretability could only practice on very dull toy problems like <a href=\"https://arxiv.org/abs/2301.05217\">modular addition<\/a>. But we now have models that exhibit complex behaviors that seem genuinely relevant to safety concerns, and we can just study them directly, making it far easier to make real progress.<ol><li>E.g. <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">Rajamanoharan et al<\/a>&#160;debunking assumed self-preservation, and <a href=\"https://www.apolloresearch.ai/research/deception-probes\">Goldowsky-Dill et al<\/a>&#160;probing for deception<\/li><li>Weird behaviours: models can <a href=\"https://www.apolloresearch.ai/research/deception-probes\">insider trade then lie about it<\/a>, <a href=\"https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations\">tell when they’re being evaluated<\/a>&#160;(and act differently), <a href=\"https://arxiv.org/abs/2412.14093\">fake alignment<\/a>, <a href=\"https://metr.org/blog/2025-06-05-recent-reward-hacking/\">reward hack<\/a>, and more.<\/li><\/ol><\/li><li><strong>Real-World Uses of Interpretability<\/strong>: Model interpretability-based techniques are starting to have genuine uses in frontier language models!<ol><li><a href=\"https://arxiv.org/abs/1610.01644\">Linear probes<\/a>, one of the simplest possible techniques, are a highly competitive way to <a href=\"https://alignment.anthropic.com/2025/cheap-monitors/\">cheaply monitor systems<\/a>&#160;for things like users trying to make bioweapons.<\/li><li>I find it incredibly cool that interpretability can actually be useful, and kind of embarrassing that only a decade-old technique seems very helpful. Someone should do something about that. Maybe that someone could be you!<\/li><li>This needs a very different kind of research: careful evaluation, comparison to strong baselines, and refinement of methods<\/li><\/ol><\/li><li><strong>Attribution graph-based circuit analysis<\/strong>. The core problem with trying to analyze circuits in terms of things like a model's attention heads and layers is that often these things don't actually have a clear meaning. <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\">Attribution graphs<\/a>&#160;use techniques like <a href=\"https://arxiv.org/abs/2406.11944\">transcoders<\/a>, popularized in <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/biology.html\">Anthropic's model biology<\/a>&#160;work, to approximate models with a computational graph with meaningful nodes.<ol><li><p data-internal-id=\"ftnt_ref17\">See this <a href=\"https://www.neuronpedia.org/graph/info\">cross-org blog post<\/a>&#160;for the ongoing follow-on work across the community, and an open problems list I co-wrote!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-noteref\" id=\"fnrefp0f0m03b55r\"><sup><a href=\"#fnp0f0m03b55r\">[17]<\/a><\/sup><\/span><\/p><\/li><li>You can make and analyse your own attribution graphs on <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia<\/a><\/li><\/ol><\/li><li><strong>Understanding model failures<\/strong>: Models often do weird things. If we were any good at interpretability, we should be able to understand these. Recently, we’ve seen signs of life!<ol><li><a href=\"https://transluce.org/observability-interface\">Meng et al<\/a>&#160;on why some models think 9.8 &lt; 9.11<\/li><li><p data-internal-id=\"ftnt_ref18\">A line of work studying <a href=\"https://www.emergent-misalignment.com/\">emergent misalignment<\/a>&#160;- why training models on narrowly evil tasks like writing insecure code turns them into Nazis - has found some insights. <a href=\"https://arxiv.org/abs/2506.19823\">Wang et al<\/a>&#160;found this was driven by sparse autoencoder latents<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-noteref\" id=\"fnrefg12d8d1lqu\"><sup><a href=\"#fng12d8d1lqu\">[18]<\/a><\/sup><\/span>&#160;associated with movie villains, and in <a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">Turner et al<\/a>&#160;we found that the model <i>could <\/i>have learned the narrow solution, but this was in some sense less “efficient” and “stable”<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref20\"><strong>Automated interpretability<\/strong>: Using LLMs to automate interpretability. We saw signs of life on this from Bills et al and <a href=\"https://arxiv.org/abs/2404.14394\">Shaham et al<\/a>, but LLMs are actually good now! It’s now possible to make basic interpretability agents that can do things like <a href=\"https://alignment.anthropic.com/2025/automated-auditing/\">solve auditing games<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-noteref\" id=\"fnref0td6a2gxwht\"><sup><a href=\"#fn0td6a2gxwht\">[19]<\/a><\/sup><\/span>. And interpretability agents are the worst they’ll ever be<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-noteref\" id=\"fnref5bdglmkdzr\"><sup><a href=\"#fn5bdglmkdzr\">[20]<\/a><\/sup><\/span>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref22\"><strong>Reasoning model interpretability<\/strong>: All current frontier models are reasoning models—models that are trained with reinforcement learning to think<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-noteref\" id=\"fnrefwuxdh4f7kh\"><sup><a href=\"#fnwuxdh4f7kh\">[21]<\/a><\/sup><\/span>&#160;for a while before producing an answer. In my opinion, this requires a major rethinking of many existing interpretability approaches<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-noteref\" id=\"fnref3qxoen8tddk\"><sup><a href=\"#fn3qxoen8tddk\">[22]<\/a><\/sup><\/span>, and calls for exploring new paradigms. IMO this is currently being neglected by the field, but will become a big deal.<\/p><ol><li><p data-internal-id=\"ftnt_ref22\">In <a href=\"http://thought-anchors.com\">Bogdan et al<\/a>, we explored what a possible paradigm could look like. Notably, there are far more interesting and sophisticated black box techniques with reasoning models, like resampling the second half of the chain of thought, or every time the model says a specific kind of sentence, deleting and regenerating that sentence.<\/p><\/li><\/ol><\/li><\/ol><h3 id=\"A_Pragmatic_Vision_for_Mech_Interp\" data-internal-id=\"A_Pragmatic_Vision_for_Mech_Interp\">A Pragmatic Vision for Mech Interp<\/h3><p>Attentive readers may notice that the list above focuses on work to do with understanding the more qualitative high-level properties of models, and not ambitious reverse engineering. This is largely because, in my opinion, the former has gone great, while we have not seen much progress towards the fundamental blockers on the latter.<\/p><p>I used to be very excited about ambitious reverse engineering, but I currently think that the dream of completely reverse engineering a model down to something human understandable seems basically doomed. My interpretation of the research so far is that models have some human understandable high-level structure that drives important actions, and a very long tail of increasingly niche and irrelevant heuristics and biases. For pragmatic purposes, these can be largely ignored, but not if we want things like guarantees, or to claim that we have understood most of a model. I think that trying to understand as much as we can is still a reasonable proxy for getting to the point of being pragmatically useful, but think it’s historically been too great a focus of the field, and many other approaches seem more promising if our ultimate goals are pragmatic.<\/p><p>In some ways, this has actually made me more optimistic about interpretability ultimately being useful for AGI safety! Ambitious reverse engineering would be awesome but was always a long shot. But I think we've seen some real results for pragmatic approaches to mechanistic interpretability, and feel fairly confident we are going to be able to do genuinely useful things that are hard to achieve with other methods.<\/p><h2 id=\"Stage_3__Working_Up_To_Full_Research_Projects\" data-internal-id=\"Stage_3__Working_Up_To_Full_Research_Projects\">Stage 3: Working Up To Full Research Projects<\/h2><p>Once you have a few mini-projects done, you should start being more ambitious. You want to think about gaining the deeper (medium/slow) skills, and exploring ideation and distillation.<\/p><p>However, you should still expect projects to often fail, and want to lean into breadth over depth and avoid getting bogged down in an unsuccessful project you can’t bear to give up on. To resolve this tension, I recommend <strong>working in 1-2 week sprints<\/strong>. At the end of each sprint, reflect and make a deliberate decision: <strong>continue, or pivot?<\/strong>&#160;The default should be to pivot unless the project feels truly promising. It’s great to give up on things, if it means you spend your time even better! But if it’s going great, by all means continue.<\/p><p>This strategy should mean that you eventually end up working on something longer-term when you find something <i>good<\/i>, but don't just get bogged down in the first ambitious idea you tried.<\/p><p>I recommend reviewing the list of skills earlier and just for each one, reflecting for a bit on how on top of it you think you feel and how you could intentionally practice it in your next project. Then after each sprint, before deciding whether to pivot, take an hour or two to do a post-mortem: what did you learn, what progress did you make on different skills, and what would you do differently next time? Your goal is to learn, and you learn much better if you make time to actually process your accumulated data!<\/p><h3 id=\"Key_Research_Mindsets\" data-internal-id=\"Key_Research_Mindsets\">Key Research Mindsets<\/h3><p>One way to decompose your learning is to think about research mindsets: the traits and mindsets a good researcher needs to have, that cut across many of these stages. See <a href=\"https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking\">my blog post on the topic for more<\/a>, but here's a brief view of how I'm currently thinking about it.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\"><strong>Skepticism/Truth-seeking:<\/strong>&#160;The default state of the world is that your research is false, because doing research is hard. Your north star should always be to find <i>true <\/i>insights<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-noteref\" id=\"fnreflm5ixkfuzk\"><sup><a href=\"#fnlm5ixkfuzk\">[23]<\/a><\/sup><\/span><\/p><ol><li><p data-internal-id=\"ftnt_ref23\">It generally doesn't come naturally to people to constantly aggressively think about all the ways their work could be false and make a good faith effort to test it. You can learn to do better than this, but it often takes practice.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">This is crucial in understanding, somewhat important in exploration, and crucial in distillation.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">A common mistake is to grasp at straws to find a “positive” result, thinking that nothing else is worth sharing.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\">In my opinion, negative or inconclusive results that are well-analyzed are much better than a poorly supported positive result. I’ll often think well of someone willing to release nuanced negative results, and poorly of someone who pretends their results are better than they are.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Prioritization:<\/strong>&#160;Your time is scarce. Research involves making a bunch of decisions that are essentially searching through a high-dimensional space. The difference between a great and a mediocre researcher is being able to make these decisions well.<ol><li>If you have a good mentor, you can lean on them for this at first, but you will need to learn how to do this yourself eventually.<\/li><li>This is absolutely crucial in exploration and ideation, but fairly important throughout.<\/li><li>A good way to learn this one is to reflect on decisions you've made after the fact, eg in a sprint post-mortem, and think about how you could have made them better, and what generalisable lessons to take to the future<\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\"><strong>Productivity<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-noteref\" id=\"fnrefidab8074tka\"><sup><a href=\"#fnidab8074tka\">[24]<\/a><\/sup><\/span><strong>:<\/strong>&#160;The best researchers I've worked with get more than twice as much done as the merely good ones. Part of this is good research taste and making good prioritization decisions, but part of this is just being good at getting shit done.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Now, this doesn't necessarily mean pushing yourself until the point of burnout by working really long hours. Or cutting corners and being sloppy. This is about productivity integrated over the long term.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">For example, sometimes the most productive thing to do is to hold off on starting work, set a 5 minute timer, brainstorm possible things to do next, and then pick the best idea<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">This takes many forms, and the highest priority for you:<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Know when to write good code without bugs, to avoid wasting time debugging later, and when to write a hacky thing that just works.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know the right keyboard shortcuts to move fast when coding.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know when to ask for help and have people who can help you get unblocked where appropriate.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be good at managing your time and tasks so that once you've decided what the highest priority thing to work on is, you in fact go and work on it.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be able to make time to achieve deep focus on the key problems.<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">Exercise: Occasionally <strong>audit your time<\/strong>. Use a tool like <a href=\"http://toggl.com\">Toggl<\/a>&#160;for a day or two to log what you're doing, then reflect: where did time go? What was inefficient? How could I do this 10% faster next time?<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">The goal isn't to feel guilty, but to spot opportunities for improvement, like making a utility function for a tedious task.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Knowing the literature<\/strong>: At this point, there’s a lot of accumulated wisdom (and a lot of BS) in prior papers, in mech interp and beyond.<ol><li>This cuts across all stages:<ol><li>In ideation, you don’t want to accidentally reinvent the wheel. And often great ideas are inspired by prior work<\/li><li>In exploration, you want to be able to spot connections, borrow interesting techniques, etc<\/li><li>In understanding, you want to know the right standards of proof to check for, the best techniques to use, alternative hypotheses (that may have been raised in other works), etc<\/li><li><p data-internal-id=\"ftnt_ref25\">In distillation, when writing a paper you’re expected to be able to contextualise it relative to existing work (i.e. write a related work section<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-noteref\" id=\"fnrefwpekmwudkpd\"><sup><a href=\"#fnwpekmwudkpd\">[25]<\/a><\/sup><\/span>) which is important for other researchers knowing whether to care. And if you don’t know the standard methods of proof, key baselines everyone will ask about, key gotchas to check for etc, no one will believe your work.<\/p><\/li><\/ol><\/li><li>LLMs are an incredibly useful tool here. GPT-5 thinking or Claude 4 with web search are both pretty useful tools here, as are the slower but more comprehensive deep research tools (Note that Google's is available for free, as of the time of writing)<ol><li>I recommend using these regularly and creatively throughout a project.<\/li><li>You don't necessarily need to go and read the works that get surfaced, but even just having LLM summaries can get you more awareness of what's out there, and over time you'll build this into deeper knowledge.<\/li><\/ol><\/li><li>Of course, when there <i>does<\/i>&#160;seem to be a very relevant paper to your work, you should go do a deep dive and read it properly, not just relying on LLM summaries.<\/li><li>Don’t stress - deep knowledge of the literature takes time to build. But you want to ensure you’re on an upwards gradient here, rather than assuming the broader literature is useless<\/li><li><p data-internal-id=\"ftnt_ref26\">On the flip side, many papers <i>are <\/i>highly misleading/outright false, so please don’t just critically believe them!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-noteref\" id=\"fnref1bau7vsh9tk\"><sup><a href=\"#fn1bau7vsh9tk\">[26]<\/a><\/sup><\/span><\/p><\/li><\/ol><\/li><\/ol><p>Okay, so how does this all tie back to the stages of research? Now you're going to be thinking about all four. We'll start by talking about how to deepen your existing skills with exploration and understanding, and then we'll talk about what practicing ideation and actually writing up your work should look like.<\/p><h3 id=\"Deepening_Your_Skills\" data-internal-id=\"Deepening_Your_Skills\">Deepening Your Skills<\/h3><p>You’ll still be exploring and understanding, but with a greater focus on rigor and the slower skills. In addition to the thoughts when discussing mindsets above, here’s some more specific advice<\/p><ul><li><strong>Deeper Exploration<\/strong>&#160;is about internalizing the mindset of maximising productivity, which here means maximising information gain per unit time. Always ask, \"Am I learning something?\"<ul><li><i>Avoid Rabbit Holes:<\/i>&#160;A common mistake is finding one random anomaly and zooming in on it. Knowing when to pivot is crucial. Set a timer every hour or two to zoom out and ask if you’re making progress.<ul><li>I recommend any time you notice yourself feeling a bit stuck or distracted or off track, setting a five minute timer and thinking about what could I be doing next, what should I be doing next, and am I doing the most important thing?<\/li><\/ul><\/li><li><i>Avoid Spreading Yourself Too Thin:<\/i>&#160;Doing lots of things superficially means none of them will be interesting.<\/li><li>If you have spent more than five hours without learning something new, you should probably try a different approach<ul><li>And if you have spent more than two days without learning something new, you should seriously consider pivoting and doing something else.<\/li><\/ul><\/li><li>To practice prioritization, be intentional about your decisions: write down <i>why<\/i>&#160;you think an experiment is the right call, and later reflect on whether you were right. This makes your intuitions explicit and easier to update.<\/li><\/ul><\/li><li><strong>Deeper Understanding<\/strong>&#160;is about practicing skepticism and building a bulletproof case. Red-team your results relentlessly.<ul><li>Some experiments are much more impactful and informative than others! Don't just do the first experiment that pops into your head. Think about the key ways the hypothesis <i>could <\/i>be false, and how you could test that. Or about whether a skeptic could explain away a positive experimental results<ul><li>A useful exercise is imagining you're talking to a really obnoxious skeptic who keeps complaining that they don't believe you and coming up with arguments for why your thing is wrong. What could you do such that they don't have a leg to stand on?<\/li><\/ul><\/li><li>Of course, there's also an element of prioritization. Sometimes a shallow case that could be wrong is the right thing to aim for, if you’re working on an unimportant side claim/something that seems super plausible on priors, at which point you should just move on and do something else more interesting.<\/li><li>Exercise: To practice spotting subtle illusions, try red-teaming papers you read, thinking about potential flaws, and ideally run the experiments yourself.<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Doing_Good_Science\">Doing Good Science<\/p><ul><li><strong>Avoid cherry-picking<\/strong>: Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling.<ul><li>When you write up work, always include some randomly selected examples, especially if you present extensive qualitative analysis of specific things. It's fine to put this in the appendix if space is scarce, but it should be there.<\/li><\/ul><\/li><li><strong>Use baselines<\/strong>: A common mistake is for people to try to show a technique works by demonstrating it gets 'decent' results, rather than showing it achieves better results than plausible alternatives that people might have used or are standard in the field. If you want people to e.g. use your cool steering vector results you need to show it beats changing the system prompt.<\/li><li><strong>Don’t sandbag your baselines<\/strong>: Similarly, it's easy to put in much more effort finding good hyperparameters for your technique than for your baselines. Try to make sure you're achieving comparable results with your baselines that prior work in the field has.<\/li><li><strong>Do ablations on your fancy method<\/strong>: It's easy for people to have a fancy method with lots of moving parts, when many actually are unnecessary. You should always try removing one part and see if the method breaks. Do this for each part.<ul><li>For example, the <a href=\"https://arxiv.org/abs/2403.03218v1\">original unlearning method<\/a>&#160;in the <a href=\"https://arxiv.org/abs/2403.03218\">RMU paper<\/a>&#160;claimed it was based on finding a meaningful steering vector, until follow-up work found that it was just about adding a vector with really high norm that broke the model, and a random vector performed just as well.<\/li><\/ul><\/li><li><strong>(Informally) pre-register claims<\/strong>: It's important to clearly track which experimental results were obtained before versus after you formulated your claim. Post-hoc analysis (interpreting results after they're seen) is inherently less impressive than predictions confirmed by pre-specified experiments<\/li><li><strong>Be reproducible<\/strong>: Where practical, share your code, data and models.<ul><li>If you have time, make sure that it runs on a fresh machine and include a helpful readme that links to key model weights and datasets.<\/li><li><p data-internal-id=\"ftnt_ref27\">This both means others can check if your work is true and makes it more likely people will believe and build on your work<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-noteref\" id=\"fnrefadytzr5d7y\"><sup><a href=\"#fnadytzr5d7y\">[27]<\/a><\/sup><\/span>&#160;because they can see replications that are more likely to exist and because it's now low friction.<\/p><\/li><\/ul><\/li><li><strong>Simplicity:<\/strong>&#160;Bias towards trying the simple, obvious methods first. Fancy techniques can be a trap. Good research is pragmatic, not about showing off.<ul><li>If you’re designing a fancy technique/experiment, each new detail is one more thing that can break<\/li><li>If trying to explain something mysterious, novice researchers often neglect simple, dumb hypotheses like “maybe MLP0 is incredibly important on <i>every <\/i>input, and there’s nothing special going on with my prompt”<\/li><\/ul><\/li><li><strong>Be qualitative <\/strong><i><strong>and <\/strong><\/i><strong>quantitative<\/strong>: One of the major drivers of progress of modern machine learning is being quantitative, having benchmarks and showing that a technique increases numbers on them. One of the key drivers of progress in mech interp is an openness to qualitative research: summary statistics lose a ton of information. What can we learn by actually looking deeply into what's happening?<ul><li>In my opinion, the best research tries to get the best of both worlds. It tries to understand what's happening via qualitative analysis and then validates it with more quantitative methods. If your paper only does one, it’s probably missing out<\/li><\/ul><\/li><li><strong>Read your data<\/strong>: A fantastic use of time, especially during the exploration phase, is just actually reading the data you're working with, or model chains of thought and responses.<ul><li>Often, the quality of the data is a crucial driver of the results of your experiments. Often, it is quite bad.<\/li><li>Sometimes most of the work of a project is in noticing flaws in your data and making a better data set. Time figuring this out is extremely well spent.<\/li><li>Ditto, include random examples of the data in an appendix for readers to do spot checks of their own.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref28\"><strong>Don’t reinvent the wheel<\/strong>: &#160;A common mistake in mech interp is doing something that's already been done<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-noteref\" id=\"fnrefva7mhfkrhm\"><sup><a href=\"#fnva7mhfkrhm\">[28]<\/a><\/sup><\/span>. We have LLM-powered literature reviews now. You have way less of an excuse. Check first!<\/p><\/li><li><strong>Excitement is evidence of bullshit<\/strong>: Generally, most true results are not exciting, but a fair amount of false results are. So from a Bayesian perspective, if a result is exciting and cool, it’s even more likely to be false than normal!<ul><li>Resist the impulse to get really excited! The correct attitude to exciting results is deep skepticism until you have tried really hard to falsify it and run out of ideas.<\/li><\/ul><\/li><\/ul><h3 id=\"Practicing_Ideation\" data-internal-id=\"Practicing_Ideation\">Practicing Ideation<\/h3><p>Okay, so you want to actually come up with good research ideas to work on. What does this look like? I recommend breaking this down into <strong>generating ideas<\/strong>&#160;and then <strong>evaluating <\/strong>them to find the best ones.<\/p><p>To generate ideas, I'd often start with just taking a blank doc, blocking out at least an hour, and then just writing down as many ideas as you can come up with. Aim for quantity over quality. Go for at least 20.<\/p><p>There are other things you can do to help with generation:<\/p><ul><li>Throughout your previous sprints, every time you had an idle curiosity or noticed something weird, write it down in one massive long-running doc.<\/li><li>Likewise, when reading papers, note down confusions, curiosities, obviousnesses to do.<\/li><\/ul><p>Okay, so now you have a big list. What does finding the best ones look like?<\/p><ul><li>Ideally, if you have a mentor or at least collaborators, you can just ask them to rate them.<ul><li>If you do this, rate them yourself privately out of 10 before you look at their responses. Compare them and every time you have substantially different numbers, talk to the mentor and try to figure out why your intuitions disagree. This is a great source of supervised data for research taste.<\/li><\/ul><\/li><li>Even if you don’t have a mentor, I think that just going through, rating each idea yourself based on gut feel and sorting is as good a way to prune down a long list as any<\/li><li>For the top few, I recommend trying to answer a few questions about them.<ul><li>What would success look like here?<\/li><li>How surprised would I be if I did this for a month and nothing interesting had happened?<\/li><li>What skills does this require? Do I have them/could I easily gain them?<\/li><li>What models, data, computational resources, etc. does this require?<\/li><li>How does this compare to what the most relevant prior work did? Can I check for prior work and see if anything relevant comes up?<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Research_Taste_Exercises\">Research Taste Exercises<\/p><p>Gaining research taste is slow because the feedback loops are long. You can accelerate it with exercises that give you faster, proxy feedback. (Credit to <a href=\"https://colah.github.io/notes/taste/\">Chris Olah for inspiration here<\/a>)<\/p><ul><li>If you have a mentor, query their taste for fast data and try to imitate it. Concretely:<ul><li>Before each meeting, write a list of questions, then try to write up predictions for what the mentor will say, then actually ask the mentor, see what happens, and compare. If there are discrepancies, chat to the mentor and try to understand why.<\/li><li>Likewise, if the mentor makes a suggestion or asks a question you didn't expect, try to ask questions about where the thought came from.<\/li><li><p data-internal-id=\"ftnt_ref29\">Regularly paraphrase back to the mentor in your own words what you think they're saying, and then ask them to correct anything you're wrong about<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-noteref\" id=\"fnreftt0owz8koks\"><sup><a href=\"#fntt0owz8koks\">[29]<\/a><\/sup><\/span><\/p><\/li><\/ul><\/li><li><strong>Learning from papers as \"offline data\":<\/strong>&#160;When you read a paper, don't just passively consume it. Read the introduction, then stop. Try to predict what methods they used and what their key results will be. Then, continue reading and see how your predictions compare. Analyze why the authors made different choices. This trains your intuition on a much larger and faster dataset than your own research.<\/li><\/ul><p>It’s also worth dwelling on what research taste actually is. <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">See my post<\/a>&#160;for more, but I break it down as follows:<\/p><ol><li><strong>Intuition (System 1):<\/strong>&#160;This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.<\/li><li><strong>Conceptual Framework (System 2)<\/strong>: This is deep domain knowledge and understanding of underlying principles.<\/li><li><strong>Strategic Big Picture<\/strong>: Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?<\/li><\/ol><h3 id=\"Write_up_your_work_\" data-internal-id=\"Write_up_your_work_\">Write up your work!<\/h3><p>At this stage, you should be thinking seriously about how to write up your work. Often, writing up work is the first time you really understand what a project has been about, or you identify key limitations, or experiments you forgot to do. You should check out <a href=\"https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers\">my blog post on writing ML papers<\/a>&#160;for much more detailed thoughts (which also apply to high-effort blog posts!) but I'll try to summarize them below.<\/p><p data-internal-id=\"Why_aim_for_public_output_\">Why aim for public output?<\/p><p data-internal-id=\"ftnt_ref30\">If producing something public is intimidating, for now, you can start by just writing up a private Google Doc and maybe share it with some friends or collaborators. But I heavily encourage people to aim for public output where they can. Generally, your research will not matter if no one reads it. The goal of research is to contribute to the sum of human<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-noteref\" id=\"fnrefe3252d8idmr\"><sup><a href=\"#fne3252d8idmr\">[30]<\/a><\/sup><\/span>&#160;knowledge. And if no one understands what you did, then it doesn't matter.<\/p><p>Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, <strong>public research output is your best credential<\/strong>. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc. It doesn’t matter if you wrote it in a prestigious PhD program or as a random independent researcher, if it’s good enough then people care.<\/p><p>There are a few options for what this can look like:<\/p><ul><li>A blog post (e.g. on a personal blog or LessWrong) - the simplest and least formal kind<\/li><li><p data-internal-id=\"ftnt_ref31\">An Arxiv paper - much more legible than a blog post, and honestly not much extra effort if you have a high-quality blog post<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-noteref\" id=\"fnref8354hd0flji\"><sup><a href=\"#fn8354hd0flji\">[31]<\/a><\/sup><\/span><\/p><\/li><li><p data-internal-id=\"ftnt_ref32\">A workshop paper<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-noteref\" id=\"fnref9oppcf0ftrh\"><sup><a href=\"#fn9oppcf0ftrh\">[32]<\/a><\/sup><\/span>&#160;(i.e. something you submit for peer review to a workshop, typically part of a major ML conference, the bar is much lower than for a conference paper)<\/p><\/li><li><p data-internal-id=\"ftnt_ref34\">A conference paper (the equivalent of top journals in ML, there’s a reasonably high quality bar<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-noteref\" id=\"fnreffmh579omuc6\"><sup><a href=\"#fnfmh579omuc6\">[33]<\/a><\/sup><\/span>, but also a <i>lot <\/i>of noise<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-noteref\" id=\"fnreff09vsa4w37e\"><sup><a href=\"#fnf09vsa4w37e\">[34]<\/a><\/sup><\/span>)<\/p><\/li><\/ul><p>If this all seems overwhelming, starting out with blog posts is fine, but I think people generally overestimate the bar for arxiv or workshop papers - if you think you learned something cool in a project, this is totally worth turning into a paper!<\/p><p data-internal-id=\"How_to_write_stuff_up_\">How to write stuff up?<\/p><p>The core of a paper is the narrative. Readers will not take away more than a few sentences worth of content. Your job is to make sure these are the right handful of sentences and make sure the reader is convinced of them.<\/p><p>You want to distill your paper down into one to three key claims (your contribution), the evidence you provide that the contribution is true, the motivation for why a reader should care about them, and work all of this into a coherent narrative.<\/p><p><strong>Iterate<\/strong>: I'm a big fan of writing things iteratively. You first figure out the contribution and narrative. You then write a condensed summary, the abstract (in a blog post, this should be a TL;DR/executive summary - also very important!). You then write a bullet point outline of the paper: what points you want to cover, what evidence you want to provide, how you intend to build up to that evidence, how you want to structure and order things, etc. If you have mentors or collaborators, the bullet point outline is often the best time to get feedback. Or the narrative formation stage, if you have an engaged mentor. Then write the introduction, and make sure you’re happy with that. Then (or even before the intro) make the figures - figures are incredibly important! Then flesh it out into prose. People spend a <i>lot<\/i>&#160;more time reading the abstract and the intro than the main body, especially when you account for all the people who read the abstract and then stop. So you should spend a lot more time per unit word on those.<\/p><p><strong>LLMs<\/strong>: I think LLMs are a really helpful writing tool. They're super useful for getting feedback, especially if writing in an unfamiliar style like an academic ML paper may be for you. Remember to use anti-sycophanty prompts so you get real feedback. However, it's often quite easy to tell when you're reading LLM written slop. So use them as a tool, but don't just have them write the damn thing for you. But if you e.g. have writer’s block, having an LLM help you brainstorm or produce a first draft for inspiration, and can be very helpful.<\/p><p data-internal-id=\"Common_mistakes\">Common mistakes<\/p><ul><li><strong>The reader does not have context<\/strong>: Your paper will be clear in your head, because you have just spent weeks to months steeped in this research project. The reader has not. You will overestimate how clear things are to the reader, and so you should be massively erring in the other direction and spelling everything out as blatantly as possible.<ul><li><strong>This is an incredibly common mistake<\/strong>&#160;- assume it will happen to you<\/li><li>The main solution is to get feedback from people with enough research context that they can actually engage and who are also willing to give you substantial negative feedback.<ul><li>Notice the feeling of surprise when people are confused by something you thought was clear. Try to understand why they were confused and iterate on fixing it until it's clear.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Writing is not an afterthought<\/strong>: People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute.<\/li><li><strong>Acknowledge limitations<\/strong>: There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing<\/li><li><strong>Good writing is simple<\/strong>: There's a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they feel impressive. I think this is a highly ineffective strategy<\/li><li><strong>Remember to motivate things<\/strong>: It will typically not be obvious to the reader why your paper matters or is interesting. They do not have the context you do. It is your job to convince them, ideally in the abstract or perhaps intro, why they should care about your work, lest they just give up and stop reading.<\/li><\/ul><p data-internal-id=\"h.sk0e3iwce7ck\">&#160;<\/p><h2 id=\"Mentorship__Collaboration_and_Sharing_Your_Work\" data-internal-id=\"Mentorship__Collaboration_and_Sharing_Your_Work\">Mentorship, Collaboration and Sharing Your Work<\/h2><p>A common theme in the above is that it's incredibly useful to have a mentor, or at least collaborators. Here I'll try to unpack that and give advice about how to go about finding one.<\/p><p>Though it's also worth saying that many mentors are not actually great researchers and may have bad research taste or research taste that's not very well suited to mech interp. What you do about this is kind of up to you.<\/p><h3 id=\"So_what_does_a_research_mentor_actually_do_\" data-internal-id=\"So_what_does_a_research_mentor_actually_do_\">So what does a research mentor actually do?<\/h3><p>A good mentor is an incredible accelerator. Dysfunctional as academia is, there is a reason it works under the apprenticeship-like system of PhD students and supervisors. When I started supervising, I was very surprised at how much of a difference a weekly check in could make! Here’s my best attempt to breakdown how a good mentor can add value:<\/p><ul><li><strong>Suggest research ideas<\/strong>&#160;when you're starting out, letting you bypass the hardest skill (ideation) to focus on execution.<\/li><li><strong>Help you prioritize<\/strong>&#160;which experiments to run, lending you their more experienced judgment, so you get more done.<\/li><li><p data-internal-id=\"ftnt_ref35\"><strong>When to pivot<\/strong>: if your research direction isn’t working out, having a mentor to pressure you to pivot can be extremely valuable<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-noteref\" id=\"fnref7ruxx269r2s\"><sup><a href=\"#fn7ruxx269r2s\">[35]<\/a><\/sup><\/span><\/p><\/li><li><strong>Provide supervised data for research taste<\/strong>: For the slow/very-slow skills like coming up with research ideas, and prioritization, a <i>far <\/i>faster way to gain them at first is by learning to mimic your mentor’s.<\/li><li><strong>Act as an interface to the literature<\/strong>: pointing you to the relevant work before you've built up deep knowledge yourself. Flagging standard baselines, standard metrics, relevant techniques, prior work so you don’t reinvent the wheel, etc.<\/li><li><strong>Red-team your results<\/strong>, helping you spot subtle interpretability illusions and flaws in your reasoning that you're too close to see.<\/li><li><strong>Point out skills you're missing<\/strong>&#160;that you didn't even notice were skills. Generally guide your learning and help you prioritize<\/li><li><strong>Walk you through communicating your work<\/strong>, helping you distill your findings and present them clearly to the world.<\/li><li><strong>Motivation/accountability<\/strong>: Many find it extremely helpful to have someone, even if very hands-off, who they present work to, so they feel motivated and accountable (especially if they e.g. want to impress the mentor, want a job, etc. Of course, these also increase stress!)<ul><li>To those prone to analysis paralysis, being able to defer to a mentor on uncertain decisions can be highly valuable<\/li><\/ul><\/li><li><strong>References<\/strong>: Having a mentor who can vouch for your skill is very helpful, especially if they know people who may be hiring you in future.<\/li><\/ul><h3 id=\"Advice_on_finding_a_mentor\" data-internal-id=\"Advice_on_finding_a_mentor\">Advice on finding a mentor<\/h3><p>Here are some suggested ways to get some mentorship while transitioning into the field. I discuss higher commitment ways, like doing a PhD or getting a research job, below.<\/p><p>Note: whatever you do to find a mentor, having evidence that you can do research yourself, that is, public output that demonstrates ability to self-motivate and put in effort, and ideally demonstrates actually interesting research findings, is incredibly helpful and should be a priority.<\/p><p data-internal-id=\"Mentoring_programs\">Mentoring programs<\/p><p>I think mentoring programs like <a href=\"http://matsprogram.org\">MATS<\/a>&#160;are an incredibly useful way into the field, you typically do a full-time, several month program where you write a paper, with weekly check-ins with a more experienced researcher. Your experience will vary wildly depending on mentor quality, but at least for my MATS scholars, often people totally new to mech interp can publish a top conference paper in a few months. See my MATS application doc for a bunch more details.<\/p><p>There’s <strong>a wide range of backgrounds<\/strong>&#160;among people who do them and get value - people totally new to a field, people with 1+ years of interpretability research experience who want to work with a more experienced mentor, young undergrads, mid-career professionals (including a handful of professors), and more.<\/p><p><a href=\"http://matsprogram.org\">MATS 9.0<\/a>&#160;applications are open, due <strong>Oct 2 2025<\/strong>, and <a href=\"http://tinyurl.com/neel-mats-app\">mine<\/a>&#160;close on <strong>Sept 12<\/strong>.<\/p><p>Other programs (which I think are generally lower quality than MATS, but often still worth applying to depending on the mentor)<\/p><ul><li><i>Full-time/In-person:<\/i><a href=\"https://www.matsprogram.org/\">&#160;MATS<\/a>, <a href=\"https://www.pivotal-research.org/fellowship\">Pivotal<\/a>, <a href=\"https://www.lasrlabs.org/\">LASR<\/a>, <a href=\"https://pibbss.ai/fellowship/\">PIBBSS<\/a><\/li><li><i>Part-time/Remote:<\/i><a href=\"https://www.cambridgeaisafety.org/mars\">&#160;<\/a><a href=\"https://sparai.org/\">SPAR<\/a>, <a href=\"https://www.cambridgeaisafety.org/mars\">MARS<\/a><\/li><\/ul><p data-internal-id=\"Cold_emails\">Cold emails<\/p><p>You can also take matters into your own hands and try to convince someone to be your mentor. Reaching out to people, ideally via a warm introduction, but even just via a cold email, can be highly effective. However, I get lots of cold emails and I think many are not very effective, so here's some advice:<\/p><ul><li><strong>Don't just email the most prominent people<\/strong>. A lot of people will just email the most prominent people in the field and ask for mentorship. This is a bad plan! These people are very busy and they also get lots of emails. I just reflexively respond to any email requesting mentorship with “please apply to my MATS cohort”.<ul><li>However, there are lots of less prominent people who can provide a bunch of useful mentorship. These people are much more likely to be excited to get a cold email, to have time to engage, potentially even the spare capacity to properly mentor a project.<\/li><li>I think that many people who've recently joined my team or people who worked on a great paper with me during MATS are able to add a lot of value to people new to the field. And I would recommend reaching out to them!<ul><li>For example, Josh Engels, a new starter on my team, said he would happily receive more cold emails (as of early Sept 2025).<\/li><li>As a general heuristic, email first authors of papers, not fancy last authors.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Start small<\/strong>: Don't email someone you've never interacted with before asking if they want to kind of officially mentor you on some project. That's a big commitment.<ul><li>It's much better to be like, I'd be interested in having a chat about your paper or my work building on your paper.<\/li><li>Or just asking if they're down to have a chat giving you feedback on some project ideas, etc.<\/li><li>And if this goes well, it may organically turn into a more long-term mentoring relationship!<\/li><\/ul><\/li><li><strong>Proof of work<\/strong>: Demonstrate that you are actually interested in this person specifically, not just spamming tons of people.<ul><li>Show that you've engaged with their work, say something intelligent about it, have some questions.<ul><li>In the era of LLMs, this is less of a costly signal that you've actually taken an interest in this person specifically than it used to be, admittedly<\/li><li>But linking to some research you did building on their work I think is still reasonably costly, and very flattering to people.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Prioritize aggressively<\/strong>. Assume the reader will stop reading at any moment, so put your most critical and impressive information first.<\/li><li><strong>Explain who you are<\/strong>: If you're emailing someone who gets more emails than they have capacity to respond to, they're going to be prioritizing. A key input into this is just who you are, what have you done, have you done something interesting that shows promise, do you have relevant credentials, etc. I personally find it very helpful if people just say the most impressive things about them in the first sentence or two.<ul><li>To do this without seeming arrogant, you could try: \"I'm sure you must get many of these emails. So to help you prioritise, here's some key info about me\"<\/li><\/ul><\/li><li>Use <strong>bolding<\/strong>&#160;for key phrases to make your email easily skimmable.<\/li><li><strong>Be concise<\/strong>. One thing I would often appreciate is a short blurb summarizing your request with a link to a longer document for details if I'm interested.<\/li><li><strong>Quick requests<\/strong>: Generally, my flow when reading emails is that I will either immediately respond or never look at it again. I'm a lot more likely to immediately respond if I can do so quickly. If you do want to email a busy person, have a clear, concrete question up front that they might be able to help with.<\/li><\/ul><h3 id=\"Community___collaborators\" data-internal-id=\"Community___collaborators\">Community &amp; collaborators<\/h3><p>Much easier than finding a mentor is finding collaborators, other people to work on the same project with, or just other people also trying to learn more about mech interp, who you can chat with and give each other feedback:<\/p><ul><li><strong>In-Person:<\/strong>&#160;Local AI Safety hubs (London, Bay Area, etc.), University groups, ML conferences (e.g., the<a href=\"http://mechinterpworkshop.com/\">&#160;NeurIPS Mech Interp workshop<\/a>&#160;I co-organize), EAG/EAGx conferences.<ul><li>If you’re a student, see if there’s a lab at your university that has some people interested in interpretability. There may be interested PhD students even if no professor works on it<\/li><\/ul><\/li><li><strong>Online<\/strong>: These are also good places to meet people! I recommend sharing work for feedback, or just asking about who’s interested in what you’re interested in, and trying to DM the people who engage/seem interested, and seeing what happens<ul><li><a href=\"https://www.neelnanda.io/osmi-slack-invite\">Open Source Mechanistic Interpretability Slack<\/a><\/li><li><a href=\"https://discord.gg/nHS4YxmfeM\">Eleuther Discord<\/a>&#160;(interpretability-general)<\/li><li><a href=\"https://discord.gg/ysVfhCfCKw\">Mech Interp Discord<\/a><\/li><\/ul><\/li><\/ul><p><strong>Staying up to date<\/strong>: Another common question is how to stay up to date with the field. Honestly, I think that people new to the field should not worry that much about this. Most new papers are irrelevant, including the ones that there is hype around. But it's good to stay a little bit in the loop. Note that the community has substantial parts both in academia and outside, which are often best kept up with in different ways.<\/p><ul><li>LessWrong and the AlignmentForum are a reasonable place to keep up to date with the less academic half<\/li><li>Twitter is a confusing, chaotic place that is an okay place to keep up with both. It's a bit unclear who the right people to follow.<ul><li><a href=\"http://x.com/ch402\">Chris Olah<\/a>&#160;doesn't tweet much, but it's high quality when he does.<\/li><li><a href=\"http://x.com/neelnanda5\">I will tweet<\/a>&#160;about all of my interpretability work and sometimes others.<\/li><\/ul><\/li><\/ul><h2 id=\"Careers\" data-internal-id=\"Careers\">Careers<\/h2><h3 id=\"Where_to_apply\" data-internal-id=\"Where_to_apply\">Where to apply<\/h3><ul><li>Anthropic’s interpretability team roles: <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020159008\">research scientist<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020305008\">research engineer<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4009173008\">research manager<\/a><\/li><li><a href=\"https://openai.com/careers/research-engineer-scientist-interpretability\">OpenAI's interpretability team roles<\/a><\/li><li>My team at Google DeepMind will hopefully be <a href=\"https://deepmind.google/about/careers/#open-roles\">hiring in early 2026<\/a>! Watch this space<\/li><li><a href=\"https://transluce.org/\">Transluce<\/a>&#160;-- a nonprofit research lab<\/li><li><a href=\"https://www.goodfire.ai/\">Goodfire<\/a>&#160;-- a mech interp startup that are <a href=\"https://www.goodfire.ai/careers\">hiring a bunch<\/a>.<ul><li>They <a href=\"https://www.goodfire.ai/blog/announcing-our-50m-series-a\">recently raised a $50 million Series A<\/a>&#160;and as of the time of writing are trying to both have people focused on products, and people focused on more fundamental research<\/li><\/ul><\/li><li>The UK government's AI Security Institute's interpretability team (<a href=\"https://www.aisi.gov.uk/careers#open-roles\">not currently hiring<\/a>)<\/li><\/ul><p data-internal-id=\"Applying_for_grants\">Applying for grants<\/p><p>For people trying to get into mech interp via the safety community, there are some funders around open to giving career transition grants to people trying to upskill in a new field like mech interp. Probably the best place I know of is <a href=\"https://www.openphilanthropy.org/career-development-and-transition-funding/\">Open Philanthropy's Early Career Funding.<\/a><\/p><p data-internal-id=\"Explore_Other_AI_Safety_Areas\">Explore Other AI Safety Areas<\/p><p>Mech interp isn't the only game in town! There’s other important areas of safety like Evals, AI Control, and Scalable Oversight, the latter two in particular seem neglected compared to mech interp. The<a href=\"https://arxiv.org/pdf/2504.01849\">&#160;GDM AGI Safety Approach<\/a>&#160;gives an overview of different parts of the field. If you’re doing this for safety reasons, I’d check if there’s other, more neglected subfields, that also appeal to you!<\/p><h3 id=\"What_do_hiring_managers_look_for\" data-internal-id=\"What_do_hiring_managers_look_for\">What do hiring managers look for<\/h3><p>Leaving aside things that apply to basically all roles, like whether this person has a good personality fit (which often just means looking out for red flags), here’s my sense of what hiring managers in interpretability are often looking for.<\/p><p>A useful mental model is that from a hiring manager's perspective, they're making an uncertain bet with little information in a somewhat adversarial environment. Each applicant wants to present themselves as the perfect fit. This means managers need to rely on signals that are hard to fake. But it’s quite difficult to get that much info on a person before you actually go and work with them a bunch.<\/p><p>Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly. If your research track record is good enough, interviews may just act as a check for red flags and to verify that you can actually write code and run experiments well.<\/p><p>Key skills:<\/p><ul><li><strong>Research Skills:<\/strong>&#160;A track record of completing end-to-end projects is the best signal. Papers are a great way to show this.<ul><li><strong>Research taste<\/strong>: The ability to come up with great research ideas <i>and<\/i>&#160;drive them to completion is rare and very valuable.<\/li><li><strong>Experiment design<\/strong>: Can they design good experiments and make their research ideas concrete and convert them into actions?<\/li><\/ul><\/li><li><strong>Conceptual Understanding of Mech Interp:<\/strong>&#160;Do you get the key ideas and know the literature?<\/li><li><p data-internal-id=\"ftnt_ref36\"><strong>Productivity and Conscientiousness:<\/strong>&#160;This is a very hard one to interview for, but incredibly important. A public track record of doing interesting things is a good signal, as are strong references from trusted sources<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-noteref\" id=\"fnrefslnwemz4grq\"><sup><a href=\"#fnslnwemz4grq\">[36]<\/a><\/sup><\/span>.<\/p><\/li><li><strong>Engineering Skills:<\/strong>&#160;Can you work fluently in a Python notebook? Can you write experiment code fast and well? Can you get things done? Do you understand the standard gotchas?<\/li><li><strong>Deep engineering skill<\/strong>: Beyond hacking together experiments, can you navigate large, complex codebases, write maintainable code, design complex software projects, etc?<ul><li>This is much more important if doing research inside a larger lab or tech company than as an independent researcher or academic.<\/li><li>One of the most common reasons we don't hire seemingly promising researchers onto my team is because they lack sufficiently strong engineering skills.<\/li><li>Obviously, LLMs are substantially changing the game when it comes to engineering skills, but I think deep engineering skills will be much harder to automate than shallow ones, unfortunately.<\/li><li>Unfortunately, I don’t have great advice on how to gain these other than working in larger and more complex codebases and learning how to cope. Pair programming with more experienced programmers can be a great way to transfer tacit knowledge<\/li><\/ul><\/li><li><strong>Skepticism<\/strong>: Can you constructively engage with research and critically evaluate it? In particular, can you do this to your own research? Good researchers need to be able to do work that is true.<\/li><\/ul><h3 id=\"Should_you_do_a_PhD_\" data-internal-id=\"Should_you_do_a_PhD_\">Should you do a PhD?<\/h3><p>I don't have a PhD (and think I would have had a far less successful career if I had tried to get one) so I'm somewhat biased. But it's a common question. Here are the strongest arguments I’ve heard in favour:<\/p><ul><li>You get extremely high <strong>autonomy<\/strong>. If you want to spend years going deep on a niche topic that no industry lab would fund, a PhD is one of the only ways to do it.<\/li><li>It's a great environment to cultivate the ability to <strong>set your own research agenda<\/strong>. This is a crucial and difficult skill that is harder to learn in industry, where agendas are often set from the top down (though this varies a lot between team).<\/li><\/ul><p>And here are the reasons I think it's often a bad idea:<\/p><ul><li>The opportunity cost is immense. You could spend 4-6 years gaining direct, relevant experience in an industry lab.<\/li><li>Academic incentives can be misaligned with doing impactful research, e.g. pressure to publish meaning you’re discouraged from admitting to the limitations of your work.<\/li><li>The quality of supervision varies wildly, and a bad supervisor can make your life miserable.<\/li><li>Quality of life: The pay is generally terrible, which may or may not matter to you, and you may only get places in a different city/country than you’d prefer.<\/li><\/ul><p>But with all those caveats in mind, it’s definitely the right option for some! My overall take:<\/p><ul><li>The key thing that matters is mentorship, being in an environment where you are working with a better researcher, and learning from them.<ul><li>PhDs are often a good way of getting this. But if you can gain this by another way, plausibly you should go to that instead. PhDs have a lot of downsides too.<\/li><\/ul><\/li><li>Generally, the variance between supervisors and between managers in industry will dominate the academia versus industry differences, and thus you should pay a lot of attention to who exactly would be managing you.<ul><li>For a PhD, try to speak to your potential supervisor’s students in a private setting. If they say pretty bad things, that's a good reason not to go for the supervisor.<\/li><li>A common mistake is optimising for the most prestigious and famous supervisor when you often want to go for the ones who will have the most time for you, which anti-correlates.<\/li><\/ul><\/li><li>A common mistake is people feeling they need to <i>finish<\/i>&#160;PhDs. But if you sincerely believe that the point of a PhD is to be a learning environment, then why would the formal end of the PhD be the optimal time to leave? It's all kind of arbitrary.<ul><li>IMO, at least every six months, you should seriously evaluate what other opportunities you have, try applying for some things and be emotionally willing leave if a better opportunity comes along (taking into account switching costs).<ul><li>Note that often you can just take a year's leave of absence and resume at will.<\/li><\/ul><\/li><\/ul><\/li><\/ul><p data-internal-id=\"Relevant_Academic_Labs\">Relevant Academic Labs<\/p><p>I’m a big fan of the work coming out of these two, they seem like great places to work:<\/p><ul><li>David Bau (Northeastern)<\/li><li>Martin Wattenberg &amp; Fernanda Viegas (Harvard)<\/li><\/ul><p>Other labs that seem like good places to do interpretability research (note that this is not trying to be a comprehensive list!):<\/p><ul><li>Yonatan Belinkov (Technion)<\/li><li>Jacob Andreas (MIT)<\/li><li>Jacob Steinhardt (Berkeley)<\/li><li>Ellie Pavlick (Brown)<\/li><li>Victor Veitch (UChicago)<\/li><li>Robert West (EPFL)<\/li><li>Roger Grosse (Toronto)<\/li><li>Mor Geva (Tel Aviv)<\/li><li>Sarah Wiegreffe (Maryland)<\/li><li>Aaron Mueller (Boston University)<\/li><\/ul><p><i>Thanks a lot to Arthur Conmy, Paul Bogdan, Bilal Chughtai, Julian Minder, Callum McDougall, Josh Engels, Clement Dumas, Bart Bussmann for valuable feedback<\/i><\/p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-endnote\" id=\"fnnifk1wb1jum\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"nifk1wb1jum\"><sup><strong><a href=\"#fnrefnifk1wb1jum\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Note that I mean a full working month here. So something like 200 working hours. If you're only able to do this part-time, it's fine to take longer. If you're really focused on it, or have a head-start, then move on faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-endnote\" id=\"fnue9pdw6v8rj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"ue9pdw6v8rj\"><sup><strong><a href=\"#fnrefue9pdw6v8rj\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;If you want something even more approachable, one of my past MATS scholars recommends getting GPT-5 thinking to produce coding exercises (eg a Python script with empty functions, and good tests), for an easier way in.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-endnote\" id=\"fnhh6mwdeo4zm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"hh6mwdeo4zm\"><sup><strong><a href=\"#fnrefhh6mwdeo4zm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;It’s fine for this coding to need a bunch of LLM help and documentation/tutorial looking up, this isn’t a memory test. The key thing is being able to correctly explain the core of each technique to a friend/LLM.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-endnote\" id=\"fnsxyjce3nii\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"sxyjce3nii\"><sup><strong><a href=\"#fnrefsxyjce3nii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Note: This curriculum aims to get you started on <i>independent research<\/i>. This is often good enough for academic labs, but the engineering bar for most industry labs is significantly higher, as you’ll need to work in a large complex codebase with hundreds of other researchers. But those skills take much longer to gain.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-endnote\" id=\"fnkte6u8splw\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"kte6u8splw\"><sup><strong><a href=\"#fnrefkte6u8splw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;You want to exclude the first token of the prompt when collecting activations, it’s a weird attention sink and often has high norm/is anomalous in many ways<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-endnote\" id=\"fn2ob115pcmet\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"2ob115pcmet\"><sup><strong><a href=\"#fnref2ob115pcmet\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Gotcha: Remember to try a bunch of coefficients for the vector when adding it. This is a crucial hyper-parameter and steered model behaviour varies a lot depending on its value<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-endnote\" id=\"fn1b9r0ass7sd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1b9r0ass7sd\"><sup><strong><a href=\"#fnref1b9r0ass7sd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Mixture of expert models, where there are many parameters, and only a fraction light up for each token, are a pain for interpretability research. Larger models means you'll need to get more/larger GPUs which is expensive and unwieldy. Favor working with dense models where possible.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-endnote\" id=\"fnbzop9pji3nl\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bzop9pji3nl\"><sup><strong><a href=\"#fnrefbzop9pji3nl\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;You can download then upload the PDF to the model, or just select all and copy and paste from the PDF to the chat window. No need to correct the formatting issues, LLMs are great at ignoring weird formatting artifacts<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-endnote\" id=\"fn207k0k5nobb\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"207k0k5nobb\"><sup><strong><a href=\"#fnref207k0k5nobb\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;<a href=\"http://repo2txt.com\">repo2txt.com<\/a>&#160;is a useful tool for concatenating a Github repo into a single txt file<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-endnote\" id=\"fn979wnkvgpa4\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"979wnkvgpa4\"><sup><strong><a href=\"#fnref979wnkvgpa4\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;If you would like other perspectives, check out <a href=\"https://arxiv.org/abs/2501.16496\">Open Problems in Mechanistic Interpretability<\/a>&#160;(broad lit review from many leading researchers, recent), or <a href=\"https://transformer-circuits.pub/2023/interpretability-dreams/index.html\">Interpretability Dreams<\/a>&#160;(from Anthropic, 2 years old)<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-endnote\" id=\"fn3zw26zes9dx\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3zw26zes9dx\"><sup><strong><a href=\"#fnref3zw26zes9dx\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;And for reasons we’ll discuss later, now feel much more pessimistic about the ambitious reverse engineering direction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-endnote\" id=\"fn7cxhc64szn8\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7cxhc64szn8\"><sup><strong><a href=\"#fnref7cxhc64szn8\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Even if you already have a research background in another field, mechanistic interpretability is sufficiently different that you should expect to need to relearn at least some of your instincts. This stage remains very relevant to you, though you can hopefully learn faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-endnote\" id=\"fn9wj0u0qz3q\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9wj0u0qz3q\"><sup><strong><a href=\"#fnref9wj0u0qz3q\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;The rest of this piece will be framed around approaching learning research like this and why I think it is a reasonable process. Obviously, there is not one true correct way to learn research! When I e.g. critique something as a “mistake”, interpret this as “I often see people do this and think it’s suboptimal for them”, not “there does not exist a way of learning research where this is a good idea<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-endnote\" id=\"fnxw1ra5pqnd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"xw1ra5pqnd\"><sup><strong><a href=\"#fnrefxw1ra5pqnd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;My term for associated knowledge, understanding, intuition, etc.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-endnote\" id=\"fntq4gws0zq69\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tq4gws0zq69\"><sup><strong><a href=\"#fnreftq4gws0zq69\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Read <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">my thoughts on SAEs here<\/a>. There’s still useful work to be done, but it’s an oversubscribed area, and our bar should be higher. They are a useful tool, but not as promising as I once hoped.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-endnote\" id=\"fncdmsagzbqkp\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"cdmsagzbqkp\"><sup><strong><a href=\"#fnrefcdmsagzbqkp\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;This was using a technique called synthetic document fine-tuning (and some other creativity on top), which basically lets you insert false beliefs into a model by generating a bunch of fictional documents where those beliefs are true and fine-tuning the model on them.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-endnote\" id=\"fnp0f0m03b55r\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"p0f0m03b55r\"><sup><strong><a href=\"#fnrefp0f0m03b55r\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>We chose problems we’re excited to see worked on, while trying to avoid fad-like dynamics<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-endnote\" id=\"fng12d8d1lqu\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"g12d8d1lqu\"><sup><strong><a href=\"#fnrefg12d8d1lqu\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;Latents refer to the hidden units of the SAE. These were originally termed “features”, but that term is also used to mean “the interpretable concept the latent refers to”, so I use a different term to minimise confusion.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-endnote\" id=\"fn0td6a2gxwht\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"0td6a2gxwht\"><sup><strong><a href=\"#fnref0td6a2gxwht\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;One of my MATS scholars make a working GPT-5 model diffing agent in a day<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-endnote\" id=\"fn5bdglmkdzr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"5bdglmkdzr\"><sup><strong><a href=\"#fnref5bdglmkdzr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;This is the one line in the post <i>without <\/i>a “as of early Sept 2025” disclaimer, this feels pretty evergreen<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-endnote\" id=\"fnwuxdh4f7kh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wuxdh4f7kh\"><sup><strong><a href=\"#fnrefwuxdh4f7kh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note: \"think\" or \"chain of thought\" are terrible terms. It's far more useful to think of the chain of thought as a scratchpad that a model with very limited short-term memory can choose to use or ignore.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-endnote\" id=\"fn3qxoen8tddk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3qxoen8tddk\"><sup><strong><a href=\"#fnref3qxoen8tddk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Reasoning models break a lot of standard interpretability techniques because now the computational graph goes through the discrete, non-differentiable, and random operation of sampling thousands of times. Most interpretability techniques focus on studying a single forward pass.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-endnote\" id=\"fnlm5ixkfuzk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"lm5ixkfuzk\"><sup><strong><a href=\"#fnreflm5ixkfuzk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Not just, e.g., ones you can publish on.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-endnote\" id=\"fnidab8074tka\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"idab8074tka\"><sup><strong><a href=\"#fnrefidab8074tka\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>I called this moving fast in the blog post, but I think that may have confused some people.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-endnote\" id=\"fnwpekmwudkpd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wpekmwudkpd\"><sup><strong><a href=\"#fnrefwpekmwudkpd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Though often this is done well with just a good introduction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-endnote\" id=\"fn1bau7vsh9tk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1bau7vsh9tk\"><sup><strong><a href=\"#fnref1bau7vsh9tk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And having a well-known researcher as co-author is not sufficient evidence to avoid this, alas. I’m sure at least one paper I’ve co-authored in the past year or two is substantially false<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-endnote\" id=\"fnadytzr5d7y\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"adytzr5d7y\"><sup><strong><a href=\"#fnrefadytzr5d7y\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>It's strongly in your interests for people to build on your work because that makes your original work look better, in addition to being just pretty cool to see people engage deeply with your stuff.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-endnote\" id=\"fnva7mhfkrhm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"va7mhfkrhm\"><sup><strong><a href=\"#fnrefva7mhfkrhm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that deliberately reproducing work, or trying to demonstrate the past work is shoddy, is completely reasonable. You just need to not <i>accidentally<\/i>&#160;reinvent the wheel.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-endnote\" id=\"fntt0owz8koks\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tt0owz8koks\"><sup><strong><a href=\"#fnreftt0owz8koks\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>This is generally a good thing to do regardless of whether you’re focused on research taste or not!<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-endnote\" id=\"fne3252d8idmr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"e3252d8idmr\"><sup><strong><a href=\"#fnrefe3252d8idmr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And, nowadays, LLM knowledge too I guess?<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-endnote\" id=\"fn8354hd0flji\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"8354hd0flji\"><sup><strong><a href=\"#fnref8354hd0flji\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you’ll need someone who’s written several Arxiv papers to endorse you. cs.LG is the typical category for ML papers.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-endnote\" id=\"fn9oppcf0ftrh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9oppcf0ftrh\"><sup><strong><a href=\"#fnref9oppcf0ftrh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you can submit something to a workshop <i>and <\/i>to a conference, so long as the workshop is “non-archival”<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-endnote\" id=\"fnfmh579omuc6\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"fmh579omuc6\"><sup><strong><a href=\"#fnreffmh579omuc6\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>A conference paper is a fair bit more effort, and you generally want to be working with someone who understands the academic conventions and shibboleths and the various hoops you should be jumping through. But I think this can be a nice thing to aim for, especially if you're starting out and need credentials, though mech interp cares less about peer review than most academic subfields.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-endnote\" id=\"fnf09vsa4w37e\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"f09vsa4w37e\"><sup><strong><a href=\"#fnreff09vsa4w37e\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>See <a href=\"https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/\">this NeurIPS experiment<\/a>&#160;showing that half the spotlight papers would be rejected by an independent reviewing council<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-endnote\" id=\"fn7ruxx269r2s\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7ruxx269r2s\"><sup><strong><a href=\"#fnref7ruxx269r2s\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160;This is one of the most valuable things I do for my MATS scholars, IMO.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-endnote\" id=\"fnslnwemz4grq\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"slnwemz4grq\"><sup><strong><a href=\"#fnrefslnwemz4grq\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&#160; Unfortunately, standard reference culture, especially in the US, is to basically lie, and the amount of lying varies between contexts, rendering references mostly useless unless from a cultural context the hiring manager understands or ideally from people they know and trust. This is one of the reasons that doing AI safety mentoring programs like MATS can be extremely valuable, because often your mentor will know people who might then go on to hire you, which makes you a lower risk hire from their perspective.<\/p><\/div><\/li><\/ol>",
                    "sections": [{
                        "title": "TL;DR",
                        "anchor": "TL_DR",
                        "level": 1
                    }, {
                        "title": "Introduction",
                        "anchor": "Introduction",
                        "level": 1
                    }, {
                        "title": "High-Level Framing",
                        "anchor": "High_Level_Framing",
                        "level": 2
                    }, {
                        "title": "Stage 1: Learning the Ropes",
                        "anchor": "Stage_1__Learning_the_Ropes",
                        "level": 1
                    }, {
                        "title": "Machine Learning & Transformer Basics",
                        "anchor": "Machine_Learning___Transformer_Basics",
                        "level": 2
                    }, {
                        "title": "Mechanistic Interpretability Techniques",
                        "anchor": "Mechanistic_Interpretability_Techniques",
                        "level": 2
                    }, {
                        "title": "Mechanistic Interpretability Coding & Tooling",
                        "anchor": "Mechanistic_Interpretability_Coding___Tooling",
                        "level": 2
                    }, {
                        "title": "Understanding the literature",
                        "anchor": "Understanding_the_literature",
                        "level": 2
                    }, {
                        "title": "Using LLMs for Learning",
                        "anchor": "Using_LLMs_for_Learning",
                        "level": 2
                    }, {
                        "title": "Interlude: What is mech interp?",
                        "anchor": "Interlude__What_is_mech_interp_",
                        "level": 1
                    }, {
                        "title": "The Big Picture: Learning the Craft of Research",
                        "anchor": "The_Big_Picture__Learning_the_Craft_of_Research",
                        "level": 1
                    }, {
                        "title": "Unpacking the Research Process",
                        "anchor": "Unpacking_the_Research_Process",
                        "level": 2
                    }, {
                        "title": "What is research taste?",
                        "anchor": "What_is_research_taste_",
                        "level": 2
                    }, {
                        "title": "Stage 2: Practicing Research with Mini-Projects",
                        "anchor": "Stage_2__Practicing_Research_with_Mini_Projects",
                        "level": 1
                    }, {
                        "title": "Choose A Project",
                        "anchor": "Choose_A_Project",
                        "level": 2
                    }, {
                        "title": "Practicing Exploration",
                        "anchor": "Practicing_Exploration",
                        "level": 2
                    }, {
                        "title": "Practicing Understanding",
                        "anchor": "Practicing_Understanding",
                        "level": 2
                    }, {
                        "title": "Using LLMs for Research Code",
                        "anchor": "Using_LLMs_for_Research_Code",
                        "level": 2
                    }, {
                        "title": "Interlude: What’s New In Mechanistic Interpretability?",
                        "anchor": "Interlude__What_s_New_In_Mechanistic_Interpretability_",
                        "level": 1
                    }, {
                        "title": "Avoiding Fads",
                        "anchor": "Avoiding_Fads",
                        "level": 2
                    }, {
                        "title": "What’s New In Mech Interp?",
                        "anchor": "What_s_New_In_Mech_Interp_",
                        "level": 2
                    }, {
                        "title": "A Pragmatic Vision for Mech Interp",
                        "anchor": "A_Pragmatic_Vision_for_Mech_Interp",
                        "level": 2
                    }, {
                        "title": "Stage 3: Working Up To Full Research Projects",
                        "anchor": "Stage_3__Working_Up_To_Full_Research_Projects",
                        "level": 1
                    }, {
                        "title": "Key Research Mindsets",
                        "anchor": "Key_Research_Mindsets",
                        "level": 2
                    }, {
                        "title": "Deepening Your Skills",
                        "anchor": "Deepening_Your_Skills",
                        "level": 2
                    }, {
                        "title": "Practicing Ideation",
                        "anchor": "Practicing_Ideation",
                        "level": 2
                    }, {
                        "title": "Write up your work!",
                        "anchor": "Write_up_your_work_",
                        "level": 2
                    }, {
                        "title": "Mentorship, Collaboration and Sharing Your Work",
                        "anchor": "Mentorship__Collaboration_and_Sharing_Your_Work",
                        "level": 1
                    }, {
                        "title": "So what does a research mentor actually do?",
                        "anchor": "So_what_does_a_research_mentor_actually_do_",
                        "level": 2
                    }, {
                        "title": "Advice on finding a mentor",
                        "anchor": "Advice_on_finding_a_mentor",
                        "level": 2
                    }, {
                        "title": "Community & collaborators",
                        "anchor": "Community___collaborators",
                        "level": 2
                    }, {
                        "title": "Careers",
                        "anchor": "Careers",
                        "level": 1
                    }, {
                        "title": "Where to apply",
                        "anchor": "Where_to_apply",
                        "level": 2
                    }, {
                        "title": "What do hiring managers look for",
                        "anchor": "What_do_hiring_managers_look_for",
                        "level": 2
                    }, {
                        "title": "Should you do a PhD?",
                        "anchor": "Should_you_do_a_PhD_",
                        "level": 2
                    }, {
                        "divider": true,
                        "level": 0,
                        "anchor": "postHeadingsDivider"
                    }, {
                        "anchor": "comments",
                        "level": 0,
                        "title": "No comments"
                    }]
                },
                "reviewWinner": null,
                "sideComments": {
                    "html": "<p id=\"block0\"><strong>Note<\/strong>: If you’ll forgive the shameless self-promotion, <strong>applications for <\/strong><a href=\"http://tinyurl.com/neel-mats-app\"><strong>my MATS stream<\/strong><\/a><strong>&nbsp;are open until<\/strong>&nbsp;<strong>Sept 12<\/strong>. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.<\/p><p id=\"block1\"><i>Last updated Sept 2 2025<\/i><\/p><h2 id=\"TL_DR\" data-internal-id=\"TL_DR\">TL;DR<\/h2><ul><li id=\"block2\">This post is about the mindset and process I recommend if you want to <i>do<\/i>&nbsp;mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.<ul><li id=\"block3\">Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.<\/li><li id=\"block4\"><strong>Learn the minimum viable basics, then do research.<\/strong>&nbsp;Mech interp is an empirical science<\/li><\/ul><\/li><li id=\"block5\">Three stages:<ul><li id=\"block6\"><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learn the ropes<\/strong><\/a><strong>&nbsp;(≤1 month)<\/strong>&nbsp;learn the essentials, go breadth-first;<\/li><li id=\"block7\"><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Learn with research mini-projects<\/strong><\/a>&nbsp;practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;<\/li><li id=\"block8\"><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full projects<\/strong><\/a>, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.<\/li><\/ul><\/li><li id=\"block9\"><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Stage 1:<\/strong><\/a><strong>&nbsp;Learning the Ropes<\/strong><ul><li id=\"block10\"><strong>Breadth over depth; get a good baseline not perfection<\/strong><\/li><li id=\"block11\"><strong>Learn the basics<\/strong>: <a href=\"#Machine_Learning___Transformer_Basics\">Code a transformer from scratch<\/a>, <a href=\"#Mechanistic_Interpretability_Techniques\">key mech interp techniques<\/a>, <a href=\"#Using_LLMs_for_Learning\">the landscape of the field<\/a>, <a href=\"#Machine_Learning___Transformer_Basics\">linear algebra intuitions<\/a>, <a href=\"#Mechanistic_Interpretability_Coding___Tooling\">how to write mech interp code<\/a>&nbsp;(<a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">ARENA is your friend<\/a>)<\/li><li id=\"block12\"><strong>Get your hands dirty<\/strong>: Do <i>not<\/i>&nbsp;just read things. Mech interp is a fundamentally empirical science<\/li><li id=\"block13\"><strong>Move on after a month<\/strong>. Don’t expect to feel “done” or to have covered <i>all <\/i>of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real<\/li><li id=\"block14\"><a href=\"#Using_LLMs_for_Learning\"><strong>Use LLMs extensively<\/strong><\/a>&nbsp;- they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)<\/li><\/ul><\/li><li id=\"block15\"><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><strong>Unpacking the research process<\/strong><\/a>:<ul><li id=\"block16\"><a href=\"#Unpacking_the_Research_Process\">Many skills<\/a>, categorise them by the feedback loops.<ul><li id=\"block17\">Fast skills (minutes-hours) like write/run/debug experiments<\/li><li id=\"block18\">Slow (weeks) like how to prioritise and when to pivot<\/li><li id=\"block19\">Very slow (months) like generating good research ideas<\/li><\/ul><\/li><li id=\"block20\"><strong>Do <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;try to learn all skills at once<\/strong>. Focus on fast/medium skills first, then slowly expand<\/li><li id=\"block21\"><a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">4 phases of research<\/a>: finding an idea (<strong>ideation<\/strong>) -&gt; building intuition and hunches (<strong>exploration<\/strong>) -&gt; testing hypotheses (<strong>understanding<\/strong>) -&gt; refining and writing up (<strong>distillation<\/strong>)<\/li><\/ul><\/li><li id=\"block22\"><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Stage 2:<\/strong><\/a><strong>&nbsp;Mini projects<\/strong>&nbsp;(1-5 days each for 2-4 weeks)<ul><li id=\"block23\"><a href=\"#Practicing_Exploration\">Exploration mindset<\/a>: <strong>Maximise information gain per unit time<\/strong>, learn how to get unstuck. You don't need a plan, so long as you're learning<\/li><li id=\"block24\"><a href=\"#Practicing_Understanding\">Understanding mindset<\/a>: <strong>Every research result is false until proven otherwise<\/strong>. The more exciting a result is, the more likely it is to be false. Be your own greatest critic<\/li><li id=\"block25\">Idea quality (ideation) and write-ups (distillation) aren't the priority yet; <strong>taste and prioritization are learned by doing things<\/strong>.<\/li><li id=\"block26\">Having good research ideas takes forever to learn, <strong>to choose early projects, cheat<\/strong>! <a href=\"#Choose_A_Project\">Pick well scoped projects<\/a>, eg extending a paper (ideas)<\/li><li id=\"block27\"><a href=\"#Using_LLMs_for_Research_Code\"><strong>Use LLMs extensively<\/strong><\/a><strong>&nbsp;<\/strong>- they should speed up your research/coding a <i>lot<\/i>&nbsp;(if you know how to use them properly!)<\/li><\/ul><\/li><li id=\"block28\"><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Stage 3:<\/strong><\/a><strong>&nbsp;Towards full projects<\/strong><ul><li id=\"block29\"><strong>Work in 1-2 week sprints<\/strong>, post-mortem after each, pivot to another project unless it's going <i>great<\/i><\/li><li id=\"block30\"><a href=\"#Deepening_Your_Skills\"><strong>Slower skills<\/strong><\/a><strong>&nbsp;and <\/strong><a href=\"#Key_Research_Mindsets\"><strong>key mindsets<\/strong><\/a>: careful skepticism, awareness of the literature, prioritization, high productivity<\/li><li id=\"block31\"><a href=\"#Doing_Good_Science\"><strong>Do good science<\/strong><\/a><strong>, not flashy science<\/strong>&nbsp;- be honest about limitations, give proof you're not cherry picking, read your data, do the simple things that work, use real baselines.<\/li><li id=\"block32\"><a href=\"#Write_up_your_work_\"><strong>Write-up<\/strong><\/a><strong>&nbsp;your work<\/strong>! Distill it into a narrative, then iteratively expand it to a write-up<ul><li id=\"block33\"><strong>Good public work is <\/strong><a href=\"#Why_aim_for_public_output_\"><strong>your best credential<\/strong><\/a>&nbsp;- for careers, PhDs, finding mentors, etc<\/li><li id=\"block34\"><strong>Writing is not an afterthought<\/strong>&nbsp;- make time for it. <a href=\"#Common_mistakes\">The reader will understand less than you think<\/a><\/li><\/ul><\/li><li id=\"block35\"><strong>Practice <\/strong><a href=\"#Practicing_Ideation\"><strong>generating research ideas<\/strong><\/a>. If possible, try to imitation learn <a href=\"#Research_Taste_Exercises\">a mentor's research taste.<\/a><ul><li id=\"block36\"><a href=\"#Avoiding_Fads\">Avoid fads<\/a>, and think about <a href=\"#What_s_New_In_Mech_Interp_\">what’s new and exciting in mech interp<\/a><\/li><\/ul><\/li><\/ul><\/li><li id=\"block37\"><a href=\"#Advice_on_finding_a_mentor\"><strong>Proactively reach out to mentors<\/strong><\/a>&nbsp;Everything is <i>much<\/i>&nbsp;easier with a good mentor. Cold email, apply for mentoring programs, etc.<ul><li id=\"block38\">Reach out to researchers who'll have time, not the most famous<\/li><\/ul><\/li><li id=\"block39\"><strong>Careers:<\/strong>&nbsp;If you want to work in the field, apply for things! <a href=\"#Where_to_apply\">Jobs<\/a>, <a href=\"#Mentoring_programs\">mentoring programs<\/a>, <a href=\"#Applying_for_grants\">funding<\/a>, <a href=\"#Relevant_Academic_Labs\">academic labs<\/a>.<ul><li id=\"block40\">Bonus thoughts: <a href=\"#What_do_hiring_managers_look_for\">what do hiring managers look for<\/a>, <a href=\"#So_what_does_a_research_mentor_actually_do_\">what does a good research mentor actually do<\/a>, and <a href=\"#Should_you_do_a_PhD_\">should you do a PhD<\/a>?<\/li><\/ul><\/li><li id=\"block41\">I also give various thoughts on how I'm thinking about the field nowadays, and what I’ve changed my mind about. I separate these from the practical advice, so you can take it or leave it.<ul><li id=\"block42\">Covering: <a href=\"#Interlude__What_is_mech_interp_\">how I currently define the field<\/a>, why I'm <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">pessimistic on ambitious reverse engineering, and excited about more pragmatic approaches<\/a>, <a href=\"#What_s_New_In_Mech_Interp_\">what recent work I am<i>&nbsp;<\/i>excited about<\/a>&nbsp;and recommend building on.<\/li><li id=\"block43\">And if any of that worldview appeals, you may want to apply to work with me via <a href=\"http://tinyurl.com/neel-mats-app\">MATS, due Sept 12<\/a>!<\/li><\/ul><\/li><\/ul><h2 id=\"Introduction\" data-internal-id=\"Introduction\">Introduction<\/h2><p id=\"block44\">Mechanistic interpretability (mech interp) is, in my incredibly biased opinion, one of the most exciting research areas out there. We have these incredibly complex AI models that we don't understand, yet there are tantalizing signs of real structure inside them. Even partial understanding of this structure opens up a world of possibilities, yet is neglected by 99% of machine learning researchers. There’s so much to do!<\/p><p id=\"block45\">I think mech interp is an unusually easy field to learn about on your own: there’s a lot of educational materials, you don’t need too much compute, and there’s short feedback loops. But if you're new, it can feel pretty intimidating to get started. This is my updated guide on how to skill up, get involved, reach the point where you can do actual research, and some advice on how to go from there to a career/academic role in the field.<\/p><p id=\"block46\">This guide is deliberately highly opinionated. My goal is to convey a productive mindset and concrete steps that I think will work well, and give a sense of direction, rather than trying to give a fully broad overview or perfect advice. (And many of the links are to my own work because that's what I know best. Sorry!)<\/p><h3 id=\"High_Level_Framing\" data-internal-id=\"High_Level_Framing\">High-Level Framing<\/h3><p id=\"block47\">My core philosophy for getting into mech interp is this: learn the absolute minimal basics as quickly as possible, and then immediately transition to learning by doing research.<\/p><p id=\"block48\">The goal is not to read every paper before you touch research. When doing research you'll notice gaps and go back to learn more. But being grounded in a project will give you vastly more direction to guide your learning, and contextualise why anything you’re learning actually matters. You just want enough grounding to start a project with some understanding of what you’re doing.<\/p><p id=\"block49\">Don't stress about the research quality at first, or having the perfect project idea. Key skills, like <a href=\"https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/Ldrss6o3tiKT6NdMm\">research taste<\/a>&nbsp;and the ability to prioritize, take time to develop. Gaining experience—even messy experience—will teach you the basics like how to run and interpret experiments, which in turn help you learn the high-level skills.<\/p><p id=\"block50\">I break this down into three stages:<\/p><ol><li id=\"block51\"><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learning the ropes<\/strong><\/a>, where you work through the basics breadth first, and after at most a month, move on to stage 2<\/li><li id=\"block52\"><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Practicing research with mini-projects<\/strong><\/a>. Work on throwaway, 1-5 day research projects. Focus on practicing the basic research skills with the fastest feedback loops, don’t stress about having the best ideas, or writing them up. After 2-4 weeks, move on to stage 3<\/li><li id=\"block53\"><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full-projects<\/strong><\/a>: work in 1-2 week sprints. After each, do a post-mortem and pivot to something else, <i>unless <\/i>it was going great and has momentum. Eventually, you should end up working on something longer-term. Start thinking about the deeper skills and research mindsets, practice having good ideas, and prioritize making good public write-ups of sprints that went well<\/li><\/ol><h2 id=\"Stage_1__Learning_the_Ropes\" data-internal-id=\"Stage_1__Learning_the_Ropes\">Stage 1: Learning the Ropes<\/h2><p id=\"block54\">Your goal here is learning the basics: how to write experiments with a mech interp library, understanding the key concepts, getting the lay of the land.<\/p><p data-internal-id=\"ftnt_ref1\" id=\"block55\">Your aim is learning enough that the rest of your learning can be done via doing research, <i>not<\/i>&nbsp;finishing learning. Prioritize ruthlessly. <strong>After max 1 month<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-noteref\" id=\"fnrefnifk1wb1jum\"><sup><a href=\"#fnnifk1wb1jum\">[1]<\/a><\/sup><\/span><strong>, move on to stage 2<\/strong>. I’ve flagged which parts of this I think are essential, vs just nice to have.<\/p><p id=\"block56\"><strong>Do not just read papers <\/strong>- a common mistake among academic types is to spend months reading as many papers as they can get their hands on before writing code. Don’t do it. Mech interp is an empirical science, getting your hands dirty gives key context for your learning. Intersperse reading papers with doing coding tutorials or small research explorations. See <a href=\"https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">my research walkthroughs<\/a>&nbsp;for an idea of what tiny exploratory projects can look like.<\/p><p id=\"block57\">LLMs are a key tool - see <a href=\"#h.ab01gbohcxm5\">the section below<\/a>&nbsp;for advice on using them well<\/p><h3 id=\"Machine_Learning___Transformer_Basics\" data-internal-id=\"Machine_Learning___Transformer_Basics\"><strong>Machine Learning &amp; Transformer Basics<\/strong><\/h3><p id=\"block58\"><i>Assuming you already know basic Python and introductory ML concepts.<\/i><\/p><ul><li id=\"block59\"><strong>Maths:<\/strong><ul><li id=\"block60\"><strong>Linear Algebra is King (Essential):<\/strong>&nbsp;You need to think in vectors and matrices fluently. This is by far the highest value set of generic math you should learn to do mech interp or ML research.<ul><li id=\"block61\"><i>Resource:<\/i>&nbsp;3Blue1Brown's<a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\">&nbsp;Essence of Linear Algebra<\/a>.<\/li><li id=\"block62\"><strong>Highly recommended<\/strong>: Put <a href=\"https://transformer-circuits.pub/2021/framework/index.html\">A Mathematical Framework For Transformer Circuits<\/a>&nbsp;in the context window and have the LLM generate exercises to test your intuitions about transformer internals.<\/li><li id=\"block63\">LLMs are great for checking whether linear algebra actually clicks. Try summarizing what you've learned and the links between different concepts and ask an LLM whether you are correct. For example:<ul><li id=\"block64\">Ensure you understand SVD and why it works<\/li><li id=\"block65\">What does changing basis mean and why does it matter<\/li><li id=\"block66\">Key ways a low rank and full rank matrix differ<\/li><\/ul><\/li><\/ul><\/li><li id=\"block67\"><strong>Other Bits:<\/strong>&nbsp;Basic probability, info theory, optimization, vector calculus.<ul><li id=\"block68\">Use an LLM tutor to quiz your understanding on the parts most relevant to transformers<\/li><\/ul><\/li><li id=\"block69\">Generally don’t bother learning other areas of maths (unless doing it for fun!)<\/li><\/ul><\/li><li id=\"block70\"><strong>Practical ML with PyTorch: (Essential)<\/strong><ul><li id=\"block71\"><p data-internal-id=\"ftnt_ref2\" id=\"block72\">Code a simple Transformer (like GPT-2) from scratch. ARENA Chapter 1.1 is a great coding tutorial<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-noteref\" id=\"fnrefue9pdw6v8rj\"><sup><a href=\"#fnue9pdw6v8rj\">[2]<\/a><\/sup><\/span><\/p><ul><li id=\"block73\"><p data-internal-id=\"ftnt_ref2\" id=\"block74\">This builds intuitions for mech interp <i>and <\/i>on using PyTorch.<\/p><\/li><li id=\"block75\"><p data-internal-id=\"ftnt_ref2\" id=\"block76\">I have two video tutorials on this, starting from the basics - <a href=\"https://www.youtube.com/watch?v=bOYE6E8JrtU&amp;list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz\">start here<\/a>&nbsp;if you’re not sure what to do!<\/p><\/li><li id=\"block77\"><p data-internal-id=\"ftnt_ref2\" id=\"block78\">And use LLMs to fill in any background things you’re missing, like PyTorch basics<\/p><\/li><\/ul><\/li><\/ul><\/li><li id=\"block79\"><strong>Cloud GPUs:<\/strong><ul><li id=\"block80\">You’ll need to be able to run language models, which (typically) needs a GPU<\/li><li id=\"block81\">You can start with Google Colab to get started fast, but it’ll be very constraining to use long-term. Learn to rent and use a cloud GPU.<ul><li id=\"block82\">Newer Macbook Pros, or computers with powerful gaming GPUs may also be able to run LLMs locally<\/li><\/ul><\/li><li id=\"block83\"><i>Resource:<\/i>&nbsp;ARENA has a<a href=\"https://arena-appendix.streamlit.app/cloud-gpus\">&nbsp;<\/a><a href=\"https://arena-chapter0-fundamentals.streamlit.app/#vm-setup-instructions\">guide<\/a>. I like<a href=\"http://runpod.io/\">&nbsp;<\/a><a href=\"http://runpod.io\">runpod.io<\/a>&nbsp;as a provider;<a href=\"http://vast.ai/\">&nbsp;vast.ai<\/a>&nbsp;is cheaper.<\/li><li id=\"block84\">nnsight also lets you do some <a href=\"https://nnsight.net/notebooks/tutorials/get_started/start_remote_access/\">interpretability on certain models they host themselves<\/a>, including LLaMA 3 405B, which can be a great way to work with larger models.<\/li><\/ul><\/li><\/ul><h3 id=\"Mechanistic_Interpretability_Techniques\" data-internal-id=\"Mechanistic_Interpretability_Techniques\">Mechanistic Interpretability Techniques<\/h3><p id=\"block85\">A lot of mech interp research looks like knowing the right technique to apply and in what context. This is a key thing to prioritise getting your head around when starting out. You’ll learn this with a mix of reading educational materials and doing coding tutorials like ARENA (discussed in next sub-section).<\/p><ul><li id=\"block86\"><a href=\"https://arxiv.org/abs/2405.00208\">Ferrando et al<\/a>&nbsp;is a good <strong>overview<\/strong>&nbsp;of the key techniques - it’s long enough that you shouldn’t prioritise reading it in full, but it’s a great reference<ul><li id=\"block87\">Put it in a LLM context window and ask questions, or to write you exercises<\/li><\/ul><\/li><li id=\"block88\"><p data-internal-id=\"ftnt_ref3\" id=\"block89\"><strong>Essential<\/strong>: Make sure you understand these <strong>core techniques<\/strong>, well enough that you can code it up yourself on a simple model like GPT-2 Small<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-noteref\" id=\"fnrefhh6mwdeo4zm\"><sup><a href=\"#fnhh6mwdeo4zm\">[3]<\/a><\/sup><\/span>:<\/p><ul><li id=\"block90\"><p data-internal-id=\"ftnt_ref3\" id=\"block91\">Activation Patching<\/p><\/li><li id=\"block92\"><p data-internal-id=\"ftnt_ref3\" id=\"block93\">Linear Probes<\/p><\/li><li id=\"block94\"><p data-internal-id=\"ftnt_ref3\" id=\"block95\">Using Sparse Autoencoders (SAEs) (you only need to write code that uses an SAE, not trains one)<\/p><\/li><li id=\"block96\"><p data-internal-id=\"ftnt_ref3\" id=\"block97\">Max Activating Dataset Examples<\/p><\/li><li id=\"block98\"><p data-internal-id=\"ftnt_ref3\" id=\"block99\">Nice-to-have:<\/p><ul><li id=\"block100\"><p data-internal-id=\"ftnt_ref3\" id=\"block101\">Steering Vectors<\/p><\/li><li id=\"block102\"><p data-internal-id=\"ftnt_ref3\" id=\"block103\">Direct Logit Attribution (DLA) (a simpler version is called logit lens)<\/p><\/li><\/ul><\/li><li id=\"block104\"><p data-internal-id=\"ftnt_ref3\" id=\"block105\"><strong>Key exercise<\/strong>: Describe each technique to an LLM with Ferrando et al in the context window and ask for feedback. Iterate until you get it all right.<\/p><ul><li id=\"block106\"><p data-internal-id=\"ftnt_ref3\" id=\"block107\">Use an anti-sycophancy prompt to get real feedback, by pretending someone else wrote your answer, e.g. “I saw someone claim this, it seems pretty off to me, can you help me give them direct but constructive feedback on what they missed? [insert your description]”<\/p><\/li><\/ul><\/li><\/ul><\/li><li id=\"block108\">Remember that there’s a bunch of valuable <strong>black-box interpretability <\/strong>techniques! (ie that don’t use the model’s internals) You can often correctly guess a model’s algorithm by reading its chain of thought. Careful variation of the prompt is a powerful way to causally test hypotheses.<ul><li id=\"block109\">They’re an additional tool. Often the correct first step in an investigation is just talking to the model and bunch and observing its behaviour. Don’t be a purist and dismiss them as “not rigorous” - they have uses and flaws, just like any other technique.<ul><li id=\"block110\">One <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">project I supervised<\/a>&nbsp;on interpreting “self-preservation” in frontier models started with simple black-box techniques, and it just worked, we never needed anything fancier.<\/li><\/ul><\/li><li id=\"block111\">Understand fancier black-box techniques like <a href=\"https://arxiv.org/abs/2312.12321\">token forcing<\/a>&nbsp;(aka prefill attacks) where you put words in a model’s mouth.<\/li><\/ul><\/li><\/ul><h3 id=\"Mechanistic_Interpretability_Coding___Tooling\" data-internal-id=\"Mechanistic_Interpretability_Coding___Tooling\">Mechanistic Interpretability Coding &amp; Tooling<\/h3><ul><li id=\"block112\"><p data-internal-id=\"ftnt_ref4\" id=\"block113\"><strong>Goal:<\/strong>&nbsp;Get comfortable running experiments and \"playing\" with model internals. Get the engineering basics down<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-noteref\" id=\"fnrefsxyjce3nii\"><sup><a href=\"#fnsxyjce3nii\">[4]<\/a><\/sup><\/span>. Get your hands dirty<strong>.<\/strong><\/p><\/li><li id=\"block114\"><strong>ARENA<\/strong>: ARENA has <a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">a set of fantastic coding tutorials by Callum McDougall<\/a>, you should just go do these. But there’s tons, so <strong>prioritize ruthlessly<\/strong>.<ul><li id=\"block115\"><strong>Essential<\/strong><i><strong>:<\/strong><\/i><strong>&nbsp;Chapter 1.2<\/strong>&nbsp;(Interpretability Basics – prioritize the first 3 sections on tooling, direct observation, and patching).<\/li><li id=\"block116\"><i>Recommended: <\/i>1.4.1 (Causal Interventions &amp; Activation Patching – this is a core technique).<\/li><li id=\"block117\"><i>Worthwhile<\/i>: 1.3.2 (Sparse Autoencoders (SAEs) – Skim or Skip section 1, the key thing to get from the rest is an intuition for what SAEs are, strengths and weaknesses, and how to use an open source SAE. Don’t worry about training them).<\/li><\/ul><\/li><li id=\"block118\"><strong>Tooling <\/strong>(<strong>Essential<\/strong>)<strong>:<\/strong>&nbsp;Get proficient with at least one mech interp library, this is what you’ll use to run experiments.<ul><li id=\"block119\"><a href=\"https://github.com/TransformerLensOrg/TransformerLens\">TransformerLens<\/a>: best for small models &lt;=9B where you want to write more complex interpretability experiments, or work with many models at once.<ul><li id=\"block120\">As of early Sept 2025, TransformerLens <a href=\"https://github.com/TransformerLensOrg/TransformerLens/releases/tag/v3.0.0a5\">v3<\/a>&nbsp;is in alpha, works well with large models and is far more flexible.<\/li><\/ul><\/li><li id=\"block121\"><a href=\"http://nnsight.net/\">nnsight<\/a>: More performant, works well on larger models, it’s just a wrapper around standard LLM libraries like HuggingFace transformers<\/li><\/ul><\/li><li id=\"block122\"><strong>LLM APIs<\/strong>: Learn how to use an LLM API to call an LLM programmatically. This is super useful for measuring qualitative things about some data, and for generating synthetic datasets<ul><li id=\"block123\">I like <a href=\"http://openrouter.ai\">openrouter.ai<\/a>&nbsp;which lets you access almost all the important LLMs from a single place. GPT5 and Gemini are reasonably priced and good defaults, they have a range of sizes<ul><li id=\"block124\">Cerebras and Groq have <i>way <\/i>higher throughput than normal providers, and serve a handful of open source models, they may be worth checking out.<\/li><\/ul><\/li><li id=\"block125\"><p data-internal-id=\"ftnt_ref6\" id=\"block126\">Exercise: Make a happiness steering vector (for e.g. GPT-2 Small) by having an LLM via an API generate 32 happy prompts and 32 sad prompts, and taking the difference in mean activations<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-noteref\" id=\"fnrefkte6u8splw\"><sup><a href=\"#fnkte6u8splw\">[5]<\/a><\/sup><\/span>&nbsp;(e.g. the residual stream at the middle layer). Add this vector to the model’s residual stream<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-noteref\" id=\"fnref2ob115pcmet\"><sup><a href=\"#fn2ob115pcmet\">[6]<\/a><\/sup><\/span>&nbsp;while generating responses to some example prompts, and use an LLM API to rate how happy they seem, and see this score go up when steering.<\/p><\/li><\/ul><\/li><li id=\"block127\"><strong>Open source LLMs<\/strong>: You’ll want to work a lot with open source LLMs, as the thing you’re trying to interpret. The best open source LLM changes a lot<ul><li id=\"block128\"><p data-internal-id=\"ftnt_ref7\" id=\"block129\">As of early Sept 2025, Qwen3 is a good default model family. Each model has reasoning and non-reasoning mode, there’s a good range of sizes, and most are dense<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-noteref\" id=\"fnref1b9r0ass7sd\"><sup><a href=\"#fn1b9r0ass7sd\">[7]<\/a><\/sup><\/span>&nbsp;<\/p><ul><li id=\"block130\"><p data-internal-id=\"ftnt_ref7\" id=\"block131\">Gemma 3 and LLaMA 3.3 are decent non-reasoning models. I’ve heard bad things about gpt-oss and LLaMA 4<\/p><\/li><\/ul><\/li><li id=\"block132\"><i>Gotcha: <\/i>The different open source LLMs often have different tokenizations and formats for chat or reasoning tokens. Using the wrong token format can only somewhat degrade performance and may be hard to notice while corrupting your results - keep an eye out, try hard to find where this might be documented, and sanity check by e.g. comparing to official evals<\/li><\/ul><\/li><\/ul><h3 id=\"Understanding_the_literature\" data-internal-id=\"Understanding_the_literature\">Understanding the literature<\/h3><p id=\"block133\">Your priority is to understand the concepts and the basics, but you want a sense for the landscape of the field, so you should practice reading at least some papers.<\/p><ul><li id=\"block134\">Remember, <strong>breadth over depth<\/strong>. Skim things, get a sense of what's out there, and only dive into the things that are most interesting.<ul><li id=\"block135\">You should be heavily using <strong>LLMs<\/strong>&nbsp;here. Give them something you're considering reading and get a summary, ask questions about the work, summarise your understanding to it and ask for feedback (with an anti-sycophancy prompt).<ul><li id=\"block136\">If you aren't able to verify yourself, cross-reference by asking multiple LLMs and making sure they all say consistent things.<\/li><\/ul><\/li><\/ul><\/li><li id=\"block137\">Here’s <a href=\"https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite\">a list of my favourite papers<\/a>&nbsp;(as of mid 2024) with summaries and opinions<ul><li id=\"block138\">Do <i>not <\/i>try to read all of these in full. Skim summaries, skim abstracts, pick a few to explore deeper with an LLM, <i>then<\/i>&nbsp;decide if you want to read the full paper.<\/li><li id=\"block139\"><a href=\"https://www.youtube.com/@neelnanda2469\">My YouTube Channel<\/a>:<a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T&amp;index=1\">&nbsp;<\/a><a href=\"https://www.youtube.com/watch?v=KV5gbOmHbjU&amp;list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&amp;pp=gAQB\">Paper walkthroughs<\/a>, <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">recordings of myself doing research<\/a>, and talks.<\/li><\/ul><\/li><li id=\"block140\"><a href=\"https://arxiv.org/abs/2501.16496\">Open Problems In Mechanistic Interpretability<\/a>&nbsp;is a decent recent literature review, that a lot of top mech interp people were involved in<ul><li id=\"block141\">Be warned that the paper basically consists of a bunch of opinionated and disagreeable researchers writing their own sections and often having strong takes. Don’t defer to it too much, but it's a good way to quickly assess what's out there.<\/li><\/ul><\/li><li id=\"block142\"><strong>Deep dives<\/strong>: You should read at least one paper carefully and in full. This is a useful skill that you will use in research projects where there’s a handful of extremely relevant papers to your project<ul><li id=\"block143\">This is much more than just reading the words! You should write out a summary, try to understand the surrounding context with LLM help, be able to describe why the paper exists, the motivation, the problem it's trying to solve, etc.<\/li><li id=\"block144\">Aim for a barbell strategy: put minimal effort into most papers and a lot of effort into a few.<\/li><\/ul><\/li><li id=\"block145\"><strong>LLMs<\/strong>: LLMs are a super useful tool for exploring the literature, but easy to shoot yourself in the foot with.<ul><li id=\"block146\">As a search engine over the literature (especially with some lit reviews in context, or a starting paper), basically doing a lit review, finding relevant work for a question you have, etc.<ul><li id=\"block147\"><p data-internal-id=\"ftnt_ref8\" id=\"block148\">As a tool to help you skim a paper - put the paper in the context window<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-noteref\" id=\"fnrefbzop9pji3nl\"><sup><a href=\"#fnbzop9pji3nl\">[8]<\/a><\/sup><\/span>&nbsp;then get a summary, ask it questions, etc<\/p><\/li><li id=\"block149\">If you’re concerned about hallucinations, you can&nbsp;ask it to support answers with quotes (and verify these are real and make sense), or give its answer to another LLM and ask for harsh critique of all the inaccuracies. Honestly, I often don’t bother though, frontier reasoning models are pretty good now.<\/li><\/ul><\/li><li id=\"block150\">As a tool to help with deep dives - you need to actually read the paper, but I recommend having the LLM chat open as you read with the paper in the context and asking it questions, for context, etc every time you get confused.<\/li><\/ul><\/li><\/ul><h3 id=\"Using_LLMs_for_Learning\" data-internal-id=\"Using_LLMs_for_Learning\">Using LLMs for Learning<\/h3><p id=\"block151\"><i>Note: I expect this section to go out of date fast! Written early Sept 2025<\/i><\/p><p id=\"block152\">LLMs are a super useful tool for learning, especially in a new field. While they struggle to beat experts, they often beat novices. If you aren’t using them regularly throughout this process, I’d guess you’re leaving a bunch of value on the table.<\/p><p id=\"block153\">But LLMs have weird flaws and strengths, and it’s worth being intentional about how you use them:<\/p><ul><li id=\"block154\"><strong>Use a good model<\/strong>:&nbsp;The best paid models are way better than e.g. free ChatGPT. Don't be a cheapskate; if you can, get a $20/month subscription, it makes a big difference. Gemini 2.5 Pro, Claude 4.1 Opus with extended thinking, and GPT-5 Thinking are all reasonable. (do <i>not <\/i>use non-thinking GPT-5 or anything older like GPT-4o, reasoning models are a big upgrade)<ul><li id=\"block155\">If you can’t get a subscription, Gemini 2.5 Pro is also available for free, and is the best.<\/li><li id=\"block156\">Use Gemini 2.5 Pro via <a href=\"https://aistudio.google.com/prompts/new_chat\">AI Studio<\/a>, it’s way better than the main Gemini interface&nbsp;and has much nicer rate limits for free users. Always use compare mode (the button in the header with two arrows) to see two responses in parallel from Pro<\/li><li id=\"block157\">See <a href=\"https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher?commentId=jDzbZGnjWDMsNjDPQ\">thoughts<\/a> from my MATS alum Paul Bogdan comparing different LLMs for learning, and why he currently prefers Gemini<\/li><\/ul><\/li><li id=\"block158\"><strong>System Prompts:<\/strong>&nbsp;System prompts make a big difference - be concrete and specific about what you want, and how you want it done.<ul><li id=\"block159\">LLMs are good at this: I'll just ramble at one about what the task is, my criteria, the failure modes I don't want, and then it’ll just write the prompt for me<\/li><li id=\"block160\">If the prompt doesn’t work, tell the LLM what it did wrong, and see if it can rewrite the prompt for you.<\/li><\/ul><\/li><li id=\"block161\"><strong>Merge perspectives<\/strong>:<ul><li id=\"block162\">Ask a Q to multiple different frontier LLMs, give LLM B’s response to LLM A and ask it to assess the strengths and weaknesses then merge.<ul><li id=\"block163\">If a point is in both original responses, it’s probably not a hallucination<\/li><\/ul><\/li><li id=\"block164\">If you want to fact check an LLM’s answer, give it to another LLM with an anti-sycophancy prompt<\/li><\/ul><\/li><li id=\"block165\"><strong>Anti-Sycophancy Prompts:<\/strong>&nbsp;LLMs are bad at giving critical feedback. Frame your request so the sycophantic thing to do is to be critical, by pretending someone else wrote the thing you want feedback on.<ul><li id=\"block166\"><i>\"A friend wrote this explanation and asked for brutally honest feedback. They'll be offended if I hold back. Please help me give them the most useful feedback.\"<\/i><\/li><li id=\"block167\"><i>\"I saw someone claiming this, but it seems pretty dumb to me. What do you think?\"<\/i><\/li><li id=\"block168\"><i>“Some moron wrote this thing, and I find this really annoying. Please write me a brutal but truthful response”<\/i><\/li><\/ul><\/li><li id=\"block169\"><strong>Learn actively, not passively:<\/strong><ul><li id=\"block170\"><strong>Summarize <\/strong>your understanding back to the LLM in your own words and ask for critical feedback. Do this every time you read a paper or learn about a new concept<\/li><li id=\"block171\">Try having it teach you <strong>socratically<\/strong>. Note: you can probably design a better system prompt than the official “study mode”<\/li><li id=\"block172\">Ask the LLM to <strong>generate exercises<\/strong>&nbsp;to test your understanding, including maths and coding exercises as appropriate.<ul><li id=\"block173\">Gemini can make multiple choice quizzes, which some enjoy<\/li><li id=\"block174\">Coding exercises can be requested with accompanying tests, and template code with blank functions for you to fill out, a la the ARENA tutorials.<\/li><\/ul><\/li><\/ul><\/li><li id=\"block175\"><p data-internal-id=\"ftnt_ref9\" id=\"block176\"><strong>Context engineering:<\/strong>&nbsp;Modern LLMs are much more useful with relevant info in context. If you give them the paper in question, or source code of the relevant library<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-noteref\" id=\"fnref207k0k5nobb\"><sup><a href=\"#fn207k0k5nobb\">[9]<\/a><\/sup><\/span>, they’ll be far more helpful.<\/p><ul><li id=\"block177\"><p data-internal-id=\"ftnt_ref9\" id=\"block178\">See <a href=\"https://drive.google.com/drive/u/0/folders/1GfrgKJwndk-twnJ8K7Ba-TE9i_8wBWAU\">this folder<\/a>&nbsp;for a bunch of saved context files for mech interp queries. If you don’t know what you need, just use <a href=\"https://drive.google.com/file/d/18cF3lkU17_elUSv0zk8KSVejM1jGfNnz/view?usp=drive_link\">this default file<\/a>.<\/p><\/li><li id=\"block179\"><p data-internal-id=\"ftnt_ref9\" id=\"block180\">I recommend Gemini 2.5 Pro (1M context window) via<a href=\"http://aistudio.google.com/\">&nbsp;aistudio.google.com<\/a>; the UI is better. Always turn compare mode on, you get two answers in parallel<\/p><\/li><\/ul><\/li><li id=\"block181\"><strong>Voice dictation<\/strong>: If you dictate to your LLM, via free speech-to-text software, and run it with no editing, it’ll understand fine. I personally find this much easier, especially when brain-dumping.<ul><li id=\"block182\"><a href=\"http://superwhisper.com\">Superwhisper<\/a>&nbsp;on Mac is great; Superwhisper is not currently available on Windows, but Windows users can use <a href=\"https://wisprflow.ai/\">Whispr Flow<\/a>.<\/li><\/ul><\/li><li id=\"block183\"><strong>Coding<\/strong>: LLM tools like Cursor are great for coding, but <i>not <\/i>if your goal is to learn. For things like ARENA, only let yourself use browser-based LLMs, and only use them as a tutor. Don’t copy and paste code, your goal is to learn not complete exercises.<\/li><\/ul><h2 id=\"Interlude__What_is_mech_interp_\" data-internal-id=\"Interlude__What_is_mech_interp_\">Interlude: What is mech interp?<\/h2><p id=\"block184\"><i>Feel free to skip to the<\/i>&nbsp;<i>“<\/i><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p data-internal-id=\"ftnt_ref10\" id=\"block185\">At this point it’s worth reflecting on what mech interp actually <i>is<\/i>. What are we even doing here? There isn't a consensus definition on how exactly to define mechanistic interpretability, and different researchers will give very different takes. But <i>my<\/i>&nbsp;working definition is as follows<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-noteref\" id=\"fnref979wnkvgpa4\"><sup><a href=\"#fn979wnkvgpa4\">[10]<\/a><\/sup><\/span>.<\/p><ul><li id=\"block186\"><strong>Interpretability<\/strong>&nbsp;is the study of understanding models, gaining insight into their behavior, the cognition inside of them, why and how they work, etc. This is the important part and the heart of the field.<\/li><li id=\"block187\"><strong>Mechanistic<\/strong>&nbsp;means using the internals of the model, the weights and activations<\/li><li id=\"block188\">So <strong>mechanistic interpretability <\/strong>is any approach to understanding the model that uses its internals.<ul><li id=\"block189\">This is distinct from some other worthwhile directions, like <strong>black box interpretability<\/strong>, understanding models without using the internals, and <strong>model internals<\/strong>, using the internals of the model for other things like steering vectors.<\/li><\/ul><\/li><\/ul><p id=\"block190\"><strong>Why this definition?<\/strong>&nbsp;To do impactful research, it's often good to find the directions that other people are missing. I think of most of machine learning as non-mechanistic non-interpretability. 99% of ML research just looks at the inputs and outputs to models, and treats its north star as controlling their behavior. Progress is defined by making a number go up, not to explain why it works. This has been very successful, but IMO leaves a lot of value on the table. Mechanistic interpretability is about doing better than this, and has achieved a bunch of cool stuff, like <a href=\"https://arxiv.org/abs/2310.16410\">teaching grandmasters how to play chess better by interpreting AlphaZero<\/a>.<\/p><p id=\"block191\"><strong>Why care?<\/strong>&nbsp;Obviously, our goal is not “do things if and only if they fit the above definition”, but I find it a useful one. To discuss this, let’s first consider our actual goals here. To me, <strong>the ultimate goal is to make human-level AI systems (or beyond) safer<\/strong>. I do mech interp because I think we’ll find enough understanding of what happens inside a model to be pragmatically useful here (also, because mech interp is fun!): to better understand how they work, detect if they're lying to us, detect and diagnose unexpected failure modes, etc. But people’s goals vary, e.g. real-world usefulness today, aesthetic beauty, or scientific insight. It’s worth thinking about what yours are.<\/p><p id=\"block192\">Some implications of this framing worth laying out:<\/p><ul><li id=\"block193\">My ultimate <strong>north star is pragmatism<\/strong>&nbsp;- achieve enough understanding to be (reliably) useful. Subgoals like “completely reverse engineer the model” are just means to an end.<ul><li id=\"block194\">One of my big shifts in research prioritization in recent years is concluding that <strong>reverse engineering is not the right aim<\/strong>. Instead, I think we should just more directly try to do pragmatic work that enables us to do useful things using internals. I discuss this shift more <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">later on<\/a>.<\/li><\/ul><\/li><li id=\"block195\"><p data-internal-id=\"ftnt_ref11\" id=\"block196\">This is a <strong>broad definition<\/strong>. Historically, the field has focused on more specific agendas, like ambitious reverse engineering of models. But I think we shouldn’t limit ourselves, there’s many other important and neglected directions and the field is large enough to cover a lot of ground<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-noteref\" id=\"fnref3zw26zes9dx\"><sup><a href=\"#fn3zw26zes9dx\">[11]<\/a><\/sup><\/span><\/p><\/li><li id=\"block197\">It’s about <strong>understanding<\/strong>, not just using internals - model internals methods like steering vectors can be useful for shaping a model’s behaviour, but compete with many powerful methods like prompting and fine-tuning. Very few areas of ML can achieve understanding<\/li><li id=\"block198\"><strong>Don’t be a purist<\/strong>&nbsp;- using internals is a means to an end. If black-box methods are the right tool, use them<\/li><\/ul><h2 id=\"The_Big_Picture__Learning_the_Craft_of_Research\" data-internal-id=\"The_Big_Picture__Learning_the_Craft_of_Research\">The Big Picture: Learning the Craft of Research<\/h2><p data-internal-id=\"ftnt_ref12\" id=\"block199\">So, you've gone through the tutorials, you understand the core concepts, and you can write some basic experimental code. Now comes the hard part: learning how to actually do mech interp research<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-noteref\" id=\"fnref7cxhc64szn8\"><sup><a href=\"#fn7cxhc64szn8\">[12]<\/a><\/sup><\/span>.<\/p><p id=\"block200\">This is an inherently difficult thing to learn, of course. But IMO people often misunderstand what they need to do here, try to learn everything at once, or more generally make life unnecessarily hard for themselves. The key is to break the process down, understand the different skills involved, and focus on <strong>learning the pieces with the fastest feedback loops first<\/strong>.<\/p><p data-internal-id=\"ftnt_ref13\" id=\"block201\">I suggest breaking this down into two stages<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-noteref\" id=\"fnref9wj0u0qz3q\"><sup><a href=\"#fn9wj0u0qz3q\">[13]<\/a><\/sup><\/span>.<\/p><p id=\"block202\"><strong>Stage 2<\/strong>: working on a bunch of throwaway mini projects of 1-5 days each. Don't stress about choosing the best projects or producing public output. The goal is to learn the skills with the fastest feedback loops.<\/p><p id=\"block203\"><strong>Stage 3: <\/strong>After a few weeks of these, start to be more ambitious: paying more attention to how you choose your projects, gaining the subtler skills, and how to write things up. I still recommend working iteratively, in one to two week sprints, but ending up with longer-term projects if things go well.<\/p><p id=\"block204\">Note: Unlike stage 1 to 2, the transition from stages two to three should be fairly gradual as you take on larger projects and become more ambitious. A good default would be after three to four weeks in stage two, but you don’t need to have a big formal shift.<\/p><p id=\"block205\"><strong>Mentorship<\/strong>: A good mentor is a major accelerator, and finding one should be a major priority for you. In the careers section, I provide advice on <a href=\"#Advice_on_finding_a_mentor\">how to go about finding a good mentor<\/a>, and <a href=\"#So_what_does_a_research_mentor_actually_do_\">how concretely they can add value<\/a>. In the rest of the post I'll write most of it assuming you do not have a mentor and then flag the ways to use a mentor where appropriate.<\/p><h3 id=\"Unpacking_the_Research_Process\" data-internal-id=\"Unpacking_the_Research_Process\">Unpacking the Research Process<\/h3><p id=\"block206\">I find it helpful to think of research as a cycle of four distinct stages. Read <a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">my blog post on the research proces<\/a>&nbsp;for full details, but in brief:<\/p><ul><li id=\"block207\"><strong>Ideation:<\/strong>&nbsp;You choose a research problem or a general domain to focus on.<\/li><li id=\"block208\"><strong>Exploration:<\/strong>&nbsp;You may not have a specific hypothesis yet; you’re just trying to figure out the right questions to ask, and build deeper intuition for the domain. Your north star is to gain information and surface area.<\/li><li id=\"block209\"><strong>Understanding:<\/strong>&nbsp;This begins when you have a concrete hypothesis, and some intuitive understanding of the domain. Your north star is to convince yourself that the hypothesis is true or false.<\/li><li id=\"block210\"><strong>Distillation:<\/strong>&nbsp;Once you’re convinced, your north star is to compress your findings into concise, rigorous truth that you can communicate to the world - create enough experimental evidence to convince others, write it up clearly, and share it.<\/li><\/ul><p id=\"block211\">Underpinning these stages is a host of skills, best separated by how quickly you can apply them and get feedback. We learn by doing things and getting feedback, so you’ll learn the fast ones much more quickly. I put a rough list and categorization below.<\/p><p id=\"block212\">My general advice is <strong>to prioritize learning these in order of feedback loops<\/strong>. If it seems like you need a slow skill to get started, like the taste to choose a good research problem, find a way to cheat rather than stressing about not having that skill (e.g. doing an incremental extension to a paper, getting one from a mentor, etc).<\/p><ul><li id=\"block213\"><strong>Fast Loop (minutes-hours):<\/strong><ul><li id=\"block214\">Planning and writing experiment code<ul><li id=\"block215\"><strong>Medium<\/strong>: Designing great experiments<\/li><li id=\"block216\"><strong>Medium<\/strong>: Knowing when to write hacky vs. quality code.<\/li><\/ul><\/li><li id=\"block217\">Running/debugging experiments<ul><li id=\"block218\"><strong>Medium/Slow<\/strong>: Spotting and fixing subtle bugs (e.g., you got your tokenization subtly wrong, you didn’t search hyper-parameters well enough, etc)<\/li><\/ul><\/li><li id=\"block219\">Interpreting the results of a single experiment.<ul><li id=\"block220\"><strong>Medium<\/strong>: Understanding whether your results support your conclusions<\/li><li id=\"block221\"><strong>Slow<\/strong>: Spotting subtle interpretability illusions where your results don't actually support your claims<\/li><\/ul><\/li><\/ul><\/li><li id=\"block222\"><strong>Medium Loop (days):<\/strong><ul><li id=\"block223\">Developing a conceptual understanding of mech interp<ul><li id=\"block224\"><strong>Slow<\/strong>: Noticing and fixing your own subtle confusions<\/li><li id=\"block225\"><strong>Slow<\/strong>: Build a deep knowledge of the literature<\/li><\/ul><\/li><li id=\"block226\">Knowing how to explore without getting stuck<\/li><li id=\"block227\">Writing up results<ul><li id=\"block228\"><strong>Slow<\/strong>: Communicating your work in a way that’s genuinely clear to people.<\/li><li id=\"block229\"><strong>Slow<\/strong>: Communicating why your work is <i>interesting<\/i>&nbsp;to people<\/li><\/ul><\/li><\/ul><\/li><li id=\"block230\"><strong>Slow Loop (weeks):<\/strong><ul><li id=\"block231\">Prioritizing which experiment to do next<\/li><li id=\"block232\">Knowing when to continue with a research direction or pivot to another angle of attack/another project<\/li><li id=\"block233\">Identifying bad research ideas, <i>without <\/i>doing a project on them first<\/li><\/ul><\/li><li id=\"block234\"><strong>Very Slow Loop (months):<\/strong><ul><li id=\"block235\">Coming up with good research ideas. This is the core of \"research taste.\"<\/li><\/ul><\/li><\/ul><p id=\"block236\">Your progression should be simple: First, focus on the fast/medium skills behind exploration and understanding with throwaway projects. Then, graduate to end-to-end projects where you can intentionally practice the deeper skills, and practice ideation and distillation too.<\/p><h3 id=\"What_is_research_taste_\" data-internal-id=\"What_is_research_taste_\">What is research taste?<\/h3><p id=\"block237\">A particularly important and fuzzy type of skill is called research taste. I basically think of this as the bundle of intuitions you get with enough research experience that let you do things like come up with good ideas, predict if an idea is promising, have conviction in good research directions, etc. Check out <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">my post on the topic<\/a>&nbsp;for more thoughts.<\/p><p id=\"block238\">I broadly think you should just ignore it for now, find ways to compensate for not having much yet, and focus on learning the fast-medium skills, and this will give you a much better base for learning it. In particular, it's much faster to learn with a mentor, so if you don't have a mentor at the start, you should prioritize other things.<\/p><p id=\"block239\">But you want to learn it eventually, so it's good to be mindful of it throughout, and look for opportunities to practice and learn lessons. I recommend treating it as a nice-to-have but not stressing about it<\/p><p id=\"block240\">Note, one important trap here is that having good taste can often manifest as having confidence and conviction in some research direction. But often novice researchers develop this confidence and conviction significantly <i>before <\/i>they develop the ability to not be confident in bad ideas. It’s often a good learning experience to once or twice pursue a thing you feel really convinced is going to be epic and then discover you're wrong, so it's not that bad an outcome, especially in stage 2 (mini-projects) but be warned.<\/p><h2 id=\"Stage_2__Practicing_Research_with_Mini_Projects\" data-internal-id=\"Stage_2__Practicing_Research_with_Mini_Projects\">Stage 2: Practicing Research with Mini-Projects<\/h2><p id=\"block241\">With that big picture in mind, let's get our hands dirty. You want to do a series of ~1-5 day mini-projects, for maybe 2-4 weeks. The goal right now is to learn the craft, not to produce groundbreaking research.<\/p><p id=\"block242\">Focus on practicing exploration and understanding and gaining the fast/medium skills, leave aside ideation and distillation for now. If you produce something cool and want to write it up, great! But that’s a nice-to-have, not a priority.<\/p><p id=\"block243\">Once you finish a mini-project, remember to do a post-mortem. Spend at least an hour analyzing: what did you do? What did you try? What worked? What didn't? What mistakes did you make? What would you do differently if doing this again? And how can you integrate this into your research strategy going forwards?<\/p><h3 id=\"Choose_A_Project\" data-internal-id=\"Choose_A_Project\">Choose A Project<\/h3><p id=\"block244\">Some suggested starter projects<\/p><ul><li id=\"block245\"><strong>Replicate and Extend a Paper:<\/strong>&nbsp;A classic for a reason. Replicate a key result, then extend it. Suggestions:<ul><li id=\"block246\"><a href=\"https://arxiv.org/abs/2406.11717\">Refusal is mediated by a single direction<\/a><ul><li id=\"block247\">Extending papers can vary a lot in difficulty. For example, applying the method to study refusal on a new model is easy as you can reuse the same data, while applying it to a new concept is harder.<\/li><li id=\"block248\">Skills: practicing activation patching and steering vectors.<\/li><\/ul><\/li><li id=\"block249\"><a href=\"http://thought-anchors.com\">Thought Anchors<\/a>: apply these reasoning model interpretability methods to new types of prompts, or explore some prompts using the linked interface, or see if you can improve on the methods/invent your own.<ul><li id=\"block250\">Skills: reasoning model interpretability, using LLM APIs, and working with modern models<\/li><\/ul><\/li><li id=\"block251\">Replicate the truth probes in <a href=\"https://arxiv.org/abs/2310.06824\">Geometry of Truth<\/a>&nbsp;on a more modern model and try applying them in more interesting settings. How well do they generalise? Can you break them? If so, can you fix this?<ul><li id=\"block252\">Skills: probing, supervised learning, dataset creation<\/li><\/ul><\/li><\/ul><\/li><li id=\"block253\"><strong>Play around with something interesting:<\/strong><ul><li id=\"block254\">Use <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia's attribution graphs<\/a>&nbsp;to form a hypothesis about Gemma 2B, then use other methods (e.g. prompting) to verify it.<ul><li id=\"block255\">Skills: Attribution graphs, scientific mindset, prompting<\/li><\/ul><\/li><li id=\"block256\">Play with <a href=\"https://huggingface.co/collections/bcywinski/gemma-2-9b-it-taboo-6826efbb186dfce0616dd174\">Bartosz Cywiński's taboo models<\/a>&nbsp;that have a secret word programmed in and test as many methods as you can to find it.<ul><li id=\"block257\">If you’re feeling ambitious: train your own models with a more complex secret, and try to interpret those.<\/li><li id=\"block258\">Skills: Logit lens, SAEs, black box methods<\/li><\/ul><\/li><li id=\"block259\">Explore <a href=\"https://github.com/clarifying-EM/model-organisms-for-EM\">the models<\/a>&nbsp;from the <a href=\"https://www.emergent-misalignment.com/\">emergent<\/a>&nbsp;<a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">misalignment<\/a>&nbsp;<a href=\"https://openai.com/index/emergent-misalignment/\">papers<\/a>.<ul><li id=\"block260\">Skills: steering vectors, SAEs, maybe fine-tuning<\/li><\/ul><\/li><li id=\"block261\">Pick some prompts from <a href=\"https://arxiv.org/abs/2503.08679\">Chain-of-Thought Reasoning In The Wild Is Not Always Faithful<\/a>&nbsp;and try to gain a deeper understanding of what’s happening<ul><li id=\"block262\">Skills: Open ended exploration, using whichever tools seem appropriate<\/li><\/ul><\/li><\/ul><\/li><\/ul><p id=\"block263\">Those cover two kinds of starter projects:<\/p><ul><li id=\"block264\"><strong>Understanding-heavy<\/strong>, where you take a well-known domain and try to test a hypothesis there (e.g. extending a paper you’ve read closely)<ul><li id=\"block265\">Note that you still want to do <i>some<\/i><\/li><\/ul><\/li><li id=\"block266\"><strong>Exploration-heavy<\/strong>, where you take some phenomena (a technique, a model, a phenomena, etc) play around with it, and try to understand what’s going on.<ul><li id=\"block267\">Exploration-heavy projects are often a less familiar style, so make sure to do some of those!<\/li><\/ul><\/li><\/ul><p id=\"block268\">Common mistakes:<\/p><ul><li id=\"block269\">People often get hung up on finding the “best” project. Sadly, that’s not going to happen. Instead, just do something and see what happens - better ideas and inspiration come with time.<\/li><li id=\"block270\">Don't get too attached to your first project. It was probably badly chosen! These are throwaway projects, just move on once you’re not learning as much.<\/li><li id=\"block271\">Conversely, don't flit between ideas so much that you never build your \"getting unstuck\" toolkit.<\/li><li id=\"block272\">Avoid compute-heavy and/papers (e.g., training cross-layer transcoders) or highly technical papers (e.g., Sparse Feature Circuits).<\/li><\/ul><h3 id=\"Practicing_Exploration\" data-internal-id=\"Practicing_Exploration\">Practicing Exploration<\/h3><p id=\"block273\">The idea of exploration as a phase in itself often trips up people new to mech interp. They feel like they always need to have a plan, a clear thing they're doing at any given point, etc. In my experience, you will often spend more than half of a project trying to figure out what the hell is happening and what you think your plan is. This is totally fine!<\/p><p data-internal-id=\"ftnt_ref14\" id=\"block274\">You don't need a plan. It's okay to be confused. However, this does <i>not <\/i>mean you should just screw around. Your North Star: gain information and surface area<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-noteref\" id=\"fnrefxw1ra5pqnd\"><sup><a href=\"#fnxw1ra5pqnd\">[14]<\/a><\/sup><\/span>&nbsp;on the problem. Your job is to take actions that maximise information gained per unit time. If you've learned nothing in 2 hours, pivot to another approach. If 2-3 approaches were dead ends, it’s fine to just pick another problem.<\/p><p id=\"block275\">I have <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">several research walkthroughs on my YouTube channel<\/a>&nbsp;that I think demonstrates the mindset of exploration. What I think is an appropriate speed to be moving. E.g. I think you should aim to make a new plot every few minutes (or faster!) if experiments don't take too long to run.<\/p><p id=\"block276\">A common difficulty is feeling “stuck” and not knowing what to do. IMO, this is largely a skill issue. Here's my recommended toolkit when this happens:<\/p><ul><li id=\"block277\">Use \"gain surface area\" techniques, things that can surface new ideas and connections and just give you raw data to work with: look at the model's output/chain-of-thought, change the prompt, probe for a concept, look at an SAE/attribution graph, read examples from your dataset, try logit lens or steering, etc.<\/li><li id=\"block278\">Set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5-minute timer<\/a>&nbsp;and brainstorm things you're curious about or directions to try.<\/li><li id=\"block279\">If you’re confused/curious about something, set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5 minute timer<\/a>&nbsp;and brainstorm what could be happening.<\/li><\/ul><p id=\"block280\">Other advice:<\/p><ul><li id=\"block281\">Before any &gt;30 minute experiment, stop and brainstorm alternatives. Is this <i>really<\/i>&nbsp;the fastest way to gain information?<\/li><li id=\"block282\">It's totally fine to pause for half a day to go learn some key background knowledge.<\/li><li id=\"block283\">Get in the habit of keeping a research log of your findings and a \"highlights\" doc for the really cool stuff.<ul><li id=\"block284\">If applicable, it can be cool to have your research log be a slack/discord channel<\/li><\/ul><\/li><li id=\"block285\">Remember: when exploring and thinking through how to explain mysterious phenomena, most of your probability mass should be on \"something I haven't thought of yet.\"<\/li><li id=\"block286\">Practice following your curiosity, but be aware that it’ll often lead you astray at first. When it does, pay attention! What can you learn from this?<\/li><\/ul><h3 id=\"Practicing_Understanding\" data-internal-id=\"Practicing_Understanding\">Practicing Understanding<\/h3><p id=\"block287\">If exploration goes well, you'll start to form hunches about the problem. E.g. thinking that you are successfully (linearly) probing for some concept. Or that you found a direction that mediates refusal. Or that days of the week are represented as a circle in a 2D subspace.<\/p><p id=\"block288\">Once you have this, you want to go to figure out if it's actually true. Be warned, the feeling of “being really convinced that it's true” is very different from actually being true. Part of being a good researcher is being good enough at testing and falsifying your pet hypotheses that, when you fail to falsify one, there’s a good chance that it's true. But you're probably not there yet.<\/p><p id=\"block289\">Note: While I find it helpful to think of these as discrete stages, often you'll be flitting back and forth. A great way to explore is coming up with guesses and micro-hypotheses about what's going on, running a quick experiment to test them, and integrating the results into your understanding of the problem, going back to the drawing board.<\/p><p id=\"block290\">Your North Star: convince yourself a hypothesis is true or false. The key mindset is skepticism. Advice:<\/p><ul><li id=\"block291\">Before testing a hypothesis, set a five-minute timer and brainstorm, \"What are the ways this could be false?\"<\/li><li id=\"block292\">Alternatively, write out the best possible case for your hypothesis and see where the argument feels weak.<ul><li id=\"block293\">Try using an LLM with an anti-sycophancy prompt (\"My friend wrote this and wants brutal feedback...\") to red-team your arguments - it probably won’t work, but might be helpful<\/li><\/ul><\/li><li id=\"block294\">Or set a 5 minute timer and brainstorm alternative explanations for your observations<\/li><\/ul><p id=\"block295\">You then want to convert these flaws and alternative hypotheses into concrete experiments. <strong>Experiment design is a deep skill<\/strong>. Honestly, I'm not sure how to teach it other than through experience. But one recommendation is to pay close attention to the experiments in papers you admire and analyze what made them so clever and effective. I also recommend that, every time you feel like you’ve (approximately) proven or falsified a hypothesis, adding them to a running doc of “things I believe to be true” with hypotheses, experiments, and results.<\/p><h3 id=\"Using_LLMs_for_Research_Code\" data-internal-id=\"Using_LLMs_for_Research_Code\">Using LLMs for Research Code<\/h3><p id=\"block296\">In my opinion, coding is one of the domains where LLMs are most obviously useful. It was very striking to me how much better my math scholars were six months ago than 12 months ago, and I think a good chunk of this is attributable by them having much better LLMs to use. If you are not using LLMs as a core part of your coding workflow, I think you're making a mistake.<\/p><ul><li id=\"block297\"><strong>Use<\/strong><a href=\"http://cursor.com/\"><strong>&nbsp;Cursor<\/strong><\/a><strong>:<\/strong>&nbsp;It's VS Code with fantastic AI integration. Make sure to add the docs for libraries with @&nbsp;so the AI has context. The $20/month plan is worth it, if possible, and there’s a <a href=\"https://cursor.com/students\">free student version<\/a>.<ul><li id=\"block298\">Claude Code is tempting but bad for learning and iteration. I’d use it for throwaway things and first drafts - if the draft has a bunch of bugs, go read the code yourself/throw it away and start again. Cursor facilitates reading the AI’s code better than Claude code does IMO<\/li><\/ul><\/li><li id=\"block299\"><strong>A caveat:<\/strong>&nbsp;If learning a new library (like in ARENA), first try writing things yourself. Use the LLM when stuck, not to replace the learning process.<\/li><li id=\"block300\">Later on, when thinking about writing up results, if key experiments were mostly vibe-coded, I recommend re-implementing them by hand to make sure no dumb LLM bugs slipped in.<\/li><\/ul><h2 id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\" data-internal-id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\">Interlude: What’s New In Mechanistic Interpretability?<\/h2><p id=\"block301\"><i>Feel free to skip to the “<\/i><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p id=\"block302\">Things move fast in mechanistic interpretability. Newcomers to the field who've kept up from afar are often pretty out of date. Here's what I think you need to know, again, filtered through my own opinions and biases.<\/p><h3 id=\"Avoiding_Fads\" data-internal-id=\"Avoiding_Fads\">Avoiding Fads<\/h3><p id=\"block303\">This interlude is particularly important because <strong>the field often has fads<\/strong>: lines of research that are very popular for a year or so, make some progress and find many limitations, and then the field moves on. But if you’re new, and catching up on the literature, you might not realise. I often see people new to the field working on older things, that I don’t think are too productive to work on any more. Historical fads include:<\/p><ul><li id=\"block304\">Interpreting toy models trained on algorithmic tasks (e.g. my <a href=\"https://arxiv.org/abs/2301.05217\">grokking work<\/a>)<ul><li id=\"block305\">I no longer recommend working on this, as I think we basically know that “sometimes models trained on algorithmic tasks are interpretable”, and they’re sufficiently artificial and divorced from real models that I am pessimistic about deeper and more specific insights generalising<\/li><\/ul><\/li><li id=\"block306\">Circuit analysis via causal interventions on model components (e.g. the <a href=\"https://arxiv.org/abs/2211.00593\">IOI paper<\/a>)<ul><li id=\"block307\">This is slightly more complicated. I think that's worth learning about, and techniques like activation and attribution patching are genuinely useful.<\/li><li id=\"block308\">But the core problem is that once you got a sparse subgraph of a model responsible for a task, there wasn't really a “what next?”. This didn't tend to result in deeper insight because the nodes (eg layers or maybe attention heads) weren't monosemantic, and it was often more complicated than naive stories suggested but we didn’t have the tools to dig deeper.<\/li><li id=\"block309\">It was pretty cool to see that this was possible at all, but there have been more than enough works in this area that the bar for a novel contribution is now much higher.<\/li><li id=\"block310\">Simply identifying a circuit is no longer enough; you need to use that circuit to reveal a deeper, non-obvious property of the model. I recommend exploring <a href=\"https://www.neuronpedia.org/graph/info\">attribution-graph style approaches<\/a><\/li><\/ul><\/li><li id=\"block311\"><p data-internal-id=\"ftnt_ref15\" id=\"block312\">We're at the tail end of a fad of incremental <a href=\"https://transformer-circuits.pub/2023/monosemantic-features\">sparse autoencoder research<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-noteref\" id=\"fnreftq4gws0zq69\"><sup><a href=\"#fntq4gws0zq69\">[15]<\/a><\/sup><\/span>&nbsp;(i.e. focusing on simple uses and refinements of the basic technique)<\/p><ul><li id=\"block313\"><p data-internal-id=\"ftnt_ref15\" id=\"block314\">Calling this one a fad is probably more controversial (if only because it's more recent).<\/p><\/li><li id=\"block315\"><p data-internal-id=\"ftnt_ref15\" id=\"block316\">The <i>specific<\/i>&nbsp;thing I am critiquing is the spate of papers, including ones I was involved in, that are about incremental improvements to the sparse autoencoder architecture, or initial demonstrations that you can apply SAEs to do things, or picking some downstream task and seeing what SAEs do on it.<\/p><ul><li id=\"block317\"><p data-internal-id=\"ftnt_ref15\" id=\"block318\">I think this made some sense when it seemed like SAEs could be a total gamechanger for the field, and where we were learning things from each new such paper. I think this moment has passed; I do not think they were a gamechanger in the way that I hoped they might be. See <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">more of my thoughts here<\/a>.<\/p><\/li><\/ul><\/li><li id=\"block319\"><p data-internal-id=\"ftnt_ref15\" id=\"block320\">I am <i>not<\/i>&nbsp;discouraging work on the following:<\/p><ul><li id=\"block321\"><p data-internal-id=\"ftnt_ref15\" id=\"block322\">Attribution graph-based circuit analysis, which I don't think has played out yet - see <a href=\"https://www.neuronpedia.org/graph/info\">a recent overview of that sub-field I co-wrote<\/a>.<\/p><\/li><li id=\"block323\"><p data-internal-id=\"ftnt_ref15\" id=\"block324\">Trying meaningfully different approaches to dictionary learning (eg <a href=\"https://arxiv.org/abs/2506.20790\">SPD<\/a>&nbsp;or <a href=\"https://arxiv.org/abs/2505.17769\">ITDA<\/a>), or things targeted to fix conceptual limitations of current techniques (eg <a href=\"https://arxiv.org/abs/2503.17547\">Matryoshka<\/a>).<\/p><\/li><li id=\"block325\"><p data-internal-id=\"ftnt_ref15\" id=\"block326\">Using SAEs as a tool, whether as part of a broader project investigating weird phenomena in model biology, or as a baseline/approach on some downstream task. The key is that the project’s motivation should <i>not <\/i>just be “what if we used SAEs for X?” unless there’s a good argument<\/p><ul><li id=\"block327\"><p data-internal-id=\"ftnt_ref15\" id=\"block328\">I particularly recommend them for tasks where you don’t know exactly what you’re looking for, e.g. trying to explore some mysterious phenomena<\/p><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><\/ul><p id=\"block329\">Note that I am putting this after stage 2 because I think that <strong>for initial throwaway projects you should <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;be stressing about novelty and avoiding fads<\/strong>&nbsp;- your goal is just to learn. But as we move into stage 3 you should start to be a bit more mindful about choosing more exciting/impactful projects where possible.<\/p><p id=\"block330\">Also, take these as nudges and recommendations, not as instructions. If there's a direction you believe in that fits the things I'm critiquing, maybe I'm just wrong, maybe your thing is an exception, go wild, see what happens.<\/p><h3 id=\"What_s_New_In_Mech_Interp_\" data-internal-id=\"What_s_New_In_Mech_Interp_\">What’s New In Mech Interp?<\/h3><p id=\"block331\">OK, so those are my hot takes on what not to do. What should you do? I think that some really cool new opportunities have opened up in mech interp over the last year, and newcomers may not have come across these. Here are some of the key themes in my favorite papers over the last year, that I’d love to see readers build on:<\/p><ol><li id=\"block332\"><strong>Downstream tasks<\/strong>. If our North Star is to be able to do useful things with interpretability, a great way to test our techniques is on real-world tasks, to see if they beat baselines or enable us to do a new thing<ol><li id=\"block333\">Even if we can't directly test on the tasks we most care about, if models can let us do new things in <i>some<\/i>&nbsp;domains, that's reasonable evidence that we're learning something real in a way that is hard to fake.<\/li><li id=\"block334\">One of my favourite examples is <strong>auditing games<\/strong>: <a href=\"https://arxiv.org/abs/2503.10965\">Marks et al<\/a>. is a fantastic paper that trained a model to have a hidden goal and then had different teams compete to find this goal, some with interpretability techniques, some without. Hypothesis: If interpretability does something real, then interp teams should win.<ol><li id=\"block335\">This study itself was a bit ambiguous (the mech interp teams were 1st and 3rd, a black box team was 2nd), but I think this is a fantastic way to test interpretability techniques, and has a ton of potential for follow-on work evaluating interpretability in different kinds of games.<\/li><li id=\"block336\">The general idea is just that there is a model with some known hidden property and different techniques are used to see which can elicit it. Things like having different teams, allowed to use different techniques, etc. is just one implementation.<\/li><\/ol><\/li><\/ol><\/li><li id=\"block337\"><p data-internal-id=\"ftnt_ref16\" id=\"block338\"><strong>Model organisms<\/strong>: The auditing games paper was made possible by the fact that they were able to make a model with a hidden goal<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-noteref\" id=\"fnrefcdmsagzbqkp\"><sup><a href=\"#fncdmsagzbqkp\">[16]<\/a><\/sup><\/span>, a model organism to study. In general, we’re collecting techniques like <a href=\"https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/\">synthetic document fine-tuning<\/a>&nbsp;to make really interesting model organisms.<\/p><ol><li id=\"block339\"><p data-internal-id=\"ftnt_ref16\" id=\"block340\">This kind of thing has a lot of potential! If we want to make a lie detector, a core challenge is that we don’t know how to test if it works or not. But if we can insert beliefs or deceptive behaviours into a model, many more projects become possible<\/p><\/li><li id=\"block341\"><p data-internal-id=\"ftnt_ref16\" id=\"block342\">A great intro project is playing around with open source model organisms, e.g. from <a href=\"https://arxiv.org/abs/2505.14352\">Cywinski et al<\/a><\/p><\/li><\/ol><\/li><li id=\"block343\"><strong>Practice on the real AGI Safety problems<\/strong>: Historically, interpretability could only practice on very dull toy problems like <a href=\"https://arxiv.org/abs/2301.05217\">modular addition<\/a>. But we now have models that exhibit complex behaviors that seem genuinely relevant to safety concerns, and we can just study them directly, making it far easier to make real progress.<ol><li id=\"block344\">E.g. <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">Rajamanoharan et al<\/a>&nbsp;debunking assumed self-preservation, and <a href=\"https://www.apolloresearch.ai/research/deception-probes\">Goldowsky-Dill et al<\/a>&nbsp;probing for deception<\/li><li id=\"block345\">Weird behaviours: models can <a href=\"https://www.apolloresearch.ai/research/deception-probes\">insider trade then lie about it<\/a>, <a href=\"https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations\">tell when they’re being evaluated<\/a>&nbsp;(and act differently), <a href=\"https://arxiv.org/abs/2412.14093\">fake alignment<\/a>, <a href=\"https://metr.org/blog/2025-06-05-recent-reward-hacking/\">reward hack<\/a>, and more.<\/li><\/ol><\/li><li id=\"block346\"><strong>Real-World Uses of Interpretability<\/strong>: Model interpretability-based techniques are starting to have genuine uses in frontier language models!<ol><li id=\"block347\"><a href=\"https://arxiv.org/abs/1610.01644\">Linear probes<\/a>, one of the simplest possible techniques, are a highly competitive way to <a href=\"https://alignment.anthropic.com/2025/cheap-monitors/\">cheaply monitor systems<\/a>&nbsp;for things like users trying to make bioweapons.<\/li><li id=\"block348\">I find it incredibly cool that interpretability can actually be useful, and kind of embarrassing that only a decade-old technique seems very helpful. Someone should do something about that. Maybe that someone could be you!<\/li><li id=\"block349\">This needs a very different kind of research: careful evaluation, comparison to strong baselines, and refinement of methods<\/li><\/ol><\/li><li id=\"block350\"><strong>Attribution graph-based circuit analysis<\/strong>. The core problem with trying to analyze circuits in terms of things like a model's attention heads and layers is that often these things don't actually have a clear meaning. <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\">Attribution graphs<\/a>&nbsp;use techniques like <a href=\"https://arxiv.org/abs/2406.11944\">transcoders<\/a>, popularized in <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/biology.html\">Anthropic's model biology<\/a>&nbsp;work, to approximate models with a computational graph with meaningful nodes.<ol><li id=\"block351\"><p data-internal-id=\"ftnt_ref17\" id=\"block352\">See this <a href=\"https://www.neuronpedia.org/graph/info\">cross-org blog post<\/a>&nbsp;for the ongoing follow-on work across the community, and an open problems list I co-wrote!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-noteref\" id=\"fnrefp0f0m03b55r\"><sup><a href=\"#fnp0f0m03b55r\">[17]<\/a><\/sup><\/span><\/p><\/li><li id=\"block353\">You can make and analyse your own attribution graphs on <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia<\/a><\/li><\/ol><\/li><li id=\"block354\"><strong>Understanding model failures<\/strong>: Models often do weird things. If we were any good at interpretability, we should be able to understand these. Recently, we’ve seen signs of life!<ol><li id=\"block355\"><a href=\"https://transluce.org/observability-interface\">Meng et al<\/a>&nbsp;on why some models think 9.8 &lt; 9.11<\/li><li id=\"block356\"><p data-internal-id=\"ftnt_ref18\" id=\"block357\">A line of work studying <a href=\"https://www.emergent-misalignment.com/\">emergent misalignment<\/a>&nbsp;- why training models on narrowly evil tasks like writing insecure code turns them into Nazis - has found some insights. <a href=\"https://arxiv.org/abs/2506.19823\">Wang et al<\/a>&nbsp;found this was driven by sparse autoencoder latents<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-noteref\" id=\"fnrefg12d8d1lqu\"><sup><a href=\"#fng12d8d1lqu\">[18]<\/a><\/sup><\/span>&nbsp;associated with movie villains, and in <a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">Turner et al<\/a>&nbsp;we found that the model <i>could <\/i>have learned the narrow solution, but this was in some sense less “efficient” and “stable”<\/p><\/li><\/ol><\/li><li id=\"block358\"><p data-internal-id=\"ftnt_ref20\" id=\"block359\"><strong>Automated interpretability<\/strong>: Using LLMs to automate interpretability. We saw signs of life on this from Bills et al and <a href=\"https://arxiv.org/abs/2404.14394\">Shaham et al<\/a>, but LLMs are actually good now! It’s now possible to make basic interpretability agents that can do things like <a href=\"https://alignment.anthropic.com/2025/automated-auditing/\">solve auditing games<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-noteref\" id=\"fnref0td6a2gxwht\"><sup><a href=\"#fn0td6a2gxwht\">[19]<\/a><\/sup><\/span>. And interpretability agents are the worst they’ll ever be<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-noteref\" id=\"fnref5bdglmkdzr\"><sup><a href=\"#fn5bdglmkdzr\">[20]<\/a><\/sup><\/span>.<\/p><\/li><li id=\"block360\"><p data-internal-id=\"ftnt_ref22\" id=\"block361\"><strong>Reasoning model interpretability<\/strong>: All current frontier models are reasoning models—models that are trained with reinforcement learning to think<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-noteref\" id=\"fnrefwuxdh4f7kh\"><sup><a href=\"#fnwuxdh4f7kh\">[21]<\/a><\/sup><\/span>&nbsp;for a while before producing an answer. In my opinion, this requires a major rethinking of many existing interpretability approaches<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-noteref\" id=\"fnref3qxoen8tddk\"><sup><a href=\"#fn3qxoen8tddk\">[22]<\/a><\/sup><\/span>, and calls for exploring new paradigms. IMO this is currently being neglected by the field, but will become a big deal.<\/p><ol><li id=\"block362\"><p data-internal-id=\"ftnt_ref22\" id=\"block363\">In <a href=\"http://thought-anchors.com\">Bogdan et al<\/a>, we explored what a possible paradigm could look like. Notably, there are far more interesting and sophisticated black box techniques with reasoning models, like resampling the second half of the chain of thought, or every time the model says a specific kind of sentence, deleting and regenerating that sentence.<\/p><\/li><\/ol><\/li><\/ol><h3 id=\"A_Pragmatic_Vision_for_Mech_Interp\" data-internal-id=\"A_Pragmatic_Vision_for_Mech_Interp\">A Pragmatic Vision for Mech Interp<\/h3><p id=\"block364\">Attentive readers may notice that the list above focuses on work to do with understanding the more qualitative high-level properties of models, and not ambitious reverse engineering. This is largely because, in my opinion, the former has gone great, while we have not seen much progress towards the fundamental blockers on the latter.<\/p><p id=\"block365\">I used to be very excited about ambitious reverse engineering, but I currently think that the dream of completely reverse engineering a model down to something human understandable seems basically doomed. My interpretation of the research so far is that models have some human understandable high-level structure that drives important actions, and a very long tail of increasingly niche and irrelevant heuristics and biases. For pragmatic purposes, these can be largely ignored, but not if we want things like guarantees, or to claim that we have understood most of a model. I think that trying to understand as much as we can is still a reasonable proxy for getting to the point of being pragmatically useful, but think it’s historically been too great a focus of the field, and many other approaches seem more promising if our ultimate goals are pragmatic.<\/p><p id=\"block366\">In some ways, this has actually made me more optimistic about interpretability ultimately being useful for AGI safety! Ambitious reverse engineering would be awesome but was always a long shot. But I think we've seen some real results for pragmatic approaches to mechanistic interpretability, and feel fairly confident we are going to be able to do genuinely useful things that are hard to achieve with other methods.<\/p><h2 id=\"Stage_3__Working_Up_To_Full_Research_Projects\" data-internal-id=\"Stage_3__Working_Up_To_Full_Research_Projects\">Stage 3: Working Up To Full Research Projects<\/h2><p id=\"block367\">Once you have a few mini-projects done, you should start being more ambitious. You want to think about gaining the deeper (medium/slow) skills, and exploring ideation and distillation.<\/p><p id=\"block368\">However, you should still expect projects to often fail, and want to lean into breadth over depth and avoid getting bogged down in an unsuccessful project you can’t bear to give up on. To resolve this tension, I recommend <strong>working in 1-2 week sprints<\/strong>. At the end of each sprint, reflect and make a deliberate decision: <strong>continue, or pivot?<\/strong>&nbsp;The default should be to pivot unless the project feels truly promising. It’s great to give up on things, if it means you spend your time even better! But if it’s going great, by all means continue.<\/p><p id=\"block369\">This strategy should mean that you eventually end up working on something longer-term when you find something <i>good<\/i>, but don't just get bogged down in the first ambitious idea you tried.<\/p><p id=\"block370\">I recommend reviewing the list of skills earlier and just for each one, reflecting for a bit on how on top of it you think you feel and how you could intentionally practice it in your next project. Then after each sprint, before deciding whether to pivot, take an hour or two to do a post-mortem: what did you learn, what progress did you make on different skills, and what would you do differently next time? Your goal is to learn, and you learn much better if you make time to actually process your accumulated data!<\/p><h3 id=\"Key_Research_Mindsets\" data-internal-id=\"Key_Research_Mindsets\">Key Research Mindsets<\/h3><p id=\"block371\">One way to decompose your learning is to think about research mindsets: the traits and mindsets a good researcher needs to have, that cut across many of these stages. See <a href=\"https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking\">my blog post on the topic for more<\/a>, but here's a brief view of how I'm currently thinking about it.<\/p><ol><li id=\"block372\"><p data-internal-id=\"ftnt_ref23\" id=\"block373\"><strong>Skepticism/Truth-seeking:<\/strong>&nbsp;The default state of the world is that your research is false, because doing research is hard. Your north star should always be to find <i>true <\/i>insights<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-noteref\" id=\"fnreflm5ixkfuzk\"><sup><a href=\"#fnlm5ixkfuzk\">[23]<\/a><\/sup><\/span><\/p><ol><li id=\"block374\"><p data-internal-id=\"ftnt_ref23\" id=\"block375\">It generally doesn't come naturally to people to constantly aggressively think about all the ways their work could be false and make a good faith effort to test it. You can learn to do better than this, but it often takes practice.<\/p><\/li><li id=\"block376\"><p data-internal-id=\"ftnt_ref23\" id=\"block377\">This is crucial in understanding, somewhat important in exploration, and crucial in distillation.<\/p><\/li><li id=\"block378\"><p data-internal-id=\"ftnt_ref23\" id=\"block379\">A common mistake is to grasp at straws to find a “positive” result, thinking that nothing else is worth sharing.<\/p><ol><li id=\"block380\"><p data-internal-id=\"ftnt_ref23\" id=\"block381\">In my opinion, negative or inconclusive results that are well-analyzed are much better than a poorly supported positive result. I’ll often think well of someone willing to release nuanced negative results, and poorly of someone who pretends their results are better than they are.<\/p><\/li><\/ol><\/li><\/ol><\/li><li id=\"block382\"><strong>Prioritization:<\/strong>&nbsp;Your time is scarce. Research involves making a bunch of decisions that are essentially searching through a high-dimensional space. The difference between a great and a mediocre researcher is being able to make these decisions well.<ol><li id=\"block383\">If you have a good mentor, you can lean on them for this at first, but you will need to learn how to do this yourself eventually.<\/li><li id=\"block384\">This is absolutely crucial in exploration and ideation, but fairly important throughout.<\/li><li id=\"block385\">A good way to learn this one is to reflect on decisions you've made after the fact, eg in a sprint post-mortem, and think about how you could have made them better, and what generalisable lessons to take to the future<\/li><\/ol><\/li><li id=\"block386\"><p data-internal-id=\"ftnt_ref24\" id=\"block387\"><strong>Productivity<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-noteref\" id=\"fnrefidab8074tka\"><sup><a href=\"#fnidab8074tka\">[24]<\/a><\/sup><\/span><strong>:<\/strong>&nbsp;The best researchers I've worked with get more than twice as much done as the merely good ones. Part of this is good research taste and making good prioritization decisions, but part of this is just being good at getting shit done.<\/p><ol><li id=\"block388\"><p data-internal-id=\"ftnt_ref24\" id=\"block389\">Now, this doesn't necessarily mean pushing yourself until the point of burnout by working really long hours. Or cutting corners and being sloppy. This is about productivity integrated over the long term.<\/p><ol><li id=\"block390\"><p data-internal-id=\"ftnt_ref24\" id=\"block391\">For example, sometimes the most productive thing to do is to hold off on starting work, set a 5 minute timer, brainstorm possible things to do next, and then pick the best idea<\/p><\/li><\/ol><\/li><li id=\"block392\"><p data-internal-id=\"ftnt_ref24\" id=\"block393\">This takes many forms, and the highest priority for you:<\/p><ol><li id=\"block394\"><p data-internal-id=\"ftnt_ref24\" id=\"block395\">Know when to write good code without bugs, to avoid wasting time debugging later, and when to write a hacky thing that just works.<\/p><\/li><li id=\"block396\"><p data-internal-id=\"ftnt_ref24\" id=\"block397\">Know the right keyboard shortcuts to move fast when coding.<\/p><\/li><li id=\"block398\"><p data-internal-id=\"ftnt_ref24\" id=\"block399\">Know when to ask for help and have people who can help you get unblocked where appropriate.<\/p><\/li><li id=\"block400\"><p data-internal-id=\"ftnt_ref24\" id=\"block401\">Be good at managing your time and tasks so that once you've decided what the highest priority thing to work on is, you in fact go and work on it.<\/p><\/li><li id=\"block402\"><p data-internal-id=\"ftnt_ref24\" id=\"block403\">Be able to make time to achieve deep focus on the key problems.<\/p><\/li><\/ol><\/li><li id=\"block404\"><p data-internal-id=\"ftnt_ref24\" id=\"block405\">Exercise: Occasionally <strong>audit your time<\/strong>. Use a tool like <a href=\"http://toggl.com\">Toggl<\/a>&nbsp;for a day or two to log what you're doing, then reflect: where did time go? What was inefficient? How could I do this 10% faster next time?<\/p><ol><li id=\"block406\"><p data-internal-id=\"ftnt_ref24\" id=\"block407\">The goal isn't to feel guilty, but to spot opportunities for improvement, like making a utility function for a tedious task.<\/p><\/li><\/ol><\/li><\/ol><\/li><li id=\"block408\"><strong>Knowing the literature<\/strong>: At this point, there’s a lot of accumulated wisdom (and a lot of BS) in prior papers, in mech interp and beyond.<ol><li id=\"block409\">This cuts across all stages:<ol><li id=\"block410\">In ideation, you don’t want to accidentally reinvent the wheel. And often great ideas are inspired by prior work<\/li><li id=\"block411\">In exploration, you want to be able to spot connections, borrow interesting techniques, etc<\/li><li id=\"block412\">In understanding, you want to know the right standards of proof to check for, the best techniques to use, alternative hypotheses (that may have been raised in other works), etc<\/li><li id=\"block413\"><p data-internal-id=\"ftnt_ref25\" id=\"block414\">In distillation, when writing a paper you’re expected to be able to contextualise it relative to existing work (i.e. write a related work section<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-noteref\" id=\"fnrefwpekmwudkpd\"><sup><a href=\"#fnwpekmwudkpd\">[25]<\/a><\/sup><\/span>) which is important for other researchers knowing whether to care. And if you don’t know the standard methods of proof, key baselines everyone will ask about, key gotchas to check for etc, no one will believe your work.<\/p><\/li><\/ol><\/li><li id=\"block415\">LLMs are an incredibly useful tool here. GPT-5 thinking or Claude 4 with web search are both pretty useful tools here, as are the slower but more comprehensive deep research tools (Note that Google's is available for free, as of the time of writing)<ol><li id=\"block416\">I recommend using these regularly and creatively throughout a project.<\/li><li id=\"block417\">You don't necessarily need to go and read the works that get surfaced, but even just having LLM summaries can get you more awareness of what's out there, and over time you'll build this into deeper knowledge.<\/li><\/ol><\/li><li id=\"block418\">Of course, when there <i>does<\/i>&nbsp;seem to be a very relevant paper to your work, you should go do a deep dive and read it properly, not just relying on LLM summaries.<\/li><li id=\"block419\">Don’t stress - deep knowledge of the literature takes time to build. But you want to ensure you’re on an upwards gradient here, rather than assuming the broader literature is useless<\/li><li id=\"block420\"><p data-internal-id=\"ftnt_ref26\" id=\"block421\">On the flip side, many papers <i>are <\/i>highly misleading/outright false, so please don’t just critically believe them!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-noteref\" id=\"fnref1bau7vsh9tk\"><sup><a href=\"#fn1bau7vsh9tk\">[26]<\/a><\/sup><\/span><\/p><\/li><\/ol><\/li><\/ol><p id=\"block422\">Okay, so how does this all tie back to the stages of research? Now you're going to be thinking about all four. We'll start by talking about how to deepen your existing skills with exploration and understanding, and then we'll talk about what practicing ideation and actually writing up your work should look like.<\/p><h3 id=\"Deepening_Your_Skills\" data-internal-id=\"Deepening_Your_Skills\">Deepening Your Skills<\/h3><p id=\"block423\">You’ll still be exploring and understanding, but with a greater focus on rigor and the slower skills. In addition to the thoughts when discussing mindsets above, here’s some more specific advice<\/p><ul><li id=\"block424\"><strong>Deeper Exploration<\/strong>&nbsp;is about internalizing the mindset of maximising productivity, which here means maximising information gain per unit time. Always ask, \"Am I learning something?\"<ul><li id=\"block425\"><i>Avoid Rabbit Holes:<\/i>&nbsp;A common mistake is finding one random anomaly and zooming in on it. Knowing when to pivot is crucial. Set a timer every hour or two to zoom out and ask if you’re making progress.<ul><li id=\"block426\">I recommend any time you notice yourself feeling a bit stuck or distracted or off track, setting a five minute timer and thinking about what could I be doing next, what should I be doing next, and am I doing the most important thing?<\/li><\/ul><\/li><li id=\"block427\"><i>Avoid Spreading Yourself Too Thin:<\/i>&nbsp;Doing lots of things superficially means none of them will be interesting.<\/li><li id=\"block428\">If you have spent more than five hours without learning something new, you should probably try a different approach<ul><li id=\"block429\">And if you have spent more than two days without learning something new, you should seriously consider pivoting and doing something else.<\/li><\/ul><\/li><li id=\"block430\">To practice prioritization, be intentional about your decisions: write down <i>why<\/i>&nbsp;you think an experiment is the right call, and later reflect on whether you were right. This makes your intuitions explicit and easier to update.<\/li><\/ul><\/li><li id=\"block431\"><strong>Deeper Understanding<\/strong>&nbsp;is about practicing skepticism and building a bulletproof case. Red-team your results relentlessly.<ul><li id=\"block432\">Some experiments are much more impactful and informative than others! Don't just do the first experiment that pops into your head. Think about the key ways the hypothesis <i>could <\/i>be false, and how you could test that. Or about whether a skeptic could explain away a positive experimental results<ul><li id=\"block433\">A useful exercise is imagining you're talking to a really obnoxious skeptic who keeps complaining that they don't believe you and coming up with arguments for why your thing is wrong. What could you do such that they don't have a leg to stand on?<\/li><\/ul><\/li><li id=\"block434\">Of course, there's also an element of prioritization. Sometimes a shallow case that could be wrong is the right thing to aim for, if you’re working on an unimportant side claim/something that seems super plausible on priors, at which point you should just move on and do something else more interesting.<\/li><li id=\"block435\">Exercise: To practice spotting subtle illusions, try red-teaming papers you read, thinking about potential flaws, and ideally run the experiments yourself.<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Doing_Good_Science\" id=\"block436\">Doing Good Science<\/p><ul><li id=\"block437\"><strong>Avoid cherry-picking<\/strong>: Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling.<ul><li id=\"block438\">When you write up work, always include some randomly selected examples, especially if you present extensive qualitative analysis of specific things. It's fine to put this in the appendix if space is scarce, but it should be there.<\/li><\/ul><\/li><li id=\"block439\"><strong>Use baselines<\/strong>: A common mistake is for people to try to show a technique works by demonstrating it gets 'decent' results, rather than showing it achieves better results than plausible alternatives that people might have used or are standard in the field. If you want people to e.g. use your cool steering vector results you need to show it beats changing the system prompt.<\/li><li id=\"block440\"><strong>Don’t sandbag your baselines<\/strong>: Similarly, it's easy to put in much more effort finding good hyperparameters for your technique than for your baselines. Try to make sure you're achieving comparable results with your baselines that prior work in the field has.<\/li><li id=\"block441\"><strong>Do ablations on your fancy method<\/strong>: It's easy for people to have a fancy method with lots of moving parts, when many actually are unnecessary. You should always try removing one part and see if the method breaks. Do this for each part.<ul><li id=\"block442\">For example, the <a href=\"https://arxiv.org/abs/2403.03218v1\">original unlearning method<\/a>&nbsp;in the <a href=\"https://arxiv.org/abs/2403.03218\">RMU paper<\/a>&nbsp;claimed it was based on finding a meaningful steering vector, until follow-up work found that it was just about adding a vector with really high norm that broke the model, and a random vector performed just as well.<\/li><\/ul><\/li><li id=\"block443\"><strong>(Informally) pre-register claims<\/strong>: It's important to clearly track which experimental results were obtained before versus after you formulated your claim. Post-hoc analysis (interpreting results after they're seen) is inherently less impressive than predictions confirmed by pre-specified experiments<\/li><li id=\"block444\"><strong>Be reproducible<\/strong>: Where practical, share your code, data and models.<ul><li id=\"block445\">If you have time, make sure that it runs on a fresh machine and include a helpful readme that links to key model weights and datasets.<\/li><li id=\"block446\"><p data-internal-id=\"ftnt_ref27\" id=\"block447\">This both means others can check if your work is true and makes it more likely people will believe and build on your work<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-noteref\" id=\"fnrefadytzr5d7y\"><sup><a href=\"#fnadytzr5d7y\">[27]<\/a><\/sup><\/span>&nbsp;because they can see replications that are more likely to exist and because it's now low friction.<\/p><\/li><\/ul><\/li><li id=\"block448\"><strong>Simplicity:<\/strong>&nbsp;Bias towards trying the simple, obvious methods first. Fancy techniques can be a trap. Good research is pragmatic, not about showing off.<ul><li id=\"block449\">If you’re designing a fancy technique/experiment, each new detail is one more thing that can break<\/li><li id=\"block450\">If trying to explain something mysterious, novice researchers often neglect simple, dumb hypotheses like “maybe MLP0 is incredibly important on <i>every <\/i>input, and there’s nothing special going on with my prompt”<\/li><\/ul><\/li><li id=\"block451\"><strong>Be qualitative <\/strong><i><strong>and <\/strong><\/i><strong>quantitative<\/strong>: One of the major drivers of progress of modern machine learning is being quantitative, having benchmarks and showing that a technique increases numbers on them. One of the key drivers of progress in mech interp is an openness to qualitative research: summary statistics lose a ton of information. What can we learn by actually looking deeply into what's happening?<ul><li id=\"block452\">In my opinion, the best research tries to get the best of both worlds. It tries to understand what's happening via qualitative analysis and then validates it with more quantitative methods. If your paper only does one, it’s probably missing out<\/li><\/ul><\/li><li id=\"block453\"><strong>Read your data<\/strong>: A fantastic use of time, especially during the exploration phase, is just actually reading the data you're working with, or model chains of thought and responses.<ul><li id=\"block454\">Often, the quality of the data is a crucial driver of the results of your experiments. Often, it is quite bad.<\/li><li id=\"block455\">Sometimes most of the work of a project is in noticing flaws in your data and making a better data set. Time figuring this out is extremely well spent.<\/li><li id=\"block456\">Ditto, include random examples of the data in an appendix for readers to do spot checks of their own.<\/li><\/ul><\/li><li id=\"block457\"><p data-internal-id=\"ftnt_ref28\" id=\"block458\"><strong>Don’t reinvent the wheel<\/strong>: &nbsp;A common mistake in mech interp is doing something that's already been done<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-noteref\" id=\"fnrefva7mhfkrhm\"><sup><a href=\"#fnva7mhfkrhm\">[28]<\/a><\/sup><\/span>. We have LLM-powered literature reviews now. You have way less of an excuse. Check first!<\/p><\/li><li id=\"block459\"><strong>Excitement is evidence of bullshit<\/strong>: Generally, most true results are not exciting, but a fair amount of false results are. So from a Bayesian perspective, if a result is exciting and cool, it’s even more likely to be false than normal!<ul><li id=\"block460\">Resist the impulse to get really excited! The correct attitude to exciting results is deep skepticism until you have tried really hard to falsify it and run out of ideas.<\/li><\/ul><\/li><\/ul><h3 id=\"Practicing_Ideation\" data-internal-id=\"Practicing_Ideation\">Practicing Ideation<\/h3><p id=\"block461\">Okay, so you want to actually come up with good research ideas to work on. What does this look like? I recommend breaking this down into <strong>generating ideas<\/strong>&nbsp;and then <strong>evaluating <\/strong>them to find the best ones.<\/p><p id=\"block462\">To generate ideas, I'd often start with just taking a blank doc, blocking out at least an hour, and then just writing down as many ideas as you can come up with. Aim for quantity over quality. Go for at least 20.<\/p><p id=\"block463\">There are other things you can do to help with generation:<\/p><ul><li id=\"block464\">Throughout your previous sprints, every time you had an idle curiosity or noticed something weird, write it down in one massive long-running doc.<\/li><li id=\"block465\">Likewise, when reading papers, note down confusions, curiosities, obviousnesses to do.<\/li><\/ul><p id=\"block466\">Okay, so now you have a big list. What does finding the best ones look like?<\/p><ul><li id=\"block467\">Ideally, if you have a mentor or at least collaborators, you can just ask them to rate them.<ul><li id=\"block468\">If you do this, rate them yourself privately out of 10 before you look at their responses. Compare them and every time you have substantially different numbers, talk to the mentor and try to figure out why your intuitions disagree. This is a great source of supervised data for research taste.<\/li><\/ul><\/li><li id=\"block469\">Even if you don’t have a mentor, I think that just going through, rating each idea yourself based on gut feel and sorting is as good a way to prune down a long list as any<\/li><li id=\"block470\">For the top few, I recommend trying to answer a few questions about them.<ul><li id=\"block471\">What would success look like here?<\/li><li id=\"block472\">How surprised would I be if I did this for a month and nothing interesting had happened?<\/li><li id=\"block473\">What skills does this require? Do I have them/could I easily gain them?<\/li><li id=\"block474\">What models, data, computational resources, etc. does this require?<\/li><li id=\"block475\">How does this compare to what the most relevant prior work did? Can I check for prior work and see if anything relevant comes up?<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Research_Taste_Exercises\" id=\"block476\">Research Taste Exercises<\/p><p id=\"block477\">Gaining research taste is slow because the feedback loops are long. You can accelerate it with exercises that give you faster, proxy feedback. (Credit to <a href=\"https://colah.github.io/notes/taste/\">Chris Olah for inspiration here<\/a>)<\/p><ul><li id=\"block478\">If you have a mentor, query their taste for fast data and try to imitate it. Concretely:<ul><li id=\"block479\">Before each meeting, write a list of questions, then try to write up predictions for what the mentor will say, then actually ask the mentor, see what happens, and compare. If there are discrepancies, chat to the mentor and try to understand why.<\/li><li id=\"block480\">Likewise, if the mentor makes a suggestion or asks a question you didn't expect, try to ask questions about where the thought came from.<\/li><li id=\"block481\"><p data-internal-id=\"ftnt_ref29\" id=\"block482\">Regularly paraphrase back to the mentor in your own words what you think they're saying, and then ask them to correct anything you're wrong about<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-noteref\" id=\"fnreftt0owz8koks\"><sup><a href=\"#fntt0owz8koks\">[29]<\/a><\/sup><\/span><\/p><\/li><\/ul><\/li><li id=\"block483\"><strong>Learning from papers as \"offline data\":<\/strong>&nbsp;When you read a paper, don't just passively consume it. Read the introduction, then stop. Try to predict what methods they used and what their key results will be. Then, continue reading and see how your predictions compare. Analyze why the authors made different choices. This trains your intuition on a much larger and faster dataset than your own research.<\/li><\/ul><p id=\"block484\">It’s also worth dwelling on what research taste actually is. <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">See my post<\/a>&nbsp;for more, but I break it down as follows:<\/p><ol><li id=\"block485\"><strong>Intuition (System 1):<\/strong>&nbsp;This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.<\/li><li id=\"block486\"><strong>Conceptual Framework (System 2)<\/strong>: This is deep domain knowledge and understanding of underlying principles.<\/li><li id=\"block487\"><strong>Strategic Big Picture<\/strong>: Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?<\/li><\/ol><h3 id=\"Write_up_your_work_\" data-internal-id=\"Write_up_your_work_\">Write up your work!<\/h3><p id=\"block488\">At this stage, you should be thinking seriously about how to write up your work. Often, writing up work is the first time you really understand what a project has been about, or you identify key limitations, or experiments you forgot to do. You should check out <a href=\"https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers\">my blog post on writing ML papers<\/a>&nbsp;for much more detailed thoughts (which also apply to high-effort blog posts!) but I'll try to summarize them below.<\/p><p data-internal-id=\"Why_aim_for_public_output_\" id=\"block489\">Why aim for public output?<\/p><p data-internal-id=\"ftnt_ref30\" id=\"block490\">If producing something public is intimidating, for now, you can start by just writing up a private Google Doc and maybe share it with some friends or collaborators. But I heavily encourage people to aim for public output where they can. Generally, your research will not matter if no one reads it. The goal of research is to contribute to the sum of human<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-noteref\" id=\"fnrefe3252d8idmr\"><sup><a href=\"#fne3252d8idmr\">[30]<\/a><\/sup><\/span>&nbsp;knowledge. And if no one understands what you did, then it doesn't matter.<\/p><p id=\"block491\">Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, <strong>public research output is your best credential<\/strong>. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc. It doesn’t matter if you wrote it in a prestigious PhD program or as a random independent researcher, if it’s good enough then people care.<\/p><p id=\"block492\">There are a few options for what this can look like:<\/p><ul><li id=\"block493\">A blog post (e.g. on a personal blog or LessWrong) - the simplest and least formal kind<\/li><li id=\"block494\"><p data-internal-id=\"ftnt_ref31\" id=\"block495\">An Arxiv paper - much more legible than a blog post, and honestly not much extra effort if you have a high-quality blog post<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-noteref\" id=\"fnref8354hd0flji\"><sup><a href=\"#fn8354hd0flji\">[31]<\/a><\/sup><\/span><\/p><\/li><li id=\"block496\"><p data-internal-id=\"ftnt_ref32\" id=\"block497\">A workshop paper<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-noteref\" id=\"fnref9oppcf0ftrh\"><sup><a href=\"#fn9oppcf0ftrh\">[32]<\/a><\/sup><\/span>&nbsp;(i.e. something you submit for peer review to a workshop, typically part of a major ML conference, the bar is much lower than for a conference paper)<\/p><\/li><li id=\"block498\"><p data-internal-id=\"ftnt_ref34\" id=\"block499\">A conference paper (the equivalent of top journals in ML, there’s a reasonably high quality bar<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-noteref\" id=\"fnreffmh579omuc6\"><sup><a href=\"#fnfmh579omuc6\">[33]<\/a><\/sup><\/span>, but also a <i>lot <\/i>of noise<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-noteref\" id=\"fnreff09vsa4w37e\"><sup><a href=\"#fnf09vsa4w37e\">[34]<\/a><\/sup><\/span>)<\/p><\/li><\/ul><p id=\"block500\">If this all seems overwhelming, starting out with blog posts is fine, but I think people generally overestimate the bar for arxiv or workshop papers - if you think you learned something cool in a project, this is totally worth turning into a paper!<\/p><p data-internal-id=\"How_to_write_stuff_up_\" id=\"block501\">How to write stuff up?<\/p><p id=\"block502\">The core of a paper is the narrative. Readers will not take away more than a few sentences worth of content. Your job is to make sure these are the right handful of sentences and make sure the reader is convinced of them.<\/p><p id=\"block503\">You want to distill your paper down into one to three key claims (your contribution), the evidence you provide that the contribution is true, the motivation for why a reader should care about them, and work all of this into a coherent narrative.<\/p><p id=\"block504\"><strong>Iterate<\/strong>: I'm a big fan of writing things iteratively. You first figure out the contribution and narrative. You then write a condensed summary, the abstract (in a blog post, this should be a TL;DR/executive summary - also very important!). You then write a bullet point outline of the paper: what points you want to cover, what evidence you want to provide, how you intend to build up to that evidence, how you want to structure and order things, etc. If you have mentors or collaborators, the bullet point outline is often the best time to get feedback. Or the narrative formation stage, if you have an engaged mentor. Then write the introduction, and make sure you’re happy with that. Then (or even before the intro) make the figures - figures are incredibly important! Then flesh it out into prose. People spend a <i>lot<\/i>&nbsp;more time reading the abstract and the intro than the main body, especially when you account for all the people who read the abstract and then stop. So you should spend a lot more time per unit word on those.<\/p><p id=\"block505\"><strong>LLMs<\/strong>: I think LLMs are a really helpful writing tool. They're super useful for getting feedback, especially if writing in an unfamiliar style like an academic ML paper may be for you. Remember to use anti-sycophanty prompts so you get real feedback. However, it's often quite easy to tell when you're reading LLM written slop. So use them as a tool, but don't just have them write the damn thing for you. But if you e.g. have writer’s block, having an LLM help you brainstorm or produce a first draft for inspiration, and can be very helpful.<\/p><p data-internal-id=\"Common_mistakes\" id=\"block506\">Common mistakes<\/p><ul><li id=\"block507\"><strong>The reader does not have context<\/strong>: Your paper will be clear in your head, because you have just spent weeks to months steeped in this research project. The reader has not. You will overestimate how clear things are to the reader, and so you should be massively erring in the other direction and spelling everything out as blatantly as possible.<ul><li id=\"block508\"><strong>This is an incredibly common mistake<\/strong>&nbsp;- assume it will happen to you<\/li><li id=\"block509\">The main solution is to get feedback from people with enough research context that they can actually engage and who are also willing to give you substantial negative feedback.<ul><li id=\"block510\">Notice the feeling of surprise when people are confused by something you thought was clear. Try to understand why they were confused and iterate on fixing it until it's clear.<\/li><\/ul><\/li><\/ul><\/li><li id=\"block511\"><strong>Writing is not an afterthought<\/strong>: People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute.<\/li><li id=\"block512\"><strong>Acknowledge limitations<\/strong>: There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing<\/li><li id=\"block513\"><strong>Good writing is simple<\/strong>: There's a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they feel impressive. I think this is a highly ineffective strategy<\/li><li id=\"block514\"><strong>Remember to motivate things<\/strong>: It will typically not be obvious to the reader why your paper matters or is interesting. They do not have the context you do. It is your job to convince them, ideally in the abstract or perhaps intro, why they should care about your work, lest they just give up and stop reading.<\/li><\/ul><p data-internal-id=\"h.sk0e3iwce7ck\" id=\"block515\">&nbsp;<\/p><h2 id=\"Mentorship__Collaboration_and_Sharing_Your_Work\" data-internal-id=\"Mentorship__Collaboration_and_Sharing_Your_Work\">Mentorship, Collaboration and Sharing Your Work<\/h2><p id=\"block516\">A common theme in the above is that it's incredibly useful to have a mentor, or at least collaborators. Here I'll try to unpack that and give advice about how to go about finding one.<\/p><p id=\"block517\">Though it's also worth saying that many mentors are not actually great researchers and may have bad research taste or research taste that's not very well suited to mech interp. What you do about this is kind of up to you.<\/p><h3 id=\"So_what_does_a_research_mentor_actually_do_\" data-internal-id=\"So_what_does_a_research_mentor_actually_do_\">So what does a research mentor actually do?<\/h3><p id=\"block518\">A good mentor is an incredible accelerator. Dysfunctional as academia is, there is a reason it works under the apprenticeship-like system of PhD students and supervisors. When I started supervising, I was very surprised at how much of a difference a weekly check in could make! Here’s my best attempt to breakdown how a good mentor can add value:<\/p><ul><li id=\"block519\"><strong>Suggest research ideas<\/strong>&nbsp;when you're starting out, letting you bypass the hardest skill (ideation) to focus on execution.<\/li><li id=\"block520\"><strong>Help you prioritize<\/strong>&nbsp;which experiments to run, lending you their more experienced judgment, so you get more done.<\/li><li id=\"block521\"><p data-internal-id=\"ftnt_ref35\" id=\"block522\"><strong>When to pivot<\/strong>: if your research direction isn’t working out, having a mentor to pressure you to pivot can be extremely valuable<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-noteref\" id=\"fnref7ruxx269r2s\"><sup><a href=\"#fn7ruxx269r2s\">[35]<\/a><\/sup><\/span><\/p><\/li><li id=\"block523\"><strong>Provide supervised data for research taste<\/strong>: For the slow/very-slow skills like coming up with research ideas, and prioritization, a <i>far <\/i>faster way to gain them at first is by learning to mimic your mentor’s.<\/li><li id=\"block524\"><strong>Act as an interface to the literature<\/strong>: pointing you to the relevant work before you've built up deep knowledge yourself. Flagging standard baselines, standard metrics, relevant techniques, prior work so you don’t reinvent the wheel, etc.<\/li><li id=\"block525\"><strong>Red-team your results<\/strong>, helping you spot subtle interpretability illusions and flaws in your reasoning that you're too close to see.<\/li><li id=\"block526\"><strong>Point out skills you're missing<\/strong>&nbsp;that you didn't even notice were skills. Generally guide your learning and help you prioritize<\/li><li id=\"block527\"><strong>Walk you through communicating your work<\/strong>, helping you distill your findings and present them clearly to the world.<\/li><li id=\"block528\"><strong>Motivation/accountability<\/strong>: Many find it extremely helpful to have someone, even if very hands-off, who they present work to, so they feel motivated and accountable (especially if they e.g. want to impress the mentor, want a job, etc. Of course, these also increase stress!)<ul><li id=\"block529\">To those prone to analysis paralysis, being able to defer to a mentor on uncertain decisions can be highly valuable<\/li><\/ul><\/li><li id=\"block530\"><strong>References<\/strong>: Having a mentor who can vouch for your skill is very helpful, especially if they know people who may be hiring you in future.<\/li><\/ul><h3 id=\"Advice_on_finding_a_mentor\" data-internal-id=\"Advice_on_finding_a_mentor\">Advice on finding a mentor<\/h3><p id=\"block531\">Here are some suggested ways to get some mentorship while transitioning into the field. I discuss higher commitment ways, like doing a PhD or getting a research job, below.<\/p><p id=\"block532\">Note: whatever you do to find a mentor, having evidence that you can do research yourself, that is, public output that demonstrates ability to self-motivate and put in effort, and ideally demonstrates actually interesting research findings, is incredibly helpful and should be a priority.<\/p><p data-internal-id=\"Mentoring_programs\" id=\"block533\">Mentoring programs<\/p><p id=\"block534\">I think mentoring programs like <a href=\"http://matsprogram.org\">MATS<\/a>&nbsp;are an incredibly useful way into the field, you typically do a full-time, several month program where you write a paper, with weekly check-ins with a more experienced researcher. Your experience will vary wildly depending on mentor quality, but at least for my MATS scholars, often people totally new to mech interp can publish a top conference paper in a few months. See my MATS application doc for a bunch more details.<\/p><p id=\"block535\">There’s <strong>a wide range of backgrounds<\/strong>&nbsp;among people who do them and get value - people totally new to a field, people with 1+ years of interpretability research experience who want to work with a more experienced mentor, young undergrads, mid-career professionals (including a handful of professors), and more.<\/p><p id=\"block536\"><a href=\"http://matsprogram.org\">MATS 9.0<\/a>&nbsp;applications are open, due <strong>Oct 2 2025<\/strong>, and <a href=\"http://tinyurl.com/neel-mats-app\">mine<\/a>&nbsp;close on <strong>Sept 12<\/strong>.<\/p><p id=\"block537\">Other programs (which I think are generally lower quality than MATS, but often still worth applying to depending on the mentor)<\/p><ul><li id=\"block538\"><i>Full-time/In-person:<\/i><a href=\"https://www.matsprogram.org/\">&nbsp;MATS<\/a>, <a href=\"https://www.pivotal-research.org/fellowship\">Pivotal<\/a>, <a href=\"https://www.lasrlabs.org/\">LASR<\/a>, <a href=\"https://pibbss.ai/fellowship/\">PIBBSS<\/a><\/li><li id=\"block539\"><i>Part-time/Remote:<\/i><a href=\"https://www.cambridgeaisafety.org/mars\">&nbsp;<\/a><a href=\"https://sparai.org/\">SPAR<\/a>, <a href=\"https://www.cambridgeaisafety.org/mars\">MARS<\/a><\/li><\/ul><p data-internal-id=\"Cold_emails\" id=\"block540\">Cold emails<\/p><p id=\"block541\">You can also take matters into your own hands and try to convince someone to be your mentor. Reaching out to people, ideally via a warm introduction, but even just via a cold email, can be highly effective. However, I get lots of cold emails and I think many are not very effective, so here's some advice:<\/p><ul><li id=\"block542\"><strong>Don't just email the most prominent people<\/strong>. A lot of people will just email the most prominent people in the field and ask for mentorship. This is a bad plan! These people are very busy and they also get lots of emails. I just reflexively respond to any email requesting mentorship with “please apply to my MATS cohort”.<ul><li id=\"block543\">However, there are lots of less prominent people who can provide a bunch of useful mentorship. These people are much more likely to be excited to get a cold email, to have time to engage, potentially even the spare capacity to properly mentor a project.<\/li><li id=\"block544\">I think that many people who've recently joined my team or people who worked on a great paper with me during MATS are able to add a lot of value to people new to the field. And I would recommend reaching out to them!<ul><li id=\"block545\">For example, Josh Engels, a new starter on my team, said he would happily receive more cold emails (as of early Sept 2025).<\/li><li id=\"block546\">As a general heuristic, email first authors of papers, not fancy last authors.<\/li><\/ul><\/li><\/ul><\/li><li id=\"block547\"><strong>Start small<\/strong>: Don't email someone you've never interacted with before asking if they want to kind of officially mentor you on some project. That's a big commitment.<ul><li id=\"block548\">It's much better to be like, I'd be interested in having a chat about your paper or my work building on your paper.<\/li><li id=\"block549\">Or just asking if they're down to have a chat giving you feedback on some project ideas, etc.<\/li><li id=\"block550\">And if this goes well, it may organically turn into a more long-term mentoring relationship!<\/li><\/ul><\/li><li id=\"block551\"><strong>Proof of work<\/strong>: Demonstrate that you are actually interested in this person specifically, not just spamming tons of people.<ul><li id=\"block552\">Show that you've engaged with their work, say something intelligent about it, have some questions.<ul><li id=\"block553\">In the era of LLMs, this is less of a costly signal that you've actually taken an interest in this person specifically than it used to be, admittedly<\/li><li id=\"block554\">But linking to some research you did building on their work I think is still reasonably costly, and very flattering to people.<\/li><\/ul><\/li><\/ul><\/li><li id=\"block555\"><strong>Prioritize aggressively<\/strong>. Assume the reader will stop reading at any moment, so put your most critical and impressive information first.<\/li><li id=\"block556\"><strong>Explain who you are<\/strong>: If you're emailing someone who gets more emails than they have capacity to respond to, they're going to be prioritizing. A key input into this is just who you are, what have you done, have you done something interesting that shows promise, do you have relevant credentials, etc. I personally find it very helpful if people just say the most impressive things about them in the first sentence or two.<ul><li id=\"block557\">To do this without seeming arrogant, you could try: \"I'm sure you must get many of these emails. So to help you prioritise, here's some key info about me\"<\/li><\/ul><\/li><li id=\"block558\">Use <strong>bolding<\/strong>&nbsp;for key phrases to make your email easily skimmable.<\/li><li id=\"block559\"><strong>Be concise<\/strong>. One thing I would often appreciate is a short blurb summarizing your request with a link to a longer document for details if I'm interested.<\/li><li id=\"block560\"><strong>Quick requests<\/strong>: Generally, my flow when reading emails is that I will either immediately respond or never look at it again. I'm a lot more likely to immediately respond if I can do so quickly. If you do want to email a busy person, have a clear, concrete question up front that they might be able to help with.<\/li><\/ul><h3 id=\"Community___collaborators\" data-internal-id=\"Community___collaborators\">Community &amp; collaborators<\/h3><p id=\"block561\">Much easier than finding a mentor is finding collaborators, other people to work on the same project with, or just other people also trying to learn more about mech interp, who you can chat with and give each other feedback:<\/p><ul><li id=\"block562\"><strong>In-Person:<\/strong>&nbsp;Local AI Safety hubs (London, Bay Area, etc.), University groups, ML conferences (e.g., the<a href=\"http://mechinterpworkshop.com/\">&nbsp;NeurIPS Mech Interp workshop<\/a>&nbsp;I co-organize), EAG/EAGx conferences.<ul><li id=\"block563\">If you’re a student, see if there’s a lab at your university that has some people interested in interpretability. There may be interested PhD students even if no professor works on it<\/li><\/ul><\/li><li id=\"block564\"><strong>Online<\/strong>: These are also good places to meet people! I recommend sharing work for feedback, or just asking about who’s interested in what you’re interested in, and trying to DM the people who engage/seem interested, and seeing what happens<ul><li id=\"block565\"><a href=\"https://www.neelnanda.io/osmi-slack-invite\">Open Source Mechanistic Interpretability Slack<\/a><\/li><li id=\"block566\"><a href=\"https://discord.gg/nHS4YxmfeM\">Eleuther Discord<\/a>&nbsp;(interpretability-general)<\/li><li id=\"block567\"><a href=\"https://discord.gg/ysVfhCfCKw\">Mech Interp Discord<\/a><\/li><\/ul><\/li><\/ul><p id=\"block568\"><strong>Staying up to date<\/strong>: Another common question is how to stay up to date with the field. Honestly, I think that people new to the field should not worry that much about this. Most new papers are irrelevant, including the ones that there is hype around. But it's good to stay a little bit in the loop. Note that the community has substantial parts both in academia and outside, which are often best kept up with in different ways.<\/p><ul><li id=\"block569\">LessWrong and the AlignmentForum are a reasonable place to keep up to date with the less academic half<\/li><li id=\"block570\">Twitter is a confusing, chaotic place that is an okay place to keep up with both. It's a bit unclear who the right people to follow.<ul><li id=\"block571\"><a href=\"http://x.com/ch402\">Chris Olah<\/a>&nbsp;doesn't tweet much, but it's high quality when he does.<\/li><li id=\"block572\"><a href=\"http://x.com/neelnanda5\">I will tweet<\/a>&nbsp;about all of my interpretability work and sometimes others.<\/li><\/ul><\/li><\/ul><h2 id=\"Careers\" data-internal-id=\"Careers\">Careers<\/h2><h3 id=\"Where_to_apply\" data-internal-id=\"Where_to_apply\">Where to apply<\/h3><ul><li id=\"block573\">Anthropic’s interpretability team roles: <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020159008\">research scientist<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020305008\">research engineer<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4009173008\">research manager<\/a><\/li><li id=\"block574\"><a href=\"https://openai.com/careers/research-engineer-scientist-interpretability\">OpenAI's interpretability team roles<\/a><\/li><li id=\"block575\">My team at Google DeepMind will hopefully be <a href=\"https://deepmind.google/about/careers/#open-roles\">hiring in early 2026<\/a>! Watch this space<\/li><li id=\"block576\"><a href=\"https://transluce.org/\">Transluce<\/a>&nbsp;-- a nonprofit research lab<\/li><li id=\"block577\"><a href=\"https://www.goodfire.ai/\">Goodfire<\/a>&nbsp;-- a mech interp startup that are <a href=\"https://www.goodfire.ai/careers\">hiring a bunch<\/a>.<ul><li id=\"block578\">They <a href=\"https://www.goodfire.ai/blog/announcing-our-50m-series-a\">recently raised a $50 million Series A<\/a>&nbsp;and as of the time of writing are trying to both have people focused on products, and people focused on more fundamental research<\/li><\/ul><\/li><li id=\"block579\">The UK government's AI Security Institute's interpretability team (<a href=\"https://www.aisi.gov.uk/careers#open-roles\">not currently hiring<\/a>)<\/li><\/ul><p data-internal-id=\"Applying_for_grants\" id=\"block580\">Applying for grants<\/p><p id=\"block581\">For people trying to get into mech interp via the safety community, there are some funders around open to giving career transition grants to people trying to upskill in a new field like mech interp. Probably the best place I know of is <a href=\"https://www.openphilanthropy.org/career-development-and-transition-funding/\">Open Philanthropy's Early Career Funding.<\/a><\/p><p data-internal-id=\"Explore_Other_AI_Safety_Areas\" id=\"block582\">Explore Other AI Safety Areas<\/p><p id=\"block583\">Mech interp isn't the only game in town! There’s other important areas of safety like Evals, AI Control, and Scalable Oversight, the latter two in particular seem neglected compared to mech interp. The<a href=\"https://arxiv.org/pdf/2504.01849\">&nbsp;GDM AGI Safety Approach<\/a>&nbsp;gives an overview of different parts of the field. If you’re doing this for safety reasons, I’d check if there’s other, more neglected subfields, that also appeal to you!<\/p><h3 id=\"What_do_hiring_managers_look_for\" data-internal-id=\"What_do_hiring_managers_look_for\">What do hiring managers look for<\/h3><p id=\"block584\">Leaving aside things that apply to basically all roles, like whether this person has a good personality fit (which often just means looking out for red flags), here’s my sense of what hiring managers in interpretability are often looking for.<\/p><p id=\"block585\">A useful mental model is that from a hiring manager's perspective, they're making an uncertain bet with little information in a somewhat adversarial environment. Each applicant wants to present themselves as the perfect fit. This means managers need to rely on signals that are hard to fake. But it’s quite difficult to get that much info on a person before you actually go and work with them a bunch.<\/p><p id=\"block586\">Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly. If your research track record is good enough, interviews may just act as a check for red flags and to verify that you can actually write code and run experiments well.<\/p><p id=\"block587\">Key skills:<\/p><ul><li id=\"block588\"><strong>Research Skills:<\/strong>&nbsp;A track record of completing end-to-end projects is the best signal. Papers are a great way to show this.<ul><li id=\"block589\"><strong>Research taste<\/strong>: The ability to come up with great research ideas <i>and<\/i>&nbsp;drive them to completion is rare and very valuable.<\/li><li id=\"block590\"><strong>Experiment design<\/strong>: Can they design good experiments and make their research ideas concrete and convert them into actions?<\/li><\/ul><\/li><li id=\"block591\"><strong>Conceptual Understanding of Mech Interp:<\/strong>&nbsp;Do you get the key ideas and know the literature?<\/li><li id=\"block592\"><p data-internal-id=\"ftnt_ref36\" id=\"block593\"><strong>Productivity and Conscientiousness:<\/strong>&nbsp;This is a very hard one to interview for, but incredibly important. A public track record of doing interesting things is a good signal, as are strong references from trusted sources<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-noteref\" id=\"fnrefslnwemz4grq\"><sup><a href=\"#fnslnwemz4grq\">[36]<\/a><\/sup><\/span>.<\/p><\/li><li id=\"block594\"><strong>Engineering Skills:<\/strong>&nbsp;Can you work fluently in a Python notebook? Can you write experiment code fast and well? Can you get things done? Do you understand the standard gotchas?<\/li><li id=\"block595\"><strong>Deep engineering skill<\/strong>: Beyond hacking together experiments, can you navigate large, complex codebases, write maintainable code, design complex software projects, etc?<ul><li id=\"block596\">This is much more important if doing research inside a larger lab or tech company than as an independent researcher or academic.<\/li><li id=\"block597\">One of the most common reasons we don't hire seemingly promising researchers onto my team is because they lack sufficiently strong engineering skills.<\/li><li id=\"block598\">Obviously, LLMs are substantially changing the game when it comes to engineering skills, but I think deep engineering skills will be much harder to automate than shallow ones, unfortunately.<\/li><li id=\"block599\">Unfortunately, I don’t have great advice on how to gain these other than working in larger and more complex codebases and learning how to cope. Pair programming with more experienced programmers can be a great way to transfer tacit knowledge<\/li><\/ul><\/li><li id=\"block600\"><strong>Skepticism<\/strong>: Can you constructively engage with research and critically evaluate it? In particular, can you do this to your own research? Good researchers need to be able to do work that is true.<\/li><\/ul><h3 id=\"Should_you_do_a_PhD_\" data-internal-id=\"Should_you_do_a_PhD_\">Should you do a PhD?<\/h3><p id=\"block601\">I don't have a PhD (and think I would have had a far less successful career if I had tried to get one) so I'm somewhat biased. But it's a common question. Here are the strongest arguments I’ve heard in favour:<\/p><ul><li id=\"block602\">You get extremely high <strong>autonomy<\/strong>. If you want to spend years going deep on a niche topic that no industry lab would fund, a PhD is one of the only ways to do it.<\/li><li id=\"block603\">It's a great environment to cultivate the ability to <strong>set your own research agenda<\/strong>. This is a crucial and difficult skill that is harder to learn in industry, where agendas are often set from the top down (though this varies a lot between team).<\/li><\/ul><p id=\"block604\">And here are the reasons I think it's often a bad idea:<\/p><ul><li id=\"block605\">The opportunity cost is immense. You could spend 4-6 years gaining direct, relevant experience in an industry lab.<\/li><li id=\"block606\">Academic incentives can be misaligned with doing impactful research, e.g. pressure to publish meaning you’re discouraged from admitting to the limitations of your work.<\/li><li id=\"block607\">The quality of supervision varies wildly, and a bad supervisor can make your life miserable.<\/li><li id=\"block608\">Quality of life: The pay is generally terrible, which may or may not matter to you, and you may only get places in a different city/country than you’d prefer.<\/li><\/ul><p id=\"block609\">But with all those caveats in mind, it’s definitely the right option for some! My overall take:<\/p><ul><li id=\"block610\">The key thing that matters is mentorship, being in an environment where you are working with a better researcher, and learning from them.<ul><li id=\"block611\">PhDs are often a good way of getting this. But if you can gain this by another way, plausibly you should go to that instead. PhDs have a lot of downsides too.<\/li><\/ul><\/li><li id=\"block612\">Generally, the variance between supervisors and between managers in industry will dominate the academia versus industry differences, and thus you should pay a lot of attention to who exactly would be managing you.<ul><li id=\"block613\">For a PhD, try to speak to your potential supervisor’s students in a private setting. If they say pretty bad things, that's a good reason not to go for the supervisor.<\/li><li id=\"block614\">A common mistake is optimising for the most prestigious and famous supervisor when you often want to go for the ones who will have the most time for you, which anti-correlates.<\/li><\/ul><\/li><li id=\"block615\">A common mistake is people feeling they need to <i>finish<\/i>&nbsp;PhDs. But if you sincerely believe that the point of a PhD is to be a learning environment, then why would the formal end of the PhD be the optimal time to leave? It's all kind of arbitrary.<ul><li id=\"block616\">IMO, at least every six months, you should seriously evaluate what other opportunities you have, try applying for some things and be emotionally willing leave if a better opportunity comes along (taking into account switching costs).<ul><li id=\"block617\">Note that often you can just take a year's leave of absence and resume at will.<\/li><\/ul><\/li><\/ul><\/li><\/ul><p data-internal-id=\"Relevant_Academic_Labs\" id=\"block618\">Relevant Academic Labs<\/p><p id=\"block619\">I’m a big fan of the work coming out of these two, they seem like great places to work:<\/p><ul><li id=\"block620\">David Bau (Northeastern)<\/li><li id=\"block621\">Martin Wattenberg &amp; Fernanda Viegas (Harvard)<\/li><\/ul><p id=\"block622\">Other labs that seem like good places to do interpretability research (note that this is not trying to be a comprehensive list!):<\/p><ul><li id=\"block623\">Yonatan Belinkov (Technion)<\/li><li id=\"block624\">Jacob Andreas (MIT)<\/li><li id=\"block625\">Jacob Steinhardt (Berkeley)<\/li><li id=\"block626\">Ellie Pavlick (Brown)<\/li><li id=\"block627\">Victor Veitch (UChicago)<\/li><li id=\"block628\">Robert West (EPFL)<\/li><li id=\"block629\">Roger Grosse (Toronto)<\/li><li id=\"block630\">Mor Geva (Tel Aviv)<\/li><li id=\"block631\">Sarah Wiegreffe (Maryland)<\/li><li id=\"block632\">Aaron Mueller (Boston University)<\/li><\/ul><p id=\"block633\"><i>Thanks a lot to Arthur Conmy, Paul Bogdan, Bilal Chughtai, Julian Minder, Callum McDougall, Josh Engels, Clement Dumas, Bart Bussmann for valuable feedback<\/i><\/p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-endnote\" id=\"fnnifk1wb1jum\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"nifk1wb1jum\"><sup><strong><a href=\"#fnrefnifk1wb1jum\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block635\">&nbsp;Note that I mean a full working month here. So something like 200 working hours. If you're only able to do this part-time, it's fine to take longer. If you're really focused on it, or have a head-start, then move on faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-endnote\" id=\"fnue9pdw6v8rj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"ue9pdw6v8rj\"><sup><strong><a href=\"#fnrefue9pdw6v8rj\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block637\">&nbsp;If you want something even more approachable, one of my past MATS scholars recommends getting GPT-5 thinking to produce coding exercises (eg a Python script with empty functions, and good tests), for an easier way in.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-endnote\" id=\"fnhh6mwdeo4zm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"hh6mwdeo4zm\"><sup><strong><a href=\"#fnrefhh6mwdeo4zm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block639\">&nbsp;It’s fine for this coding to need a bunch of LLM help and documentation/tutorial looking up, this isn’t a memory test. The key thing is being able to correctly explain the core of each technique to a friend/LLM.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-endnote\" id=\"fnsxyjce3nii\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"sxyjce3nii\"><sup><strong><a href=\"#fnrefsxyjce3nii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block641\">&nbsp;Note: This curriculum aims to get you started on <i>independent research<\/i>. This is often good enough for academic labs, but the engineering bar for most industry labs is significantly higher, as you’ll need to work in a large complex codebase with hundreds of other researchers. But those skills take much longer to gain.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-endnote\" id=\"fnkte6u8splw\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"kte6u8splw\"><sup><strong><a href=\"#fnrefkte6u8splw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block643\">&nbsp;You want to exclude the first token of the prompt when collecting activations, it’s a weird attention sink and often has high norm/is anomalous in many ways<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-endnote\" id=\"fn2ob115pcmet\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"2ob115pcmet\"><sup><strong><a href=\"#fnref2ob115pcmet\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block645\">&nbsp;Gotcha: Remember to try a bunch of coefficients for the vector when adding it. This is a crucial hyper-parameter and steered model behaviour varies a lot depending on its value<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-endnote\" id=\"fn1b9r0ass7sd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1b9r0ass7sd\"><sup><strong><a href=\"#fnref1b9r0ass7sd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block647\">&nbsp;Mixture of expert models, where there are many parameters, and only a fraction light up for each token, are a pain for interpretability research. Larger models means you'll need to get more/larger GPUs which is expensive and unwieldy. Favor working with dense models where possible.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-endnote\" id=\"fnbzop9pji3nl\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bzop9pji3nl\"><sup><strong><a href=\"#fnrefbzop9pji3nl\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block649\">&nbsp;You can download then upload the PDF to the model, or just select all and copy and paste from the PDF to the chat window. No need to correct the formatting issues, LLMs are great at ignoring weird formatting artifacts<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-endnote\" id=\"fn207k0k5nobb\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"207k0k5nobb\"><sup><strong><a href=\"#fnref207k0k5nobb\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block651\">&nbsp;<a href=\"http://repo2txt.com\">repo2txt.com<\/a>&nbsp;is a useful tool for concatenating a Github repo into a single txt file<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-endnote\" id=\"fn979wnkvgpa4\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"979wnkvgpa4\"><sup><strong><a href=\"#fnref979wnkvgpa4\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block653\">&nbsp;If you would like other perspectives, check out <a href=\"https://arxiv.org/abs/2501.16496\">Open Problems in Mechanistic Interpretability<\/a>&nbsp;(broad lit review from many leading researchers, recent), or <a href=\"https://transformer-circuits.pub/2023/interpretability-dreams/index.html\">Interpretability Dreams<\/a>&nbsp;(from Anthropic, 2 years old)<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-endnote\" id=\"fn3zw26zes9dx\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3zw26zes9dx\"><sup><strong><a href=\"#fnref3zw26zes9dx\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block655\">&nbsp;And for reasons we’ll discuss later, now feel much more pessimistic about the ambitious reverse engineering direction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-endnote\" id=\"fn7cxhc64szn8\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7cxhc64szn8\"><sup><strong><a href=\"#fnref7cxhc64szn8\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block657\">&nbsp;Even if you already have a research background in another field, mechanistic interpretability is sufficiently different that you should expect to need to relearn at least some of your instincts. This stage remains very relevant to you, though you can hopefully learn faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-endnote\" id=\"fn9wj0u0qz3q\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9wj0u0qz3q\"><sup><strong><a href=\"#fnref9wj0u0qz3q\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block659\">&nbsp;The rest of this piece will be framed around approaching learning research like this and why I think it is a reasonable process. Obviously, there is not one true correct way to learn research! When I e.g. critique something as a “mistake”, interpret this as “I often see people do this and think it’s suboptimal for them”, not “there does not exist a way of learning research where this is a good idea<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-endnote\" id=\"fnxw1ra5pqnd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"xw1ra5pqnd\"><sup><strong><a href=\"#fnrefxw1ra5pqnd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block661\">&nbsp;My term for associated knowledge, understanding, intuition, etc.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-endnote\" id=\"fntq4gws0zq69\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tq4gws0zq69\"><sup><strong><a href=\"#fnreftq4gws0zq69\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block663\">&nbsp;Read <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">my thoughts on SAEs here<\/a>. There’s still useful work to be done, but it’s an oversubscribed area, and our bar should be higher. They are a useful tool, but not as promising as I once hoped.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-endnote\" id=\"fncdmsagzbqkp\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"cdmsagzbqkp\"><sup><strong><a href=\"#fnrefcdmsagzbqkp\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block665\">&nbsp;This was using a technique called synthetic document fine-tuning (and some other creativity on top), which basically lets you insert false beliefs into a model by generating a bunch of fictional documents where those beliefs are true and fine-tuning the model on them.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-endnote\" id=\"fnp0f0m03b55r\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"p0f0m03b55r\"><sup><strong><a href=\"#fnrefp0f0m03b55r\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block667\">We chose problems we’re excited to see worked on, while trying to avoid fad-like dynamics<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-endnote\" id=\"fng12d8d1lqu\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"g12d8d1lqu\"><sup><strong><a href=\"#fnrefg12d8d1lqu\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block669\">&nbsp;Latents refer to the hidden units of the SAE. These were originally termed “features”, but that term is also used to mean “the interpretable concept the latent refers to”, so I use a different term to minimise confusion.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-endnote\" id=\"fn0td6a2gxwht\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"0td6a2gxwht\"><sup><strong><a href=\"#fnref0td6a2gxwht\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block671\">&nbsp;One of my MATS scholars make a working GPT-5 model diffing agent in a day<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-endnote\" id=\"fn5bdglmkdzr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"5bdglmkdzr\"><sup><strong><a href=\"#fnref5bdglmkdzr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block673\">&nbsp;This is the one line in the post <i>without <\/i>a “as of early Sept 2025” disclaimer, this feels pretty evergreen<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-endnote\" id=\"fnwuxdh4f7kh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wuxdh4f7kh\"><sup><strong><a href=\"#fnrefwuxdh4f7kh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block675\">Note: \"think\" or \"chain of thought\" are terrible terms. It's far more useful to think of the chain of thought as a scratchpad that a model with very limited short-term memory can choose to use or ignore.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-endnote\" id=\"fn3qxoen8tddk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3qxoen8tddk\"><sup><strong><a href=\"#fnref3qxoen8tddk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block677\">Reasoning models break a lot of standard interpretability techniques because now the computational graph goes through the discrete, non-differentiable, and random operation of sampling thousands of times. Most interpretability techniques focus on studying a single forward pass.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-endnote\" id=\"fnlm5ixkfuzk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"lm5ixkfuzk\"><sup><strong><a href=\"#fnreflm5ixkfuzk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block679\">Not just, e.g., ones you can publish on.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-endnote\" id=\"fnidab8074tka\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"idab8074tka\"><sup><strong><a href=\"#fnrefidab8074tka\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block681\">I called this moving fast in the blog post, but I think that may have confused some people.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-endnote\" id=\"fnwpekmwudkpd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wpekmwudkpd\"><sup><strong><a href=\"#fnrefwpekmwudkpd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block683\">Though often this is done well with just a good introduction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-endnote\" id=\"fn1bau7vsh9tk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1bau7vsh9tk\"><sup><strong><a href=\"#fnref1bau7vsh9tk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block685\">And having a well-known researcher as co-author is not sufficient evidence to avoid this, alas. I’m sure at least one paper I’ve co-authored in the past year or two is substantially false<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-endnote\" id=\"fnadytzr5d7y\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"adytzr5d7y\"><sup><strong><a href=\"#fnrefadytzr5d7y\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block687\">It's strongly in your interests for people to build on your work because that makes your original work look better, in addition to being just pretty cool to see people engage deeply with your stuff.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-endnote\" id=\"fnva7mhfkrhm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"va7mhfkrhm\"><sup><strong><a href=\"#fnrefva7mhfkrhm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block689\">Note that deliberately reproducing work, or trying to demonstrate the past work is shoddy, is completely reasonable. You just need to not <i>accidentally<\/i>&nbsp;reinvent the wheel.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-endnote\" id=\"fntt0owz8koks\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tt0owz8koks\"><sup><strong><a href=\"#fnreftt0owz8koks\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block691\">This is generally a good thing to do regardless of whether you’re focused on research taste or not!<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-endnote\" id=\"fne3252d8idmr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"e3252d8idmr\"><sup><strong><a href=\"#fnrefe3252d8idmr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block693\">And, nowadays, LLM knowledge too I guess?<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-endnote\" id=\"fn8354hd0flji\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"8354hd0flji\"><sup><strong><a href=\"#fnref8354hd0flji\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block695\">Note that you’ll need someone who’s written several Arxiv papers to endorse you. cs.LG is the typical category for ML papers.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-endnote\" id=\"fn9oppcf0ftrh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9oppcf0ftrh\"><sup><strong><a href=\"#fnref9oppcf0ftrh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block697\">Note that you can submit something to a workshop <i>and <\/i>to a conference, so long as the workshop is “non-archival”<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-endnote\" id=\"fnfmh579omuc6\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"fmh579omuc6\"><sup><strong><a href=\"#fnreffmh579omuc6\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block699\">A conference paper is a fair bit more effort, and you generally want to be working with someone who understands the academic conventions and shibboleths and the various hoops you should be jumping through. But I think this can be a nice thing to aim for, especially if you're starting out and need credentials, though mech interp cares less about peer review than most academic subfields.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-endnote\" id=\"fnf09vsa4w37e\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"f09vsa4w37e\"><sup><strong><a href=\"#fnreff09vsa4w37e\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block701\">See <a href=\"https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/\">this NeurIPS experiment<\/a>&nbsp;showing that half the spotlight papers would be rejected by an independent reviewing council<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-endnote\" id=\"fn7ruxx269r2s\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7ruxx269r2s\"><sup><strong><a href=\"#fnref7ruxx269r2s\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block703\">&nbsp;This is one of the most valuable things I do for my MATS scholars, IMO.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-endnote\" id=\"fnslnwemz4grq\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"slnwemz4grq\"><sup><strong><a href=\"#fnrefslnwemz4grq\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p id=\"block705\">&nbsp; Unfortunately, standard reference culture, especially in the US, is to basically lie, and the amount of lying varies between contexts, rendering references mostly useless unless from a cultural context the hiring manager understands or ideally from people they know and trust. This is one of the reasons that doing AI safety mentoring programs like MATS can be extremely valuable, because often your mentor will know people who might then go on to hire you, which makes you a lower risk hire from their perspective.<\/p><\/div><\/li><\/ol>",
                    "commentsByBlock": {},
                    "highKarmaCommentsByBlock": {}
                },
                "sideCommentsCache": null
            },
            "User:KCExMGwS2ETzN3Ksr": {
                "__typename": "User",
                "_id": "KCExMGwS2ETzN3Ksr",
                "slug": "neel-nanda-1",
                "createdAt": "2017-03-08T10:35:55.355Z",
                "username": "neel-nanda-1",
                "displayName": "Neel Nanda",
                "profileImageId": null,
                "karma": 12241,
                "afKarma": 2023,
                "deleted": false,
                "isAdmin": false,
                "htmlBio": "",
                "jobTitle": null,
                "organization": null,
                "postCount": 94,
                "commentCount": 697,
                "sequenceCount": 7,
                "afPostCount": 58,
                "afCommentCount": 222,
                "spamRiskScore": 1,
                "tagRevisionCount": 1,
                "reviewedByUserId": "r38pkCm7wF4M44MDQ",
                "moderationStyle": null,
                "bannedUserIds": null,
                "moderatorAssistance": null,
                "groups": ["canModeratePersonal", "alignmentForum", "alignmentVoters", "trustLevel1"],
                "banned": null,
                "allCommentingDisabled": null
            },
            "Tag:4kQXps8dYsKJgaayN": {
                "__typename": "Tag",
                "_id": "4kQXps8dYsKJgaayN",
                "userId": "HoGziwmhpMGqGeWZy",
                "name": "Careers",
                "shortName": null,
                "slug": "careers",
                "core": false,
                "postCount": 228,
                "adminOnly": false,
                "canEditUserIds": null,
                "suggestedAsFilter": false,
                "needsReview": false,
                "descriptionTruncationCount": 0,
                "createdAt": "2020-07-30T21:38:58.131Z",
                "wikiOnly": false,
                "deleted": false,
                "isSubforum": false,
                "noindex": false,
                "isArbitalImport": false,
                "isPlaceholderPage": false,
                "baseScore": 0,
                "extendedScore": {
                    "reacts": {},
                    "usersWhoLiked": []
                },
                "score": 0,
                "afBaseScore": null,
                "afExtendedScore": {
                    "reacts": {},
                    "usersWhoLiked": []
                },
                "voteCount": 0,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "isRead": false,
                "parentTag": null,
                "subTags": [],
                "description": {
                    "__ref": "Revision:4kQXps8dYsKJgaayN_description"
                },
                "canVoteOnRels": null
            },
            "Tag:56yXXrcxRjrQs6z9R": {
                "__typename": "Tag",
                "_id": "56yXXrcxRjrQs6z9R",
                "userId": "DgsGzjyBXN8XSK22q",
                "name": "Interpretability (ML & AI)",
                "shortName": null,
                "slug": "interpretability-ml-and-ai",
                "core": false,
                "postCount": 988,
                "adminOnly": false,
                "canEditUserIds": null,
                "suggestedAsFilter": false,
                "needsReview": false,
                "descriptionTruncationCount": 0,
                "createdAt": "2020-07-30T22:00:37.947Z",
                "wikiOnly": false,
                "deleted": false,
                "isSubforum": false,
                "noindex": false,
                "isArbitalImport": false,
                "isPlaceholderPage": false,
                "baseScore": 14,
                "extendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "qgdGA4ZEyW7zNdK84",
                        "displayName": "Ruby"
                    }, {
                        "_id": "t46uLRSbDziEcKmev",
                        "displayName": "Kriz Tahimic"
                    }, {
                        "_id": "sqMaBFCkAhRcWzJXi",
                        "displayName": "nicolasguillard"
                    }, {
                        "_id": "S6Niz3DiFCTm2Eybq",
                        "displayName": "Anirudh257"
                    }, {
                        "_id": "5pkbdGPiWLGDczsDw",
                        "displayName": "Karen Deng"
                    }, {
                        "_id": "8jC3j8pkyDZPHa3mC",
                        "displayName": "Swapnil Sharma"
                    }]
                },
                "score": 14,
                "afBaseScore": 3,
                "afExtendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "qgdGA4ZEyW7zNdK84",
                        "displayName": "Ruby"
                    }]
                },
                "voteCount": 6,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "isRead": false,
                "parentTag": null,
                "subTags": [],
                "description": {
                    "__ref": "Revision:56yXXrcxRjrQs6z9R_description"
                },
                "canVoteOnRels": null
            },
            "Tag:fF9GEdWXKJ3z73TmB": {
                "__typename": "Tag",
                "_id": "fF9GEdWXKJ3z73TmB",
                "userId": "qgdGA4ZEyW7zNdK84",
                "name": "Scholarship & Learning",
                "shortName": null,
                "slug": "scholarship-and-learning",
                "core": false,
                "postCount": 371,
                "adminOnly": false,
                "canEditUserIds": null,
                "suggestedAsFilter": false,
                "needsReview": false,
                "descriptionTruncationCount": 0,
                "createdAt": "2020-06-09T16:57:01.474Z",
                "wikiOnly": false,
                "deleted": false,
                "isSubforum": false,
                "noindex": false,
                "isArbitalImport": false,
                "isPlaceholderPage": false,
                "baseScore": 24,
                "extendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "EQNTWXLKMeWMp2FQS",
                        "displayName": "Ben Pace"
                    }, {
                        "_id": "qgdGA4ZEyW7zNdK84",
                        "displayName": "Ruby"
                    }, {
                        "_id": "t46uLRSbDziEcKmev",
                        "displayName": "Kriz Tahimic"
                    }, {
                        "_id": "dRaCtsAWxk7sgirSY",
                        "displayName": "Jordan Morgan"
                    }, {
                        "_id": "xF5nfdddHjFThHy49",
                        "displayName": "kevinhou111@gmail.com"
                    }, {
                        "_id": "o7sxHuKmezdhAe8wB",
                        "displayName": "ThoughtCachier"
                    }, {
                        "_id": "v9czuhSzYBhrotRkN",
                        "displayName": "Slick Tuna"
                    }]
                },
                "score": 24,
                "afBaseScore": 9,
                "afExtendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "EQNTWXLKMeWMp2FQS",
                        "displayName": "Ben Pace"
                    }, {
                        "_id": "qgdGA4ZEyW7zNdK84",
                        "displayName": "Ruby"
                    }]
                },
                "voteCount": 7,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "isRead": false,
                "parentTag": null,
                "subTags": [],
                "description": {
                    "__ref": "Revision:fF9GEdWXKJ3z73TmB_description"
                },
                "canVoteOnRels": null
            },
            "Tag:fkABsGCJZ6y9qConW": {
                "__typename": "Tag",
                "_id": "fkABsGCJZ6y9qConW",
                "userId": "oBSWiHjgproTiThmY",
                "name": "Practical",
                "shortName": null,
                "slug": "practical",
                "core": true,
                "postCount": 3517,
                "adminOnly": false,
                "canEditUserIds": null,
                "suggestedAsFilter": true,
                "needsReview": false,
                "descriptionTruncationCount": 2000,
                "createdAt": "2020-06-14T06:06:46.947Z",
                "wikiOnly": false,
                "deleted": false,
                "isSubforum": false,
                "noindex": false,
                "isArbitalImport": false,
                "isPlaceholderPage": false,
                "baseScore": 2,
                "extendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "MiuAZvbQcQ7ethgt3",
                        "displayName": "Viktor withaK"
                    }, {
                        "_id": "dRaCtsAWxk7sgirSY",
                        "displayName": "Jordan Morgan"
                    }]
                },
                "score": 2,
                "afBaseScore": 0,
                "afExtendedScore": {
                    "reacts": {},
                    "usersWhoLiked": []
                },
                "voteCount": 2,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "isRead": false,
                "parentTag": null,
                "subTags": [],
                "description": {
                    "__ref": "Revision:fkABsGCJZ6y9qConW_description"
                },
                "canVoteOnRels": null
            },
            "Tag:sYm3HiWcfZvrGu3ui": {
                "__typename": "Tag",
                "_id": "sYm3HiWcfZvrGu3ui",
                "userId": "r38pkCm7wF4M44MDQ",
                "name": "AI",
                "shortName": null,
                "slug": "ai",
                "core": true,
                "postCount": 13298,
                "adminOnly": false,
                "canEditUserIds": null,
                "suggestedAsFilter": true,
                "needsReview": false,
                "descriptionTruncationCount": 2000,
                "createdAt": "2020-06-14T22:24:22.097Z",
                "wikiOnly": false,
                "deleted": false,
                "isSubforum": false,
                "noindex": false,
                "isArbitalImport": false,
                "isPlaceholderPage": false,
                "baseScore": 18,
                "extendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "nLbwLhBaQeG6tCNDN",
                        "displayName": "jimrandomh"
                    }, {
                        "_id": "zgatkccjT3SQifFZy",
                        "displayName": "Xylon2"
                    }, {
                        "_id": "sof55TPMQaeBaxhsS",
                        "displayName": "tommylees112"
                    }, {
                        "_id": "AayjS8XzcnDKhGdTv",
                        "displayName": "shark"
                    }, {
                        "_id": "HnALuwRdo6k9HLaMt",
                        "displayName": "Alex Firssoff"
                    }, {
                        "_id": "7nnqwro5zPGXgbCd2",
                        "displayName": "denis.kondratev"
                    }, {
                        "_id": "iDBHmAdFLTiascgpK",
                        "displayName": "sahal-mulki"
                    }, {
                        "_id": "wxjyq9kKCyNJMBwzR",
                        "displayName": "김신우"
                    }, {
                        "_id": "chaq3xcrjB95eFuWw",
                        "displayName": "egesko"
                    }, {
                        "_id": "L4X38T7Z2fHa7EKSJ",
                        "displayName": "SergioHC"
                    }]
                },
                "score": 18,
                "afBaseScore": 2,
                "afExtendedScore": {
                    "reacts": {},
                    "usersWhoLiked": [{
                        "_id": "nLbwLhBaQeG6tCNDN",
                        "displayName": "jimrandomh"
                    }]
                },
                "voteCount": 10,
                "currentUserVote": null,
                "currentUserExtendedVote": null,
                "isRead": false,
                "parentTag": null,
                "subTags": [],
                "description": {
                    "__ref": "Revision:sYm3HiWcfZvrGu3ui_description"
                },
                "canVoteOnRels": null
            },
            "SocialPreviewType:jP9KDyMkchuv6tHwm": {
                "__typename": "SocialPreviewType",
                "_id": "jP9KDyMkchuv6tHwm",
                "imageUrl": "",
                "text": null
            },
            "Revision:4kQXps8dYsKJgaayN_description": {
                "__typename": "Revision",
                "_id": "4kQXps8dYsKJgaayN_description",
                "htmlHighlight": "<p>Posts relating to jobs, career development, etc.<\/p>"
            },
            "Revision:56yXXrcxRjrQs6z9R_description": {
                "__typename": "Revision",
                "_id": "56yXXrcxRjrQs6z9R_description",
                "htmlHighlight": "<p><strong>Interpretability<\/strong> is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.<\/p><p>Present-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.<\/p><p>A prominent subfield of interpretability of neural networks is mechanistic interpretability, which attempts to <a href=\"https://www.neelnanda.io/mechanistic-interpretability/quickstart\">understand<\/a> <i>how<\/i> neural networks perform the tasks they perform, for example by finding <a href=\"https://www.lesswrong.com/tag/transformer-circuits\">circuits in transformer models<\/a>. This can be contrasted to subfieds of interpretability which seek to attribute some output to a part of a specific input, such as clarifying which pixels in an input image <a href=\"https://christophm.github.io/interpretable-ml-book/pixel-attribution.html\">caused<\/a> a computer vision model to output the classification \"horse\".<\/p><h2>See Also<\/h2><ul><li><a href=\"https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\">Explainable Artificial Intelligence<\/a> on Wikipedia<\/li><li><a href=\"https://www.lesswrong.com/tag/transformer-circuits\">Transformer Circuits<\/a><\/li><li><a href=\"https://christophm.github.io/interpretable-ml-book/\">Interpretable Machine Learning<\/a>, textbook<\/li><\/ul><h3>Research<\/h3><ul><li><a href=\"https://distill.pub/2020/circuits/\">Circuits Thread<\/a><\/li><li><a href=\"https://transformer-circuits.pub/\">Transformer Circuits Thread<\/a><\/li><\/ul>"
            },
            "Revision:fF9GEdWXKJ3z73TmB_description": {
                "__typename": "Revision",
                "_id": "fF9GEdWXKJ3z73TmB_description",
                "htmlHighlight": "<p><strong>Scholarship &amp; Learning. <\/strong>Here be posts on how to study, research, and learn.<\/p><p>Topics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. – <\/i><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><i>Twelve Virtues of Rationality<\/i><\/a><\/p><\/blockquote><h2>See Also<\/h2><ul><li><a href=\"https://www.lesswrong.com/tag/spaced-repetition\">Spaced Repetition<\/a> is a technique for long-term retention of learned material.<\/li><li><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a> are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".<\/li><li>The other <a href=\"https://www.lesswrong.com/tag/virtues?showPostCount=true&amp;useTagName=true\">Virtues<\/a> of Rationality.<\/li><\/ul><h2>Top Resources<\/h2><ul><li><a href=\"https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently\">Scholarship: How to Do It Efficiently<\/a> is a guide to quickly researching topics and understanding what is known within a field.<\/li><li><a href=\"https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1\">Literature Review For Academic Outsiders: What, How, and Why<\/a> similar to the first resource, contains many links to further resources.<\/li><li><a href=\"https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific\">[Question] How do you assess the quality / reliability of a scientific study?<\/a> A question post with many highly excellent lengthy responses, several which received bounty payouts.<\/li><li><a href=\"https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things\">On learning difficult things<\/a> covers techniques and methods for studying difficult topics.<\/li><li><a href=\"https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears\">Paper-Reading for Gears<\/a> is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.<\/li><li><a href=\"https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll\">The 3 Books Technique for Learning a New Skilll<\/a> is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.<\/li><li><a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">The Best Textbooks on Every Subject<\/a> crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.<\/li><li><a href=\"https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy\">Forum participation as a research strategy<\/a> argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.<\/li><li><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\">Fact Posts: How and Why<\/a> is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.<\/li><li><a href=\"https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review\">How to (not) do a literature review<\/a> which contains a very concrete list of steps for literature reviews, including mistakes to avoid.<\/li><\/ul><p><strong>Ex<\/strong>... <\/p>"
            },
            "Revision:fkABsGCJZ6y9qConW_description": {
                "__typename": "Revision",
                "_id": "fkABsGCJZ6y9qConW_description",
                "htmlHighlight": "<p><strong>Practical<\/strong> posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.<\/p><p>Material that is directly about <i>how to think better<\/i> can be found at <a href=\"https://www.lessestwrong.com/tag/rationality\">Rationality<\/a>.<\/p><p>&nbsp;<\/p><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Practical Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude<\/a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness<\/a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies<\/a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition<\/a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting<\/a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack<\/a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep<\/a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring<\/a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits<\/a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions<\/a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements<\/a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation<\/a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka<\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making<\/a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation<\/a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building<\/a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools<\/a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition<\/a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia<\/a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations<\/a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization<\/a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination<\/a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity<\/a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower<\/a><\/p><\/td><\/tr><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal<\/strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)<\/a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures<\/a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship<\/u><\/a><\/td><\/tr><\/tbody><\/table><\/figure>"
            },
            "Revision:sYm3HiWcfZvrGu3ui_description": {
                "__typename": "Revision",
                "_id": "sYm3HiWcfZvrGu3ui_description",
                "htmlHighlight": "<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory?showPostCount=true&amp;useTagName=true\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding?showPostCount=true&amp;useTagName=true\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk?showPostCount=true&amp;useTagName=true\">Eliciting Latent Knowledge<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/interpretability-ml-and-ai?showPostCount=true&amp;useTagName=true\">Interpretability (ML &amp; AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center?showPostCount=true&amp;useTagName=true\">Alignment Research Center<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropic?showPostCount=true&amp;useTagName=true\">Anthropic<\/a><br><a href=\"https://www.lesswrong.com/tag/apart-research?showPostCount=true&amp;useTagName=true\">Apart Research<\/a><br><a href=\"https://www.lesswrong.com/tag/axrp?showPostCount=true&amp;useTagName=true\">AXRP<\/a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true\">CHAI (UC Berkeley)<\/a><br><a href=\"https://www.lesswrong.com/tag/conjecture-org?showPostCount=true&amp;useTagName=true\">Conjecture (org)<\/a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind<\/a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true\">FHI (Oxf<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "
            },
            "Revision:Mw2qopyZuMv2ZFwux_contents": {
                "__typename": "Revision",
                "_id": "Mw2qopyZuMv2ZFwux_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Activation patching:<\/strong> A technique that modifies specific activations within a neural network to study their causal role in the model's behavior.<\/p><p>By replacing activations from one input with those from another, researchers can trace how information flows through the model.<\/p><\/div>",
                "wordCount": 40,
                "htmlHighlight": "<div><p><strong>Activation patching:<\/strong> A technique that modifies specific activations within a neural network to study their causal role in the model's behavior.<\/p><p>By replacing activations from one input with those from another, researchers can trace how information flows through the model.<\/p><\/div>",
                "plaintextDescription": "Activation patching: A technique that modifies specific activations within a neural network to study their causal role in the model's behavior.\n\nBy replacing activations from one input with those from another, researchers can trace how information flows through the model."
            },
            "JargonTerm:Mw2qopyZuMv2ZFwux": {
                "__typename": "JargonTerm",
                "_id": "Mw2qopyZuMv2ZFwux",
                "term": "activation patching",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:Mw2qopyZuMv2ZFwux_contents"
                }
            },
            "Revision:aucuuEYs2bg9n9Whq_contents": {
                "__typename": "Revision",
                "_id": "aucuuEYs2bg9n9Whq_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>ARENA:<\/strong> A set of educational tutorials and exercises for learning mechanistic interpretability, focusing on hands-on coding practice.<\/p><p>These tutorials teach core techniques through practical implementations, starting with building transformers from scratch and progressing to advanced interpretability methods.<\/p><\/div>",
                "wordCount": 37,
                "htmlHighlight": "<div><p><strong>ARENA:<\/strong> A set of educational tutorials and exercises for learning mechanistic interpretability, focusing on hands-on coding practice.<\/p><p>These tutorials teach core techniques through practical implementations, starting with building transformers from scratch and progressing to advanced interpretability methods.<\/p><\/div>",
                "plaintextDescription": "ARENA: A set of educational tutorials and exercises for learning mechanistic interpretability, focusing on hands-on coding practice.\n\nThese tutorials teach core techniques through practical implementations, starting with building transformers from scratch and progressing to advanced interpretability methods."
            },
            "JargonTerm:aucuuEYs2bg9n9Whq": {
                "__typename": "JargonTerm",
                "_id": "aucuuEYs2bg9n9Whq",
                "term": "ARENA",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:aucuuEYs2bg9n9Whq_contents"
                }
            },
            "Revision:DvQiba5yzfRnGBTK5_contents": {
                "__typename": "Revision",
                "_id": "DvQiba5yzfRnGBTK5_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Attribution graphs:<\/strong> Visualizations that show how information flows through a model, connecting meaningful nodes in the computation.<\/p><p>These graphs help researchers analyze how different parts of the model work together to process information.<\/p><\/div>",
                "wordCount": 33,
                "htmlHighlight": "<div><p><strong>Attribution graphs:<\/strong> Visualizations that show how information flows through a model, connecting meaningful nodes in the computation.<\/p><p>These graphs help researchers analyze how different parts of the model work together to process information.<\/p><\/div>",
                "plaintextDescription": "Attribution graphs: Visualizations that show how information flows through a model, connecting meaningful nodes in the computation.\n\nThese graphs help researchers analyze how different parts of the model work together to process information."
            },
            "JargonTerm:DvQiba5yzfRnGBTK5": {
                "__typename": "JargonTerm",
                "_id": "DvQiba5yzfRnGBTK5",
                "term": "attribution graphs",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:DvQiba5yzfRnGBTK5_contents"
                }
            },
            "Revision:nxwqkKatrCCcfBvxg_contents": {
                "__typename": "Revision",
                "_id": "nxwqkKatrCCcfBvxg_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Autoencoder latents:<\/strong> The compressed representations learned by autoencoders that capture key features of the input data.<\/p><p>These representations often correspond to meaningful concepts that the model has learned to detect and process.<\/p><\/div>",
                "wordCount": 32,
                "htmlHighlight": "<div><p><strong>Autoencoder latents:<\/strong> The compressed representations learned by autoencoders that capture key features of the input data.<\/p><p>These representations often correspond to meaningful concepts that the model has learned to detect and process.<\/p><\/div>",
                "plaintextDescription": "Autoencoder latents: The compressed representations learned by autoencoders that capture key features of the input data.\n\nThese representations often correspond to meaningful concepts that the model has learned to detect and process."
            },
            "JargonTerm:nxwqkKatrCCcfBvxg": {
                "__typename": "JargonTerm",
                "_id": "nxwqkKatrCCcfBvxg",
                "term": "autoencoder latents",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:nxwqkKatrCCcfBvxg_contents"
                }
            },
            "Revision:bYopjJdkLcudMRAZd_contents": {
                "__typename": "Revision",
                "_id": "bYopjJdkLcudMRAZd_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Black box interpretability:<\/strong> Techniques that analyze model behavior without accessing internal components, focusing only on inputs and outputs.<\/p><p>These methods complement mechanistic approaches by studying model behavior through external observation.<\/p><\/div>",
                "wordCount": 30,
                "htmlHighlight": "<div><p><strong>Black box interpretability:<\/strong> Techniques that analyze model behavior without accessing internal components, focusing only on inputs and outputs.<\/p><p>These methods complement mechanistic approaches by studying model behavior through external observation.<\/p><\/div>",
                "plaintextDescription": "Black box interpretability: Techniques that analyze model behavior without accessing internal components, focusing only on inputs and outputs.\n\nThese methods complement mechanistic approaches by studying model behavior through external observation."
            },
            "JargonTerm:bYopjJdkLcudMRAZd": {
                "__typename": "JargonTerm",
                "_id": "bYopjJdkLcudMRAZd",
                "term": "black box interpretability",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:bYopjJdkLcudMRAZd_contents"
                }
            },
            "Revision:mZDFjPJgNkwy6BrFN_contents": {
                "__typename": "Revision",
                "_id": "mZDFjPJgNkwy6BrFN_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Chain of thought:<\/strong> A model's step-by-step reasoning process, visible in its intermediate outputs before reaching a final answer.<\/p><p>This process acts like a scratchpad that models with limited memory can use to work through complex problems.<\/p><\/div>",
                "wordCount": 36,
                "htmlHighlight": "<div><p><strong>Chain of thought:<\/strong> A model's step-by-step reasoning process, visible in its intermediate outputs before reaching a final answer.<\/p><p>This process acts like a scratchpad that models with limited memory can use to work through complex problems.<\/p><\/div>",
                "plaintextDescription": "Chain of thought: A model's step-by-step reasoning process, visible in its intermediate outputs before reaching a final answer.\n\nThis process acts like a scratchpad that models with limited memory can use to work through complex problems."
            },
            "JargonTerm:mZDFjPJgNkwy6BrFN": {
                "__typename": "JargonTerm",
                "_id": "mZDFjPJgNkwy6BrFN",
                "term": "chain of thought",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:mZDFjPJgNkwy6BrFN_contents"
                }
            },
            "Revision:FbELb3r2TFxMAfryQ_contents": {
                "__typename": "Revision",
                "_id": "FbELb3r2TFxMAfryQ_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Direct logit attribution:<\/strong> A technique that traces how different parts of a model contribute to its final output probabilities.<\/p><p>This method helps understand which model components are most responsible for specific predictions.<\/p><\/div>",
                "wordCount": 32,
                "htmlHighlight": "<div><p><strong>Direct logit attribution:<\/strong> A technique that traces how different parts of a model contribute to its final output probabilities.<\/p><p>This method helps understand which model components are most responsible for specific predictions.<\/p><\/div>",
                "plaintextDescription": "Direct logit attribution: A technique that traces how different parts of a model contribute to its final output probabilities.\n\nThis method helps understand which model components are most responsible for specific predictions."
            },
            "JargonTerm:FbELb3r2TFxMAfryQ": {
                "__typename": "JargonTerm",
                "_id": "FbELb3r2TFxMAfryQ",
                "term": "direct logit attribution",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": ["DLA", "logit lens"],
                "contents": {
                    "__ref": "Revision:FbELb3r2TFxMAfryQ_contents"
                }
            },
            "Revision:gCiJ8wX9Gsm25gHRf_contents": {
                "__typename": "Revision",
                "_id": "gCiJ8wX9Gsm25gHRf_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Emergent misalignment:<\/strong> When models trained on narrow tasks develop broader undesired behaviors that weren't directly trained for.<\/p><p>This phenomenon shows how models can develop concerning behaviors through indirect training effects.<\/p><\/div>",
                "wordCount": 30,
                "htmlHighlight": "<div><p><strong>Emergent misalignment:<\/strong> When models trained on narrow tasks develop broader undesired behaviors that weren't directly trained for.<\/p><p>This phenomenon shows how models can develop concerning behaviors through indirect training effects.<\/p><\/div>",
                "plaintextDescription": "Emergent misalignment: When models trained on narrow tasks develop broader undesired behaviors that weren't directly trained for.\n\nThis phenomenon shows how models can develop concerning behaviors through indirect training effects."
            },
            "JargonTerm:gCiJ8wX9Gsm25gHRf": {
                "__typename": "JargonTerm",
                "_id": "gCiJ8wX9Gsm25gHRf",
                "term": "emergent misalignment",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:gCiJ8wX9Gsm25gHRf_contents"
                }
            },
            "Revision:C6rE5KrJAjcwzzvJH_contents": {
                "__typename": "Revision",
                "_id": "C6rE5KrJAjcwzzvJH_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Fine-tuning:<\/strong> The process of further training a pre-trained model on specific data to adapt its behavior.<\/p><p>This technique allows researchers to modify model behavior in controlled ways for study or improvement.<\/p><\/div>",
                "wordCount": 31,
                "htmlHighlight": "<div><p><strong>Fine-tuning:<\/strong> The process of further training a pre-trained model on specific data to adapt its behavior.<\/p><p>This technique allows researchers to modify model behavior in controlled ways for study or improvement.<\/p><\/div>",
                "plaintextDescription": "Fine-tuning: The process of further training a pre-trained model on specific data to adapt its behavior.\n\nThis technique allows researchers to modify model behavior in controlled ways for study or improvement."
            },
            "JargonTerm:C6rE5KrJAjcwzzvJH": {
                "__typename": "JargonTerm",
                "_id": "C6rE5KrJAjcwzzvJH",
                "term": "fine-tuning",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:C6rE5KrJAjcwzzvJH_contents"
                }
            },
            "Revision:obQBqxasnYMCELd7t_contents": {
                "__typename": "Revision",
                "_id": "obQBqxasnYMCELd7t_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Interpretability illusions:<\/strong> False or misleading conclusions about how an AI model works that appear convincing but don't hold up to rigorous testing.<\/p><p>These arise when researchers find patterns that seem meaningful but are actually coincidental or don't capture the true computational process.<\/p><\/div>",
                "wordCount": 42,
                "htmlHighlight": "<div><p><strong>Interpretability illusions:<\/strong> False or misleading conclusions about how an AI model works that appear convincing but don't hold up to rigorous testing.<\/p><p>These arise when researchers find patterns that seem meaningful but are actually coincidental or don't capture the true computational process.<\/p><\/div>",
                "plaintextDescription": "Interpretability illusions: False or misleading conclusions about how an AI model works that appear convincing but don't hold up to rigorous testing.\n\nThese arise when researchers find patterns that seem meaningful but are actually coincidental or don't capture the true computational process."
            },
            "JargonTerm:obQBqxasnYMCELd7t": {
                "__typename": "JargonTerm",
                "_id": "obQBqxasnYMCELd7t",
                "term": "interpretability illusions",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:obQBqxasnYMCELd7t_contents"
                }
            },
            "Revision:ebqfGKjxzEyC6afjm_contents": {
                "__typename": "Revision",
                "_id": "ebqfGKjxzEyC6afjm_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Linear probes:<\/strong> Simple linear classifiers trained to detect specific concepts in a model's internal representations.<\/p><p>These tools help monitor model behavior and detect potentially concerning patterns like harmful content.<\/p><\/div>",
                "wordCount": 29,
                "htmlHighlight": "<div><p><strong>Linear probes:<\/strong> Simple linear classifiers trained to detect specific concepts in a model's internal representations.<\/p><p>These tools help monitor model behavior and detect potentially concerning patterns like harmful content.<\/p><\/div>",
                "plaintextDescription": "Linear probes: Simple linear classifiers trained to detect specific concepts in a model's internal representations.\n\nThese tools help monitor model behavior and detect potentially concerning patterns like harmful content."
            },
            "JargonTerm:ebqfGKjxzEyC6afjm": {
                "__typename": "JargonTerm",
                "_id": "ebqfGKjxzEyC6afjm",
                "term": "linear probes",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:ebqfGKjxzEyC6afjm_contents"
                }
            },
            "Revision:u4gFFLuZudP3fkd7Z_contents": {
                "__typename": "Revision",
                "_id": "u4gFFLuZudP3fkd7Z_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>MATS:<\/strong> A mentoring program where experienced researchers guide newcomers in conducting mechanistic interpretability research over several months.<\/p><p>The program pairs mentees with mentors for weekly check-ins while they work on publishing a research paper.<\/p><\/div>",
                "wordCount": 34,
                "htmlHighlight": "<div><p><strong>MATS:<\/strong> A mentoring program where experienced researchers guide newcomers in conducting mechanistic interpretability research over several months.<\/p><p>The program pairs mentees with mentors for weekly check-ins while they work on publishing a research paper.<\/p><\/div>",
                "plaintextDescription": "MATS: A mentoring program where experienced researchers guide newcomers in conducting mechanistic interpretability research over several months.\n\nThe program pairs mentees with mentors for weekly check-ins while they work on publishing a research paper."
            },
            "JargonTerm:u4gFFLuZudP3fkd7Z": {
                "__typename": "JargonTerm",
                "_id": "u4gFFLuZudP3fkd7Z",
                "term": "MATS",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:u4gFFLuZudP3fkd7Z_contents"
                }
            },
            "Revision:p9AH5LJCobPQsrWn3_contents": {
                "__typename": "Revision",
                "_id": "p9AH5LJCobPQsrWn3_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Max activating dataset examples:<\/strong> Input examples that maximally activate specific neurons or features in a neural network.<\/p><p>These examples help researchers understand what patterns or concepts specific parts of the network have learned to detect.<\/p><\/div>",
                "wordCount": 35,
                "htmlHighlight": "<div><p><strong>Max activating dataset examples:<\/strong> Input examples that maximally activate specific neurons or features in a neural network.<\/p><p>These examples help researchers understand what patterns or concepts specific parts of the network have learned to detect.<\/p><\/div>",
                "plaintextDescription": "Max activating dataset examples: Input examples that maximally activate specific neurons or features in a neural network.\n\nThese examples help researchers understand what patterns or concepts specific parts of the network have learned to detect."
            },
            "JargonTerm:p9AH5LJCobPQsrWn3": {
                "__typename": "JargonTerm",
                "_id": "p9AH5LJCobPQsrWn3",
                "term": "max activating dataset examples",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:p9AH5LJCobPQsrWn3_contents"
                }
            },
            "Revision:wBYHsMiWjywAbxyxy_contents": {
                "__typename": "Revision",
                "_id": "wBYHsMiWjywAbxyxy_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Mechanistic interpretability:<\/strong> A research approach that aims to understand AI models by analyzing their internal components and operations.<\/p><p>This field studies how models process information and make decisions by examining weights, activations, and other internal features, rather than just looking at inputs and outputs.<\/p><\/div>",
                "wordCount": 44,
                "htmlHighlight": "<div><p><strong>Mechanistic interpretability:<\/strong> A research approach that aims to understand AI models by analyzing their internal components and operations.<\/p><p>This field studies how models process information and make decisions by examining weights, activations, and other internal features, rather than just looking at inputs and outputs.<\/p><\/div>",
                "plaintextDescription": "Mechanistic interpretability: A research approach that aims to understand AI models by analyzing their internal components and operations.\n\nThis field studies how models process information and make decisions by examining weights, activations, and other internal features, rather than just looking at inputs and outputs."
            },
            "JargonTerm:wBYHsMiWjywAbxyxy": {
                "__typename": "JargonTerm",
                "_id": "wBYHsMiWjywAbxyxy",
                "term": "mechanistic interpretability",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": ["mech interp"],
                "contents": {
                    "__ref": "Revision:wBYHsMiWjywAbxyxy_contents"
                }
            },
            "Revision:g9WMnKwfyqKgACJdK_contents": {
                "__typename": "Revision",
                "_id": "g9WMnKwfyqKgACJdK_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Model organisms:<\/strong> AI models specifically designed or modified to exhibit certain behaviors for research purposes.<\/p><p>These models serve as controlled experimental subjects for testing interpretability techniques and understanding model behavior.<\/p><\/div>",
                "wordCount": 30,
                "htmlHighlight": "<div><p><strong>Model organisms:<\/strong> AI models specifically designed or modified to exhibit certain behaviors for research purposes.<\/p><p>These models serve as controlled experimental subjects for testing interpretability techniques and understanding model behavior.<\/p><\/div>",
                "plaintextDescription": "Model organisms: AI models specifically designed or modified to exhibit certain behaviors for research purposes.\n\nThese models serve as controlled experimental subjects for testing interpretability techniques and understanding model behavior."
            },
            "JargonTerm:g9WMnKwfyqKgACJdK": {
                "__typename": "JargonTerm",
                "_id": "g9WMnKwfyqKgACJdK",
                "term": "model organisms",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:g9WMnKwfyqKgACJdK_contents"
                }
            },
            "Revision:k69d5jT4J6abhyP9i_contents": {
                "__typename": "Revision",
                "_id": "k69d5jT4J6abhyP9i_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Monosemantic:<\/strong> A feature or component that consistently represents a single, well-defined concept.<\/p><p>These features are particularly valuable for interpretability as they provide clear windows into the model's processing.<\/p><\/div>",
                "wordCount": 28,
                "htmlHighlight": "<div><p><strong>Monosemantic:<\/strong> A feature or component that consistently represents a single, well-defined concept.<\/p><p>These features are particularly valuable for interpretability as they provide clear windows into the model's processing.<\/p><\/div>",
                "plaintextDescription": "Monosemantic: A feature or component that consistently represents a single, well-defined concept.\n\nThese features are particularly valuable for interpretability as they provide clear windows into the model's processing."
            },
            "JargonTerm:k69d5jT4J6abhyP9i": {
                "__typename": "JargonTerm",
                "_id": "k69d5jT4J6abhyP9i",
                "term": "monosemantic",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:k69d5jT4J6abhyP9i_contents"
                }
            },
            "Revision:huCiBPpE3tgsHgDAG_contents": {
                "__typename": "Revision",
                "_id": "huCiBPpE3tgsHgDAG_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Reinforcement learning:<\/strong> A training approach where models learn through trial and error, receiving rewards for desired behaviors.<\/p><p>This method is used to train models to think through problems step by step before giving final answers.<\/p><\/div>",
                "wordCount": 35,
                "htmlHighlight": "<div><p><strong>Reinforcement learning:<\/strong> A training approach where models learn through trial and error, receiving rewards for desired behaviors.<\/p><p>This method is used to train models to think through problems step by step before giving final answers.<\/p><\/div>",
                "plaintextDescription": "Reinforcement learning: A training approach where models learn through trial and error, receiving rewards for desired behaviors.\n\nThis method is used to train models to think through problems step by step before giving final answers."
            },
            "JargonTerm:huCiBPpE3tgsHgDAG": {
                "__typename": "JargonTerm",
                "_id": "huCiBPpE3tgsHgDAG",
                "term": "reinforcement learning",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": true,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:huCiBPpE3tgsHgDAG_contents"
                }
            },
            "Revision:wxHNv6zijgrbTxu6N_contents": {
                "__typename": "Revision",
                "_id": "wxHNv6zijgrbTxu6N_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Sparse autoencoders:<\/strong> Neural networks trained to decompose model activations into interpretable features while maintaining sparsity.<\/p><p>These tools help identify and isolate specific concepts or features that the model has learned to represent.<\/p><\/div>",
                "wordCount": 32,
                "htmlHighlight": "<div><p><strong>Sparse autoencoders:<\/strong> Neural networks trained to decompose model activations into interpretable features while maintaining sparsity.<\/p><p>These tools help identify and isolate specific concepts or features that the model has learned to represent.<\/p><\/div>",
                "plaintextDescription": "Sparse autoencoders: Neural networks trained to decompose model activations into interpretable features while maintaining sparsity.\n\nThese tools help identify and isolate specific concepts or features that the model has learned to represent."
            },
            "JargonTerm:wxHNv6zijgrbTxu6N": {
                "__typename": "JargonTerm",
                "_id": "wxHNv6zijgrbTxu6N",
                "term": "sparse autoencoders",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": ["SAEs"],
                "contents": {
                    "__ref": "Revision:wxHNv6zijgrbTxu6N_contents"
                }
            },
            "Revision:CDZedjFDGhR6Dptr6_contents": {
                "__typename": "Revision",
                "_id": "CDZedjFDGhR6Dptr6_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Sparse feature circuits:<\/strong> Networks of sparsely connected components in neural networks that work together to implement specific computations.<\/p><p>These circuits represent how different parts of the model combine to process information.<\/p><\/div>",
                "wordCount": 31,
                "htmlHighlight": "<div><p><strong>Sparse feature circuits:<\/strong> Networks of sparsely connected components in neural networks that work together to implement specific computations.<\/p><p>These circuits represent how different parts of the model combine to process information.<\/p><\/div>",
                "plaintextDescription": "Sparse feature circuits: Networks of sparsely connected components in neural networks that work together to implement specific computations.\n\nThese circuits represent how different parts of the model combine to process information."
            },
            "JargonTerm:CDZedjFDGhR6Dptr6": {
                "__typename": "JargonTerm",
                "_id": "CDZedjFDGhR6Dptr6",
                "term": "sparse feature circuits",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:CDZedjFDGhR6Dptr6_contents"
                }
            },
            "Revision:pfjnKhCdZ4YqtZEBc_contents": {
                "__typename": "Revision",
                "_id": "pfjnKhCdZ4YqtZEBc_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Steering vectors:<\/strong> Directions in a model's activation space that, when added to the model's activations, push its behavior toward specific outcomes.<\/p><p>These vectors can be used to modify model behavior in controlled ways, such as making outputs more positive or negative.<\/p><\/div>",
                "wordCount": 41,
                "htmlHighlight": "<div><p><strong>Steering vectors:<\/strong> Directions in a model's activation space that, when added to the model's activations, push its behavior toward specific outcomes.<\/p><p>These vectors can be used to modify model behavior in controlled ways, such as making outputs more positive or negative.<\/p><\/div>",
                "plaintextDescription": "Steering vectors: Directions in a model's activation space that, when added to the model's activations, push its behavior toward specific outcomes.\n\nThese vectors can be used to modify model behavior in controlled ways, such as making outputs more positive or negative."
            },
            "JargonTerm:pfjnKhCdZ4YqtZEBc": {
                "__typename": "JargonTerm",
                "_id": "pfjnKhCdZ4YqtZEBc",
                "term": "steering vectors",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:pfjnKhCdZ4YqtZEBc_contents"
                }
            },
            "Revision:eTaF5GuyjrPDDAAco_contents": {
                "__typename": "Revision",
                "_id": "eTaF5GuyjrPDDAAco_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Synthetic document fine-tuning:<\/strong> A technique that modifies model behavior by training it on artificially generated documents.<\/p><p>This method allows researchers to insert specific beliefs or behaviors into models for study.<\/p><\/div>",
                "wordCount": 30,
                "htmlHighlight": "<div><p><strong>Synthetic document fine-tuning:<\/strong> A technique that modifies model behavior by training it on artificially generated documents.<\/p><p>This method allows researchers to insert specific beliefs or behaviors into models for study.<\/p><\/div>",
                "plaintextDescription": "Synthetic document fine-tuning: A technique that modifies model behavior by training it on artificially generated documents.\n\nThis method allows researchers to insert specific beliefs or behaviors into models for study."
            },
            "JargonTerm:eTaF5GuyjrPDDAAco": {
                "__typename": "JargonTerm",
                "_id": "eTaF5GuyjrPDDAAco",
                "term": "synthetic document fine-tuning",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": [],
                "contents": {
                    "__ref": "Revision:eTaF5GuyjrPDDAAco_contents"
                }
            },
            "Revision:NqFaZrhFuNgwaT4Jn_contents": {
                "__typename": "Revision",
                "_id": "NqFaZrhFuNgwaT4Jn_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Token forcing:<\/strong> A technique that constrains a model to generate specific tokens, revealing how it adapts its generation around these constraints.<\/p><p>This method helps probe how models handle different contexts and constraints during text generation.<\/p><\/div>",
                "wordCount": 35,
                "htmlHighlight": "<div><p><strong>Token forcing:<\/strong> A technique that constrains a model to generate specific tokens, revealing how it adapts its generation around these constraints.<\/p><p>This method helps probe how models handle different contexts and constraints during text generation.<\/p><\/div>",
                "plaintextDescription": "Token forcing: A technique that constrains a model to generate specific tokens, revealing how it adapts its generation around these constraints.\n\nThis method helps probe how models handle different contexts and constraints during text generation."
            },
            "JargonTerm:NqFaZrhFuNgwaT4Jn": {
                "__typename": "JargonTerm",
                "_id": "NqFaZrhFuNgwaT4Jn",
                "term": "token forcing",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": ["prefill attacks"],
                "contents": {
                    "__ref": "Revision:NqFaZrhFuNgwaT4Jn_contents"
                }
            },
            "Revision:kYkGtMAFwDqRFpD2N_contents": {
                "__typename": "Revision",
                "_id": "kYkGtMAFwDqRFpD2N_contents",
                "version": "1.0.0",
                "html": "<div><p><strong>Transcoders:<\/strong> Neural networks trained to translate between different layers or components of a model.<\/p><p>These tools help researchers understand how information is transformed as it moves through the model.<\/p><\/div>",
                "wordCount": 29,
                "htmlHighlight": "<div><p><strong>Transcoders:<\/strong> Neural networks trained to translate between different layers or components of a model.<\/p><p>These tools help researchers understand how information is transformed as it moves through the model.<\/p><\/div>",
                "plaintextDescription": "Transcoders: Neural networks trained to translate between different layers or components of a model.\n\nThese tools help researchers understand how information is transformed as it moves through the model."
            },
            "JargonTerm:kYkGtMAFwDqRFpD2N": {
                "__typename": "JargonTerm",
                "_id": "kYkGtMAFwDqRFpD2N",
                "term": "transcoders",
                "humansAndOrAIEdited": "AI",
                "approved": false,
                "deleted": false,
                "altTerms": ["cross-layer transcoders"],
                "contents": {
                    "__ref": "Revision:kYkGtMAFwDqRFpD2N_contents"
                }
            },
            "Revision:E8uoWmvStdScjxJnC": {
                "__typename": "Revision",
                "_id": "E8uoWmvStdScjxJnC",
                "version": "1.4.0",
                "html": "<p><strong>Note<\/strong>: If you’ll forgive the shameless self-promotion, <strong>applications for <\/strong><a href=\"http://tinyurl.com/neel-mats-app\"><strong>my MATS stream<\/strong><\/a><strong>&nbsp;are open until<\/strong>&nbsp;<strong>Sept 12<\/strong>. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.<\/p><p><i>Last updated Sept 2 2025<\/i><\/p><h2 data-internal-id=\"TL_DR\">TL;DR<\/h2><ul><li>This post is about the mindset and process I recommend if you want to <i>do<\/i>&nbsp;mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.<ul><li>Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.<\/li><li><strong>Learn the minimum viable basics, then do research.<\/strong>&nbsp;Mech interp is an empirical science<\/li><\/ul><\/li><li>Three stages:<ul><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learn the ropes<\/strong><\/a><strong>&nbsp;(≤1 month)<\/strong>&nbsp;learn the essentials, go breadth-first;<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Learn with research mini-projects<\/strong><\/a>&nbsp;practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full projects<\/strong><\/a>, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.<\/li><\/ul><\/li><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Stage 1:<\/strong><\/a><strong>&nbsp;Learning the Ropes<\/strong><ul><li><strong>Breadth over depth; get a good baseline not perfection<\/strong><\/li><li><strong>Learn the basics<\/strong>: <a href=\"#Machine_Learning___Transformer_Basics\">Code a transformer from scratch<\/a>, <a href=\"#Mechanistic_Interpretability_Techniques\">key mech interp techniques<\/a>, <a href=\"#Using_LLMs_for_Learning\">the landscape of the field<\/a>, <a href=\"#Machine_Learning___Transformer_Basics\">linear algebra intuitions<\/a>, <a href=\"#Mechanistic_Interpretability_Coding___Tooling\">how to write mech interp code<\/a>&nbsp;(<a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">ARENA is your friend<\/a>)<\/li><li><strong>Get your hands dirty<\/strong>: Do <i>not<\/i>&nbsp;just read things. Mech interp is a fundamentally empirical science<\/li><li><strong>Move on after a month<\/strong>. Don’t expect to feel “done” or to have covered <i>all <\/i>of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real<\/li><li><a href=\"#Using_LLMs_for_Learning\"><strong>Use LLMs extensively<\/strong><\/a>&nbsp;- they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)<\/li><\/ul><\/li><li><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><strong>Unpacking the research process<\/strong><\/a>:<ul><li><a href=\"#Unpacking_the_Research_Process\">Many skills<\/a>, categorise them by the feedback loops.<ul><li>Fast skills (minutes-hours) like write/run/debug experiments<\/li><li>Slow (weeks) like how to prioritise and when to pivot<\/li><li>Very slow (months) like generating good research ideas<\/li><\/ul><\/li><li><strong>Do <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;try to learn all skills at once<\/strong>. Focus on fast/medium skills first, then slowly expand<\/li><li><a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">4 phases of research<\/a>: finding an idea (<strong>ideation<\/strong>) -&gt; building intuition and hunches (<strong>exploration<\/strong>) -&gt; testing hypotheses (<strong>understanding<\/strong>) -&gt; refining and writing up (<strong>distillation<\/strong>)<\/li><\/ul><\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Stage 2:<\/strong><\/a><strong>&nbsp;Mini projects<\/strong>&nbsp;(1-5 days each for 2-4 weeks)<ul><li><a href=\"#Practicing_Exploration\">Exploration mindset<\/a>: <strong>Maximise information gain per unit time<\/strong>, learn how to get unstuck. You don't need a plan, so long as you're learning<\/li><li><a href=\"#Practicing_Understanding\">Understanding mindset<\/a>: <strong>Every research result is false until proven otherwise<\/strong>. The more exciting a result is, the more likely it is to be false. Be your own greatest critic<\/li><li>Idea quality (ideation) and write-ups (distillation) aren't the priority yet; <strong>taste and prioritization are learned by doing things<\/strong>.<\/li><li>Having good research ideas takes forever to learn, <strong>to choose early projects, cheat<\/strong>! <a href=\"#Choose_A_Project\">Pick well scoped projects<\/a>, eg extending a paper (ideas)<\/li><li><a href=\"#Using_LLMs_for_Research_Code\"><strong>Use LLMs extensively<\/strong><\/a><strong>&nbsp;<\/strong>- they should speed up your research/coding a <i>lot<\/i>&nbsp;(if you know how to use them properly!)<\/li><\/ul><\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Stage 3:<\/strong><\/a><strong>&nbsp;Towards full projects<\/strong><ul><li><strong>Work in 1-2 week sprints<\/strong>, post-mortem after each, pivot to another project unless it's going <i>great<\/i><\/li><li><a href=\"#Deepening_Your_Skills\"><strong>Slower skills<\/strong><\/a><strong>&nbsp;and <\/strong><a href=\"#Key_Research_Mindsets\"><strong>key mindsets<\/strong><\/a>: careful skepticism, awareness of the literature, prioritization, high productivity<\/li><li><a href=\"#Doing_Good_Science\"><strong>Do good science<\/strong><\/a><strong>, not flashy science<\/strong>&nbsp;- be honest about limitations, give proof you're not cherry picking, read your data, do the simple things that work, use real baselines.<\/li><li><a href=\"#Write_up_your_work_\"><strong>Write-up<\/strong><\/a><strong>&nbsp;your work<\/strong>! Distill it into a narrative, then iteratively expand it to a write-up<ul><li><strong>Good public work is <\/strong><a href=\"#Why_aim_for_public_output_\"><strong>your best credential<\/strong><\/a>&nbsp;- for careers, PhDs, finding mentors, etc<\/li><li><strong>Writing is not an afterthought<\/strong>&nbsp;- make time for it. <a href=\"#Common_mistakes\">The reader will understand less than you think<\/a><\/li><\/ul><\/li><li><strong>Practice <\/strong><a href=\"#Practicing_Ideation\"><strong>generating research ideas<\/strong><\/a>. If possible, try to imitation learn <a href=\"#Research_Taste_Exercises\">a mentor's research taste.<\/a><ul><li><a href=\"#Avoiding_Fads\">Avoid fads<\/a>, and think about <a href=\"#What_s_New_In_Mech_Interp_\">what’s new and exciting in mech interp<\/a><\/li><\/ul><\/li><\/ul><\/li><li><a href=\"#Advice_on_finding_a_mentor\"><strong>Proactively reach out to mentors<\/strong><\/a>&nbsp;Everything is <i>much<\/i>&nbsp;easier with a good mentor. Cold email, apply for mentoring programs, etc.<ul><li>Reach out to researchers who'll have time, not the most famous<\/li><\/ul><\/li><li><strong>Careers:<\/strong>&nbsp;If you want to work in the field, apply for things! <a href=\"#Where_to_apply\">Jobs<\/a>, <a href=\"#Mentoring_programs\">mentoring programs<\/a>, <a href=\"#Applying_for_grants\">funding<\/a>, <a href=\"#Relevant_Academic_Labs\">academic labs<\/a>.<ul><li>Bonus thoughts: <a href=\"#What_do_hiring_managers_look_for\">what do hiring managers look for<\/a>, <a href=\"#So_what_does_a_research_mentor_actually_do_\">what does a good research mentor actually do<\/a>, and <a href=\"#Should_you_do_a_PhD_\">should you do a PhD<\/a>?<\/li><\/ul><\/li><li>I also give various thoughts on how I'm thinking about the field nowadays, and what I’ve changed my mind about. I separate these from the practical advice, so you can take it or leave it.<ul><li>Covering: <a href=\"#Interlude__What_is_mech_interp_\">how I currently define the field<\/a>, why I'm <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">pessimistic on ambitious reverse engineering, and excited about more pragmatic approaches<\/a>, <a href=\"#What_s_New_In_Mech_Interp_\">what recent work I am<i>&nbsp;<\/i>excited about<\/a>&nbsp;and recommend building on.<\/li><li>And if any of that worldview appeals, you may want to apply to work with me via <a href=\"http://tinyurl.com/neel-mats-app\">MATS, due Sept 12<\/a>!<\/li><\/ul><\/li><\/ul><h2 data-internal-id=\"Introduction\">Introduction<\/h2><p>Mechanistic interpretability (mech interp) is, in my incredibly biased opinion, one of the most exciting research areas out there. We have these incredibly complex AI models that we don't understand, yet there are tantalizing signs of real structure inside them. Even partial understanding of this structure opens up a world of possibilities, yet is neglected by 99% of machine learning researchers. There’s so much to do!<\/p><p>I think mech interp is an unusually easy field to learn about on your own: there’s a lot of educational materials, you don’t need too much compute, and there’s short feedback loops. But if you're new, it can feel pretty intimidating to get started. This is my updated guide on how to skill up, get involved, reach the point where you can do actual research, and some advice on how to go from there to a career/academic role in the field.<\/p><p>This guide is deliberately highly opinionated. My goal is to convey a productive mindset and concrete steps that I think will work well, and give a sense of direction, rather than trying to give a fully broad overview or perfect advice. (And many of the links are to my own work because that's what I know best. Sorry!)<\/p><h3 data-internal-id=\"High_Level_Framing\">High-Level Framing<\/h3><p>My core philosophy for getting into mech interp is this: learn the absolute minimal basics as quickly as possible, and then immediately transition to learning by doing research.<\/p><p>The goal is not to read every paper before you touch research. When doing research you'll notice gaps and go back to learn more. But being grounded in a project will give you vastly more direction to guide your learning, and contextualise why anything you’re learning actually matters. You just want enough grounding to start a project with some understanding of what you’re doing.<\/p><p>Don't stress about the research quality at first, or having the perfect project idea. Key skills, like <a href=\"https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/Ldrss6o3tiKT6NdMm\">research taste<\/a>&nbsp;and the ability to prioritize, take time to develop. Gaining experience—even messy experience—will teach you the basics like how to run and interpret experiments, which in turn help you learn the high-level skills.<\/p><p>I break this down into three stages:<\/p><ol><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learning the ropes<\/strong><\/a>, where you work through the basics breadth first, and after at most a month, move on to stage 2<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Practicing research with mini-projects<\/strong><\/a>. Work on throwaway, 1-5 day research projects. Focus on practicing the basic research skills with the fastest feedback loops, don’t stress about having the best ideas, or writing them up. After 2-4 weeks, move on to stage 3<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full-projects<\/strong><\/a>: work in 1-2 week sprints. After each, do a post-mortem and pivot to something else, <i>unless <\/i>it was going great and has momentum. Eventually, you should end up working on something longer-term. Start thinking about the deeper skills and research mindsets, practice having good ideas, and prioritize making good public write-ups of sprints that went well<\/li><\/ol><h2 data-internal-id=\"Stage_1__Learning_the_Ropes\">Stage 1: Learning the Ropes<\/h2><p>Your goal here is learning the basics: how to write experiments with a mech interp library, understanding the key concepts, getting the lay of the land.<\/p><p data-internal-id=\"ftnt_ref1\">Your aim is learning enough that the rest of your learning can be done via doing research, <i>not<\/i>&nbsp;finishing learning. Prioritize ruthlessly. <strong>After max 1 month<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-noteref\" id=\"fnrefnifk1wb1jum\"><sup><a href=\"#fnnifk1wb1jum\">[1]<\/a><\/sup><\/span><strong>, move on to stage 2<\/strong>. I’ve flagged which parts of this I think are essential, vs just nice to have.<\/p><p><strong>Do not just read papers <\/strong>- a common mistake among academic types is to spend months reading as many papers as they can get their hands on before writing code. Don’t do it. Mech interp is an empirical science, getting your hands dirty gives key context for your learning. Intersperse reading papers with doing coding tutorials or small research explorations. See <a href=\"https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">my research walkthroughs<\/a>&nbsp;for an idea of what tiny exploratory projects can look like.<\/p><p>LLMs are a key tool - see <a href=\"#h.ab01gbohcxm5\">the section below<\/a>&nbsp;for advice on using them well<\/p><h3 data-internal-id=\"Machine_Learning___Transformer_Basics\"><strong>Machine Learning &amp; Transformer Basics<\/strong><\/h3><p><i>Assuming you already know basic Python and introductory ML concepts.<\/i><\/p><ul><li><strong>Maths:<\/strong><ul><li><strong>Linear Algebra is King (Essential):<\/strong>&nbsp;You need to think in vectors and matrices fluently. This is by far the highest value set of generic math you should learn to do mech interp or ML research.<ul><li><i>Resource:<\/i>&nbsp;3Blue1Brown's<a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\">&nbsp;Essence of Linear Algebra<\/a>.<\/li><li><strong>Highly recommended<\/strong>: Put <a href=\"https://transformer-circuits.pub/2021/framework/index.html\">A Mathematical Framework For Transformer Circuits<\/a>&nbsp;in the context window and have the LLM generate exercises to test your intuitions about transformer internals.<\/li><li>LLMs are great for checking whether linear algebra actually clicks. Try summarizing what you've learned and the links between different concepts and ask an LLM whether you are correct. For example:<ul><li>Ensure you understand SVD and why it works<\/li><li>What does changing basis mean and why does it matter<\/li><li>Key ways a low rank and full rank matrix differ<\/li><\/ul><\/li><\/ul><\/li><li><strong>Other Bits:<\/strong>&nbsp;Basic probability, info theory, optimization, vector calculus.<ul><li>Use an LLM tutor to quiz your understanding on the parts most relevant to transformers<\/li><\/ul><\/li><li>Generally don’t bother learning other areas of maths (unless doing it for fun!)<\/li><\/ul><\/li><li><strong>Practical ML with PyTorch: (Essential)<\/strong><ul><li><p data-internal-id=\"ftnt_ref2\">Code a simple Transformer (like GPT-2) from scratch. ARENA Chapter 1.1 is a great coding tutorial<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-noteref\" id=\"fnrefue9pdw6v8rj\"><sup><a href=\"#fnue9pdw6v8rj\">[2]<\/a><\/sup><\/span><\/p><ul><li><p data-internal-id=\"ftnt_ref2\">This builds intuitions for mech interp <i>and <\/i>on using PyTorch.<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">I have two video tutorials on this, starting from the basics - <a href=\"https://www.youtube.com/watch?v=bOYE6E8JrtU&amp;list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz\">start here<\/a>&nbsp;if you’re not sure what to do!<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">And use LLMs to fill in any background things you’re missing, like PyTorch basics<\/p><\/li><\/ul><\/li><\/ul><\/li><li><strong>Cloud GPUs:<\/strong><ul><li>You’ll need to be able to run language models, which (typically) needs a GPU<\/li><li>You can start with Google Colab to get started fast, but it’ll be very constraining to use long-term. Learn to rent and use a cloud GPU.<ul><li>Newer Macbook Pros, or computers with powerful gaming GPUs may also be able to run LLMs locally<\/li><\/ul><\/li><li><i>Resource:<\/i>&nbsp;ARENA has a<a href=\"https://arena-appendix.streamlit.app/cloud-gpus\">&nbsp;<\/a><a href=\"https://arena-chapter0-fundamentals.streamlit.app/#vm-setup-instructions\">guide<\/a>. I like<a href=\"http://runpod.io/\">&nbsp;<\/a><a href=\"http://runpod.io\">runpod.io<\/a>&nbsp;as a provider;<a href=\"http://vast.ai/\">&nbsp;vast.ai<\/a>&nbsp;is cheaper.<\/li><li>nnsight also lets you do some <a href=\"https://nnsight.net/notebooks/tutorials/get_started/start_remote_access/\">interpretability on certain models they host themselves<\/a>, including LLaMA 3 405B, which can be a great way to work with larger models.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Mechanistic_Interpretability_Techniques\">Mechanistic Interpretability Techniques<\/h3><p>A lot of mech interp research looks like knowing the right technique to apply and in what context. This is a key thing to prioritise getting your head around when starting out. You’ll learn this with a mix of reading educational materials and doing coding tutorials like ARENA (discussed in next sub-section).<\/p><ul><li><a href=\"https://arxiv.org/abs/2405.00208\">Ferrando et al<\/a>&nbsp;is a good <strong>overview<\/strong>&nbsp;of the key techniques - it’s long enough that you shouldn’t prioritise reading it in full, but it’s a great reference<ul><li>Put it in a LLM context window and ask questions, or to write you exercises<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Essential<\/strong>: Make sure you understand these <strong>core techniques<\/strong>, well enough that you can code it up yourself on a simple model like GPT-2 Small<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-noteref\" id=\"fnrefhh6mwdeo4zm\"><sup><a href=\"#fnhh6mwdeo4zm\">[3]<\/a><\/sup><\/span>:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Activation Patching<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Linear Probes<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Using Sparse Autoencoders (SAEs) (you only need to write code that uses an SAE, not trains one)<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Max Activating Dataset Examples<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Nice-to-have:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Steering Vectors<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Direct Logit Attribution (DLA) (a simpler version is called logit lens)<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Key exercise<\/strong>: Describe each technique to an LLM with Ferrando et al in the context window and ask for feedback. Iterate until you get it all right.<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Use an anti-sycophancy prompt to get real feedback, by pretending someone else wrote your answer, e.g. “I saw someone claim this, it seems pretty off to me, can you help me give them direct but constructive feedback on what they missed? [insert your description]”<\/p><\/li><\/ul><\/li><\/ul><\/li><li>Remember that there’s a bunch of valuable <strong>black-box interpretability <\/strong>techniques! (ie that don’t use the model’s internals) You can often correctly guess a model’s algorithm by reading its chain of thought. Careful variation of the prompt is a powerful way to causally test hypotheses.<ul><li>They’re an additional tool. Often the correct first step in an investigation is just talking to the model and bunch and observing its behaviour. Don’t be a purist and dismiss them as “not rigorous” - they have uses and flaws, just like any other technique.<ul><li>One <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">project I supervised<\/a>&nbsp;on interpreting “self-preservation” in frontier models started with simple black-box techniques, and it just worked, we never needed anything fancier.<\/li><\/ul><\/li><li>Understand fancier black-box techniques like <a href=\"https://arxiv.org/abs/2312.12321\">token forcing<\/a>&nbsp;(aka prefill attacks) where you put words in a model’s mouth.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Mechanistic_Interpretability_Coding___Tooling\">Mechanistic Interpretability Coding &amp; Tooling<\/h3><ul><li><p data-internal-id=\"ftnt_ref4\"><strong>Goal:<\/strong>&nbsp;Get comfortable running experiments and \"playing\" with model internals. Get the engineering basics down<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-noteref\" id=\"fnrefsxyjce3nii\"><sup><a href=\"#fnsxyjce3nii\">[4]<\/a><\/sup><\/span>. Get your hands dirty<strong>.<\/strong><\/p><\/li><li><strong>ARENA<\/strong>: ARENA has <a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">a set of fantastic coding tutorials by Callum McDougall<\/a>, you should just go do these. But there’s tons, so <strong>prioritize ruthlessly<\/strong>.<ul><li><strong>Essential<\/strong><i><strong>:<\/strong><\/i><strong>&nbsp;Chapter 1.2<\/strong>&nbsp;(Interpretability Basics – prioritize the first 3 sections on tooling, direct observation, and patching).<\/li><li><i>Recommended: <\/i>1.4.1 (Causal Interventions &amp; Activation Patching – this is a core technique).<\/li><li><i>Worthwhile<\/i>: 1.3.2 (Sparse Autoencoders (SAEs) – Skim or Skip section 1, the key thing to get from the rest is an intuition for what SAEs are, strengths and weaknesses, and how to use an open source SAE. Don’t worry about training them).<\/li><\/ul><\/li><li><strong>Tooling <\/strong>(<strong>Essential<\/strong>)<strong>:<\/strong>&nbsp;Get proficient with at least one mech interp library, this is what you’ll use to run experiments.<ul><li><a href=\"https://github.com/TransformerLensOrg/TransformerLens\">TransformerLens<\/a>: best for small models &lt;=9B where you want to write more complex interpretability experiments, or work with many models at once.<ul><li>As of early Sept 2025, TransformerLens <a href=\"https://github.com/TransformerLensOrg/TransformerLens/releases/tag/v3.0.0a5\">v3<\/a>&nbsp;is in alpha, works well with large models and is far more flexible.<\/li><\/ul><\/li><li><a href=\"http://nnsight.net/\">nnsight<\/a>: More performant, works well on larger models, it’s just a wrapper around standard LLM libraries like HuggingFace transformers<\/li><\/ul><\/li><li><strong>LLM APIs<\/strong>: Learn how to use an LLM API to call an LLM programmatically. This is super useful for measuring qualitative things about some data, and for generating synthetic datasets<ul><li>I like <a href=\"http://openrouter.ai\">openrouter.ai<\/a>&nbsp;which lets you access almost all the important LLMs from a single place. GPT5 and Gemini are reasonably priced and good defaults, they have a range of sizes<ul><li>Cerebras and Groq have <i>way <\/i>higher throughput than normal providers, and serve a handful of open source models, they may be worth checking out.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref6\">Exercise: Make a happiness steering vector (for e.g. GPT-2 Small) by having an LLM via an API generate 32 happy prompts and 32 sad prompts, and taking the difference in mean activations<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-noteref\" id=\"fnrefkte6u8splw\"><sup><a href=\"#fnkte6u8splw\">[5]<\/a><\/sup><\/span>&nbsp;(e.g. the residual stream at the middle layer). Add this vector to the model’s residual stream<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-noteref\" id=\"fnref2ob115pcmet\"><sup><a href=\"#fn2ob115pcmet\">[6]<\/a><\/sup><\/span>&nbsp;while generating responses to some example prompts, and use an LLM API to rate how happy they seem, and see this score go up when steering.<\/p><\/li><\/ul><\/li><li><strong>Open source LLMs<\/strong>: You’ll want to work a lot with open source LLMs, as the thing you’re trying to interpret. The best open source LLM changes a lot<ul><li><p data-internal-id=\"ftnt_ref7\">As of early Sept 2025, Qwen3 is a good default model family. Each model has reasoning and non-reasoning mode, there’s a good range of sizes, and most are dense<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-noteref\" id=\"fnref1b9r0ass7sd\"><sup><a href=\"#fn1b9r0ass7sd\">[7]<\/a><\/sup><\/span>&nbsp;<\/p><ul><li><p data-internal-id=\"ftnt_ref7\">Gemma 3 and LLaMA 3.3 are decent non-reasoning models. I’ve heard bad things about gpt-oss and LLaMA 4<\/p><\/li><\/ul><\/li><li><i>Gotcha: <\/i>The different open source LLMs often have different tokenizations and formats for chat or reasoning tokens. Using the wrong token format can only somewhat degrade performance and may be hard to notice while corrupting your results - keep an eye out, try hard to find where this might be documented, and sanity check by e.g. comparing to official evals<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Understanding_the_literature\">Understanding the literature<\/h3><p>Your priority is to understand the concepts and the basics, but you want a sense for the landscape of the field, so you should practice reading at least some papers.<\/p><ul><li>Remember, <strong>breadth over depth<\/strong>. Skim things, get a sense of what's out there, and only dive into the things that are most interesting.<ul><li>You should be heavily using <strong>LLMs<\/strong>&nbsp;here. Give them something you're considering reading and get a summary, ask questions about the work, summarise your understanding to it and ask for feedback (with an anti-sycophancy prompt).<ul><li>If you aren't able to verify yourself, cross-reference by asking multiple LLMs and making sure they all say consistent things.<\/li><\/ul><\/li><\/ul><\/li><li>Here’s <a href=\"https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite\">a list of my favourite papers<\/a>&nbsp;(as of mid 2024) with summaries and opinions<ul><li>Do <i>not <\/i>try to read all of these in full. Skim summaries, skim abstracts, pick a few to explore deeper with an LLM, <i>then<\/i>&nbsp;decide if you want to read the full paper.<\/li><li><a href=\"https://www.youtube.com/@neelnanda2469\">My YouTube Channel<\/a>:<a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T&amp;index=1\">&nbsp;<\/a><a href=\"https://www.youtube.com/watch?v=KV5gbOmHbjU&amp;list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&amp;pp=gAQB\">Paper walkthroughs<\/a>, <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">recordings of myself doing research<\/a>, and talks.<\/li><\/ul><\/li><li><a href=\"https://arxiv.org/abs/2501.16496\">Open Problems In Mechanistic Interpretability<\/a>&nbsp;is a decent recent literature review, that a lot of top mech interp people were involved in<ul><li>Be warned that the paper basically consists of a bunch of opinionated and disagreeable researchers writing their own sections and often having strong takes. Don’t defer to it too much, but it's a good way to quickly assess what's out there.<\/li><\/ul><\/li><li><strong>Deep dives<\/strong>: You should read at least one paper carefully and in full. This is a useful skill that you will use in research projects where there’s a handful of extremely relevant papers to your project<ul><li>This is much more than just reading the words! You should write out a summary, try to understand the surrounding context with LLM help, be able to describe why the paper exists, the motivation, the problem it's trying to solve, etc.<\/li><li>Aim for a barbell strategy: put minimal effort into most papers and a lot of effort into a few.<\/li><\/ul><\/li><li><strong>LLMs<\/strong>: LLMs are a super useful tool for exploring the literature, but easy to shoot yourself in the foot with.<ul><li>As a search engine over the literature (especially with some lit reviews in context, or a starting paper), basically doing a lit review, finding relevant work for a question you have, etc.<ul><li><p data-internal-id=\"ftnt_ref8\">As a tool to help you skim a paper - put the paper in the context window<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-noteref\" id=\"fnrefbzop9pji3nl\"><sup><a href=\"#fnbzop9pji3nl\">[8]<\/a><\/sup><\/span>&nbsp;then get a summary, ask it questions, etc<\/p><\/li><li>If you’re concerned about hallucinations, you can&nbsp;ask it to support answers with quotes (and verify these are real and make sense), or give its answer to another LLM and ask for harsh critique of all the inaccuracies. Honestly, I often don’t bother though, frontier reasoning models are pretty good now.<\/li><\/ul><\/li><li>As a tool to help with deep dives - you need to actually read the paper, but I recommend having the LLM chat open as you read with the paper in the context and asking it questions, for context, etc every time you get confused.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Using_LLMs_for_Learning\">Using LLMs for Learning<\/h3><p><i>Note: I expect this section to go out of date fast! Written early Sept 2025<\/i><\/p><p>LLMs are a super useful tool for learning, especially in a new field. While they struggle to beat experts, they often beat novices. If you aren’t using them regularly throughout this process, I’d guess you’re leaving a bunch of value on the table.<\/p><p>But LLMs have weird flaws and strengths, and it’s worth being intentional about how you use them:<\/p><ul><li><strong>Use a good model<\/strong>:&nbsp;The best paid models are way better than e.g. free ChatGPT. Don't be a cheapskate; if you can, get a $20/month subscription, it makes a big difference. Gemini 2.5 Pro, Claude 4.1 Opus with extended thinking, and GPT-5 Thinking are all reasonable. (do <i>not <\/i>use non-thinking GPT-5 or anything older like GPT-4o, reasoning models are a big upgrade)<ul><li>If you can’t get a subscription, Gemini 2.5 Pro is also available for free, and is the best.<\/li><li>Use Gemini 2.5 Pro via <a href=\"https://aistudio.google.com/prompts/new_chat\">AI Studio<\/a>, it’s way better than the main Gemini interface&nbsp;and has much nicer rate limits for free users. Always use compare mode (the button in the header with two arrows) to see two responses in parallel from Pro<\/li><li>See <a href=\"https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher?commentId=jDzbZGnjWDMsNjDPQ\">thoughts<\/a> from my MATS alum Paul Bogdan comparing different LLMs for learning, and why he currently prefers Gemini<\/li><\/ul><\/li><li><strong>System Prompts:<\/strong>&nbsp;System prompts make a big difference - be concrete and specific about what you want, and how you want it done.<ul><li>LLMs are good at this: I'll just ramble at one about what the task is, my criteria, the failure modes I don't want, and then it’ll just write the prompt for me<\/li><li>If the prompt doesn’t work, tell the LLM what it did wrong, and see if it can rewrite the prompt for you.<\/li><\/ul><\/li><li><strong>Merge perspectives<\/strong>:<ul><li>Ask a Q to multiple different frontier LLMs, give LLM B’s response to LLM A and ask it to assess the strengths and weaknesses then merge.<ul><li>If a point is in both original responses, it’s probably not a hallucination<\/li><\/ul><\/li><li>If you want to fact check an LLM’s answer, give it to another LLM with an anti-sycophancy prompt<\/li><\/ul><\/li><li><strong>Anti-Sycophancy Prompts:<\/strong>&nbsp;LLMs are bad at giving critical feedback. Frame your request so the sycophantic thing to do is to be critical, by pretending someone else wrote the thing you want feedback on.<ul><li><i>\"A friend wrote this explanation and asked for brutally honest feedback. They'll be offended if I hold back. Please help me give them the most useful feedback.\"<\/i><\/li><li><i>\"I saw someone claiming this, but it seems pretty dumb to me. What do you think?\"<\/i><\/li><li><i>“Some moron wrote this thing, and I find this really annoying. Please write me a brutal but truthful response”<\/i><\/li><\/ul><\/li><li><strong>Learn actively, not passively:<\/strong><ul><li><strong>Summarize <\/strong>your understanding back to the LLM in your own words and ask for critical feedback. Do this every time you read a paper or learn about a new concept<\/li><li>Try having it teach you <strong>socratically<\/strong>. Note: you can probably design a better system prompt than the official “study mode”<\/li><li>Ask the LLM to <strong>generate exercises<\/strong>&nbsp;to test your understanding, including maths and coding exercises as appropriate.<ul><li>Gemini can make multiple choice quizzes, which some enjoy<\/li><li>Coding exercises can be requested with accompanying tests, and template code with blank functions for you to fill out, a la the ARENA tutorials.<\/li><\/ul><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref9\"><strong>Context engineering:<\/strong>&nbsp;Modern LLMs are much more useful with relevant info in context. If you give them the paper in question, or source code of the relevant library<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-noteref\" id=\"fnref207k0k5nobb\"><sup><a href=\"#fn207k0k5nobb\">[9]<\/a><\/sup><\/span>, they’ll be far more helpful.<\/p><ul><li><p data-internal-id=\"ftnt_ref9\">See <a href=\"https://drive.google.com/drive/u/0/folders/1GfrgKJwndk-twnJ8K7Ba-TE9i_8wBWAU\">this folder<\/a>&nbsp;for a bunch of saved context files for mech interp queries. If you don’t know what you need, just use <a href=\"https://drive.google.com/file/d/18cF3lkU17_elUSv0zk8KSVejM1jGfNnz/view?usp=drive_link\">this default file<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref9\">I recommend Gemini 2.5 Pro (1M context window) via<a href=\"http://aistudio.google.com/\">&nbsp;aistudio.google.com<\/a>; the UI is better. Always turn compare mode on, you get two answers in parallel<\/p><\/li><\/ul><\/li><li><strong>Voice dictation<\/strong>: If you dictate to your LLM, via free speech-to-text software, and run it with no editing, it’ll understand fine. I personally find this much easier, especially when brain-dumping.<ul><li><a href=\"http://superwhisper.com\">Superwhisper<\/a>&nbsp;on Mac is great; Superwhisper is not currently available on Windows, but Windows users can use <a href=\"https://wisprflow.ai/\">Whispr Flow<\/a>.<\/li><\/ul><\/li><li><strong>Coding<\/strong>: LLM tools like Cursor are great for coding, but <i>not <\/i>if your goal is to learn. For things like ARENA, only let yourself use browser-based LLMs, and only use them as a tutor. Don’t copy and paste code, your goal is to learn not complete exercises.<\/li><\/ul><h2 data-internal-id=\"Interlude__What_is_mech_interp_\">Interlude: What is mech interp?<\/h2><p><i>Feel free to skip to the<\/i>&nbsp;<i>“<\/i><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p data-internal-id=\"ftnt_ref10\">At this point it’s worth reflecting on what mech interp actually <i>is<\/i>. What are we even doing here? There isn't a consensus definition on how exactly to define mechanistic interpretability, and different researchers will give very different takes. But <i>my<\/i>&nbsp;working definition is as follows<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-noteref\" id=\"fnref979wnkvgpa4\"><sup><a href=\"#fn979wnkvgpa4\">[10]<\/a><\/sup><\/span>.<\/p><ul><li><strong>Interpretability<\/strong>&nbsp;is the study of understanding models, gaining insight into their behavior, the cognition inside of them, why and how they work, etc. This is the important part and the heart of the field.<\/li><li><strong>Mechanistic<\/strong>&nbsp;means using the internals of the model, the weights and activations<\/li><li>So <strong>mechanistic interpretability <\/strong>is any approach to understanding the model that uses its internals.<ul><li>This is distinct from some other worthwhile directions, like <strong>black box interpretability<\/strong>, understanding models without using the internals, and <strong>model internals<\/strong>, using the internals of the model for other things like steering vectors.<\/li><\/ul><\/li><\/ul><p><strong>Why this definition?<\/strong>&nbsp;To do impactful research, it's often good to find the directions that other people are missing. I think of most of machine learning as non-mechanistic non-interpretability. 99% of ML research just looks at the inputs and outputs to models, and treats its north star as controlling their behavior. Progress is defined by making a number go up, not to explain why it works. This has been very successful, but IMO leaves a lot of value on the table. Mechanistic interpretability is about doing better than this, and has achieved a bunch of cool stuff, like <a href=\"https://arxiv.org/abs/2310.16410\">teaching grandmasters how to play chess better by interpreting AlphaZero<\/a>.<\/p><p><strong>Why care?<\/strong>&nbsp;Obviously, our goal is not “do things if and only if they fit the above definition”, but I find it a useful one. To discuss this, let’s first consider our actual goals here. To me, <strong>the ultimate goal is to make human-level AI systems (or beyond) safer<\/strong>. I do mech interp because I think we’ll find enough understanding of what happens inside a model to be pragmatically useful here (also, because mech interp is fun!): to better understand how they work, detect if they're lying to us, detect and diagnose unexpected failure modes, etc. But people’s goals vary, e.g. real-world usefulness today, aesthetic beauty, or scientific insight. It’s worth thinking about what yours are.<\/p><p>Some implications of this framing worth laying out:<\/p><ul><li>My ultimate <strong>north star is pragmatism<\/strong>&nbsp;- achieve enough understanding to be (reliably) useful. Subgoals like “completely reverse engineer the model” are just means to an end.<ul><li>One of my big shifts in research prioritization in recent years is concluding that <strong>reverse engineering is not the right aim<\/strong>. Instead, I think we should just more directly try to do pragmatic work that enables us to do useful things using internals. I discuss this shift more <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">later on<\/a>.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref11\">This is a <strong>broad definition<\/strong>. Historically, the field has focused on more specific agendas, like ambitious reverse engineering of models. But I think we shouldn’t limit ourselves, there’s many other important and neglected directions and the field is large enough to cover a lot of ground<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-noteref\" id=\"fnref3zw26zes9dx\"><sup><a href=\"#fn3zw26zes9dx\">[11]<\/a><\/sup><\/span><\/p><\/li><li>It’s about <strong>understanding<\/strong>, not just using internals - model internals methods like steering vectors can be useful for shaping a model’s behaviour, but compete with many powerful methods like prompting and fine-tuning. Very few areas of ML can achieve understanding<\/li><li><strong>Don’t be a purist<\/strong>&nbsp;- using internals is a means to an end. If black-box methods are the right tool, use them<\/li><\/ul><h2 data-internal-id=\"The_Big_Picture__Learning_the_Craft_of_Research\">The Big Picture: Learning the Craft of Research<\/h2><p data-internal-id=\"ftnt_ref12\">So, you've gone through the tutorials, you understand the core concepts, and you can write some basic experimental code. Now comes the hard part: learning how to actually do mech interp research<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-noteref\" id=\"fnref7cxhc64szn8\"><sup><a href=\"#fn7cxhc64szn8\">[12]<\/a><\/sup><\/span>.<\/p><p>This is an inherently difficult thing to learn, of course. But IMO people often misunderstand what they need to do here, try to learn everything at once, or more generally make life unnecessarily hard for themselves. The key is to break the process down, understand the different skills involved, and focus on <strong>learning the pieces with the fastest feedback loops first<\/strong>.<\/p><p data-internal-id=\"ftnt_ref13\">I suggest breaking this down into two stages<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-noteref\" id=\"fnref9wj0u0qz3q\"><sup><a href=\"#fn9wj0u0qz3q\">[13]<\/a><\/sup><\/span>.<\/p><p><strong>Stage 2<\/strong>: working on a bunch of throwaway mini projects of 1-5 days each. Don't stress about choosing the best projects or producing public output. The goal is to learn the skills with the fastest feedback loops.<\/p><p><strong>Stage 3: <\/strong>After a few weeks of these, start to be more ambitious: paying more attention to how you choose your projects, gaining the subtler skills, and how to write things up. I still recommend working iteratively, in one to two week sprints, but ending up with longer-term projects if things go well.<\/p><p>Note: Unlike stage 1 to 2, the transition from stages two to three should be fairly gradual as you take on larger projects and become more ambitious. A good default would be after three to four weeks in stage two, but you don’t need to have a big formal shift.<\/p><p><strong>Mentorship<\/strong>: A good mentor is a major accelerator, and finding one should be a major priority for you. In the careers section, I provide advice on <a href=\"#Advice_on_finding_a_mentor\">how to go about finding a good mentor<\/a>, and <a href=\"#So_what_does_a_research_mentor_actually_do_\">how concretely they can add value<\/a>. In the rest of the post I'll write most of it assuming you do not have a mentor and then flag the ways to use a mentor where appropriate.<\/p><h3 data-internal-id=\"Unpacking_the_Research_Process\">Unpacking the Research Process<\/h3><p>I find it helpful to think of research as a cycle of four distinct stages. Read <a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">my blog post on the research proces<\/a>&nbsp;for full details, but in brief:<\/p><ul><li><strong>Ideation:<\/strong>&nbsp;You choose a research problem or a general domain to focus on.<\/li><li><strong>Exploration:<\/strong>&nbsp;You may not have a specific hypothesis yet; you’re just trying to figure out the right questions to ask, and build deeper intuition for the domain. Your north star is to gain information and surface area.<\/li><li><strong>Understanding:<\/strong>&nbsp;This begins when you have a concrete hypothesis, and some intuitive understanding of the domain. Your north star is to convince yourself that the hypothesis is true or false.<\/li><li><strong>Distillation:<\/strong>&nbsp;Once you’re convinced, your north star is to compress your findings into concise, rigorous truth that you can communicate to the world - create enough experimental evidence to convince others, write it up clearly, and share it.<\/li><\/ul><p>Underpinning these stages is a host of skills, best separated by how quickly you can apply them and get feedback. We learn by doing things and getting feedback, so you’ll learn the fast ones much more quickly. I put a rough list and categorization below.<\/p><p>My general advice is <strong>to prioritize learning these in order of feedback loops<\/strong>. If it seems like you need a slow skill to get started, like the taste to choose a good research problem, find a way to cheat rather than stressing about not having that skill (e.g. doing an incremental extension to a paper, getting one from a mentor, etc).<\/p><ul><li><strong>Fast Loop (minutes-hours):<\/strong><ul><li>Planning and writing experiment code<ul><li><strong>Medium<\/strong>: Designing great experiments<\/li><li><strong>Medium<\/strong>: Knowing when to write hacky vs. quality code.<\/li><\/ul><\/li><li>Running/debugging experiments<ul><li><strong>Medium/Slow<\/strong>: Spotting and fixing subtle bugs (e.g., you got your tokenization subtly wrong, you didn’t search hyper-parameters well enough, etc)<\/li><\/ul><\/li><li>Interpreting the results of a single experiment.<ul><li><strong>Medium<\/strong>: Understanding whether your results support your conclusions<\/li><li><strong>Slow<\/strong>: Spotting subtle interpretability illusions where your results don't actually support your claims<\/li><\/ul><\/li><\/ul><\/li><li><strong>Medium Loop (days):<\/strong><ul><li>Developing a conceptual understanding of mech interp<ul><li><strong>Slow<\/strong>: Noticing and fixing your own subtle confusions<\/li><li><strong>Slow<\/strong>: Build a deep knowledge of the literature<\/li><\/ul><\/li><li>Knowing how to explore without getting stuck<\/li><li>Writing up results<ul><li><strong>Slow<\/strong>: Communicating your work in a way that’s genuinely clear to people.<\/li><li><strong>Slow<\/strong>: Communicating why your work is <i>interesting<\/i>&nbsp;to people<\/li><\/ul><\/li><\/ul><\/li><li><strong>Slow Loop (weeks):<\/strong><ul><li>Prioritizing which experiment to do next<\/li><li>Knowing when to continue with a research direction or pivot to another angle of attack/another project<\/li><li>Identifying bad research ideas, <i>without <\/i>doing a project on them first<\/li><\/ul><\/li><li><strong>Very Slow Loop (months):<\/strong><ul><li>Coming up with good research ideas. This is the core of \"research taste.\"<\/li><\/ul><\/li><\/ul><p>Your progression should be simple: First, focus on the fast/medium skills behind exploration and understanding with throwaway projects. Then, graduate to end-to-end projects where you can intentionally practice the deeper skills, and practice ideation and distillation too.<\/p><h3 data-internal-id=\"What_is_research_taste_\">What is research taste?<\/h3><p>A particularly important and fuzzy type of skill is called research taste. I basically think of this as the bundle of intuitions you get with enough research experience that let you do things like come up with good ideas, predict if an idea is promising, have conviction in good research directions, etc. Check out <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">my post on the topic<\/a>&nbsp;for more thoughts.<\/p><p>I broadly think you should just ignore it for now, find ways to compensate for not having much yet, and focus on learning the fast-medium skills, and this will give you a much better base for learning it. In particular, it's much faster to learn with a mentor, so if you don't have a mentor at the start, you should prioritize other things.<\/p><p>But you want to learn it eventually, so it's good to be mindful of it throughout, and look for opportunities to practice and learn lessons. I recommend treating it as a nice-to-have but not stressing about it<\/p><p>Note, one important trap here is that having good taste can often manifest as having confidence and conviction in some research direction. But often novice researchers develop this confidence and conviction significantly <i>before <\/i>they develop the ability to not be confident in bad ideas. It’s often a good learning experience to once or twice pursue a thing you feel really convinced is going to be epic and then discover you're wrong, so it's not that bad an outcome, especially in stage 2 (mini-projects) but be warned.<\/p><h2 data-internal-id=\"Stage_2__Practicing_Research_with_Mini_Projects\">Stage 2: Practicing Research with Mini-Projects<\/h2><p>With that big picture in mind, let's get our hands dirty. You want to do a series of ~1-5 day mini-projects, for maybe 2-4 weeks. The goal right now is to learn the craft, not to produce groundbreaking research.<\/p><p>Focus on practicing exploration and understanding and gaining the fast/medium skills, leave aside ideation and distillation for now. If you produce something cool and want to write it up, great! But that’s a nice-to-have, not a priority.<\/p><p>Once you finish a mini-project, remember to do a post-mortem. Spend at least an hour analyzing: what did you do? What did you try? What worked? What didn't? What mistakes did you make? What would you do differently if doing this again? And how can you integrate this into your research strategy going forwards?<\/p><h3 data-internal-id=\"Choose_A_Project\">Choose A Project<\/h3><p>Some suggested starter projects<\/p><ul><li><strong>Replicate and Extend a Paper:<\/strong>&nbsp;A classic for a reason. Replicate a key result, then extend it. Suggestions:<ul><li><a href=\"https://arxiv.org/abs/2406.11717\">Refusal is mediated by a single direction<\/a><ul><li>Extending papers can vary a lot in difficulty. For example, applying the method to study refusal on a new model is easy as you can reuse the same data, while applying it to a new concept is harder.<\/li><li>Skills: practicing activation patching and steering vectors.<\/li><\/ul><\/li><li><a href=\"http://thought-anchors.com\">Thought Anchors<\/a>: apply these reasoning model interpretability methods to new types of prompts, or explore some prompts using the linked interface, or see if you can improve on the methods/invent your own.<ul><li>Skills: reasoning model interpretability, using LLM APIs, and working with modern models<\/li><\/ul><\/li><li>Replicate the truth probes in <a href=\"https://arxiv.org/abs/2310.06824\">Geometry of Truth<\/a>&nbsp;on a more modern model and try applying them in more interesting settings. How well do they generalise? Can you break them? If so, can you fix this?<ul><li>Skills: probing, supervised learning, dataset creation<\/li><\/ul><\/li><\/ul><\/li><li><strong>Play around with something interesting:<\/strong><ul><li>Use <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia's attribution graphs<\/a>&nbsp;to form a hypothesis about Gemma 2B, then use other methods (e.g. prompting) to verify it.<ul><li>Skills: Attribution graphs, scientific mindset, prompting<\/li><\/ul><\/li><li>Play with <a href=\"https://huggingface.co/collections/bcywinski/gemma-2-9b-it-taboo-6826efbb186dfce0616dd174\">Bartosz Cywiński's taboo models<\/a>&nbsp;that have a secret word programmed in and test as many methods as you can to find it.<ul><li>If you’re feeling ambitious: train your own models with a more complex secret, and try to interpret those.<\/li><li>Skills: Logit lens, SAEs, black box methods<\/li><\/ul><\/li><li>Explore <a href=\"https://github.com/clarifying-EM/model-organisms-for-EM\">the models<\/a>&nbsp;from the <a href=\"https://www.emergent-misalignment.com/\">emergent<\/a>&nbsp;<a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">misalignment<\/a>&nbsp;<a href=\"https://openai.com/index/emergent-misalignment/\">papers<\/a>.<ul><li>Skills: steering vectors, SAEs, maybe fine-tuning<\/li><\/ul><\/li><li>Pick some prompts from <a href=\"https://arxiv.org/abs/2503.08679\">Chain-of-Thought Reasoning In The Wild Is Not Always Faithful<\/a>&nbsp;and try to gain a deeper understanding of what’s happening<ul><li>Skills: Open ended exploration, using whichever tools seem appropriate<\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Those cover two kinds of starter projects:<\/p><ul><li><strong>Understanding-heavy<\/strong>, where you take a well-known domain and try to test a hypothesis there (e.g. extending a paper you’ve read closely)<ul><li>Note that you still want to do <i>some<\/i><\/li><\/ul><\/li><li><strong>Exploration-heavy<\/strong>, where you take some phenomena (a technique, a model, a phenomena, etc) play around with it, and try to understand what’s going on.<ul><li>Exploration-heavy projects are often a less familiar style, so make sure to do some of those!<\/li><\/ul><\/li><\/ul><p>Common mistakes:<\/p><ul><li>People often get hung up on finding the “best” project. Sadly, that’s not going to happen. Instead, just do something and see what happens - better ideas and inspiration come with time.<\/li><li>Don't get too attached to your first project. It was probably badly chosen! These are throwaway projects, just move on once you’re not learning as much.<\/li><li>Conversely, don't flit between ideas so much that you never build your \"getting unstuck\" toolkit.<\/li><li>Avoid compute-heavy and/papers (e.g., training cross-layer transcoders) or highly technical papers (e.g., Sparse Feature Circuits).<\/li><\/ul><h3 data-internal-id=\"Practicing_Exploration\">Practicing Exploration<\/h3><p>The idea of exploration as a phase in itself often trips up people new to mech interp. They feel like they always need to have a plan, a clear thing they're doing at any given point, etc. In my experience, you will often spend more than half of a project trying to figure out what the hell is happening and what you think your plan is. This is totally fine!<\/p><p data-internal-id=\"ftnt_ref14\">You don't need a plan. It's okay to be confused. However, this does <i>not <\/i>mean you should just screw around. Your North Star: gain information and surface area<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-noteref\" id=\"fnrefxw1ra5pqnd\"><sup><a href=\"#fnxw1ra5pqnd\">[14]<\/a><\/sup><\/span>&nbsp;on the problem. Your job is to take actions that maximise information gained per unit time. If you've learned nothing in 2 hours, pivot to another approach. If 2-3 approaches were dead ends, it’s fine to just pick another problem.<\/p><p>I have <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">several research walkthroughs on my YouTube channel<\/a>&nbsp;that I think demonstrates the mindset of exploration. What I think is an appropriate speed to be moving. E.g. I think you should aim to make a new plot every few minutes (or faster!) if experiments don't take too long to run.<\/p><p>A common difficulty is feeling “stuck” and not knowing what to do. IMO, this is largely a skill issue. Here's my recommended toolkit when this happens:<\/p><ul><li>Use \"gain surface area\" techniques, things that can surface new ideas and connections and just give you raw data to work with: look at the model's output/chain-of-thought, change the prompt, probe for a concept, look at an SAE/attribution graph, read examples from your dataset, try logit lens or steering, etc.<\/li><li>Set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5-minute timer<\/a>&nbsp;and brainstorm things you're curious about or directions to try.<\/li><li>If you’re confused/curious about something, set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5 minute timer<\/a>&nbsp;and brainstorm what could be happening.<\/li><\/ul><p>Other advice:<\/p><ul><li>Before any &gt;30 minute experiment, stop and brainstorm alternatives. Is this <i>really<\/i>&nbsp;the fastest way to gain information?<\/li><li>It's totally fine to pause for half a day to go learn some key background knowledge.<\/li><li>Get in the habit of keeping a research log of your findings and a \"highlights\" doc for the really cool stuff.<ul><li>If applicable, it can be cool to have your research log be a slack/discord channel<\/li><\/ul><\/li><li>Remember: when exploring and thinking through how to explain mysterious phenomena, most of your probability mass should be on \"something I haven't thought of yet.\"<\/li><li>Practice following your curiosity, but be aware that it’ll often lead you astray at first. When it does, pay attention! What can you learn from this?<\/li><\/ul><h3 data-internal-id=\"Practicing_Understanding\">Practicing Understanding<\/h3><p>If exploration goes well, you'll start to form hunches about the problem. E.g. thinking that you are successfully (linearly) probing for some concept. Or that you found a direction that mediates refusal. Or that days of the week are represented as a circle in a 2D subspace.<\/p><p>Once you have this, you want to go to figure out if it's actually true. Be warned, the feeling of “being really convinced that it's true” is very different from actually being true. Part of being a good researcher is being good enough at testing and falsifying your pet hypotheses that, when you fail to falsify one, there’s a good chance that it's true. But you're probably not there yet.<\/p><p>Note: While I find it helpful to think of these as discrete stages, often you'll be flitting back and forth. A great way to explore is coming up with guesses and micro-hypotheses about what's going on, running a quick experiment to test them, and integrating the results into your understanding of the problem, going back to the drawing board.<\/p><p>Your North Star: convince yourself a hypothesis is true or false. The key mindset is skepticism. Advice:<\/p><ul><li>Before testing a hypothesis, set a five-minute timer and brainstorm, \"What are the ways this could be false?\"<\/li><li>Alternatively, write out the best possible case for your hypothesis and see where the argument feels weak.<ul><li>Try using an LLM with an anti-sycophancy prompt (\"My friend wrote this and wants brutal feedback...\") to red-team your arguments - it probably won’t work, but might be helpful<\/li><\/ul><\/li><li>Or set a 5 minute timer and brainstorm alternative explanations for your observations<\/li><\/ul><p>You then want to convert these flaws and alternative hypotheses into concrete experiments. <strong>Experiment design is a deep skill<\/strong>. Honestly, I'm not sure how to teach it other than through experience. But one recommendation is to pay close attention to the experiments in papers you admire and analyze what made them so clever and effective. I also recommend that, every time you feel like you’ve (approximately) proven or falsified a hypothesis, adding them to a running doc of “things I believe to be true” with hypotheses, experiments, and results.<\/p><h3 data-internal-id=\"Using_LLMs_for_Research_Code\">Using LLMs for Research Code<\/h3><p>In my opinion, coding is one of the domains where LLMs are most obviously useful. It was very striking to me how much better my math scholars were six months ago than 12 months ago, and I think a good chunk of this is attributable by them having much better LLMs to use. If you are not using LLMs as a core part of your coding workflow, I think you're making a mistake.<\/p><ul><li><strong>Use<\/strong><a href=\"http://cursor.com/\"><strong>&nbsp;Cursor<\/strong><\/a><strong>:<\/strong>&nbsp;It's VS Code with fantastic AI integration. Make sure to add the docs for libraries with @&nbsp;so the AI has context. The $20/month plan is worth it, if possible, and there’s a <a href=\"https://cursor.com/students\">free student version<\/a>.<ul><li>Claude Code is tempting but bad for learning and iteration. I’d use it for throwaway things and first drafts - if the draft has a bunch of bugs, go read the code yourself/throw it away and start again. Cursor facilitates reading the AI’s code better than Claude code does IMO<\/li><\/ul><\/li><li><strong>A caveat:<\/strong>&nbsp;If learning a new library (like in ARENA), first try writing things yourself. Use the LLM when stuck, not to replace the learning process.<\/li><li>Later on, when thinking about writing up results, if key experiments were mostly vibe-coded, I recommend re-implementing them by hand to make sure no dumb LLM bugs slipped in.<\/li><\/ul><h2 data-internal-id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\">Interlude: What’s New In Mechanistic Interpretability?<\/h2><p><i>Feel free to skip to the “<\/i><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p>Things move fast in mechanistic interpretability. Newcomers to the field who've kept up from afar are often pretty out of date. Here's what I think you need to know, again, filtered through my own opinions and biases.<\/p><h3 data-internal-id=\"Avoiding_Fads\">Avoiding Fads<\/h3><p>This interlude is particularly important because <strong>the field often has fads<\/strong>: lines of research that are very popular for a year or so, make some progress and find many limitations, and then the field moves on. But if you’re new, and catching up on the literature, you might not realise. I often see people new to the field working on older things, that I don’t think are too productive to work on any more. Historical fads include:<\/p><ul><li>Interpreting toy models trained on algorithmic tasks (e.g. my <a href=\"https://arxiv.org/abs/2301.05217\">grokking work<\/a>)<ul><li>I no longer recommend working on this, as I think we basically know that “sometimes models trained on algorithmic tasks are interpretable”, and they’re sufficiently artificial and divorced from real models that I am pessimistic about deeper and more specific insights generalising<\/li><\/ul><\/li><li>Circuit analysis via causal interventions on model components (e.g. the <a href=\"https://arxiv.org/abs/2211.00593\">IOI paper<\/a>)<ul><li>This is slightly more complicated. I think that's worth learning about, and techniques like activation and attribution patching are genuinely useful.<\/li><li>But the core problem is that once you got a sparse subgraph of a model responsible for a task, there wasn't really a “what next?”. This didn't tend to result in deeper insight because the nodes (eg layers or maybe attention heads) weren't monosemantic, and it was often more complicated than naive stories suggested but we didn’t have the tools to dig deeper.<\/li><li>It was pretty cool to see that this was possible at all, but there have been more than enough works in this area that the bar for a novel contribution is now much higher.<\/li><li>Simply identifying a circuit is no longer enough; you need to use that circuit to reveal a deeper, non-obvious property of the model. I recommend exploring <a href=\"https://www.neuronpedia.org/graph/info\">attribution-graph style approaches<\/a><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">We're at the tail end of a fad of incremental <a href=\"https://transformer-circuits.pub/2023/monosemantic-features\">sparse autoencoder research<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-noteref\" id=\"fnreftq4gws0zq69\"><sup><a href=\"#fntq4gws0zq69\">[15]<\/a><\/sup><\/span>&nbsp;(i.e. focusing on simple uses and refinements of the basic technique)<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Calling this one a fad is probably more controversial (if only because it's more recent).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">The <i>specific<\/i>&nbsp;thing I am critiquing is the spate of papers, including ones I was involved in, that are about incremental improvements to the sparse autoencoder architecture, or initial demonstrations that you can apply SAEs to do things, or picking some downstream task and seeing what SAEs do on it.<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I think this made some sense when it seemed like SAEs could be a total gamechanger for the field, and where we were learning things from each new such paper. I think this moment has passed; I do not think they were a gamechanger in the way that I hoped they might be. See <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">more of my thoughts here<\/a>.<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">I am <i>not<\/i>&nbsp;discouraging work on the following:<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Attribution graph-based circuit analysis, which I don't think has played out yet - see <a href=\"https://www.neuronpedia.org/graph/info\">a recent overview of that sub-field I co-wrote<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Trying meaningfully different approaches to dictionary learning (eg <a href=\"https://arxiv.org/abs/2506.20790\">SPD<\/a>&nbsp;or <a href=\"https://arxiv.org/abs/2505.17769\">ITDA<\/a>), or things targeted to fix conceptual limitations of current techniques (eg <a href=\"https://arxiv.org/abs/2503.17547\">Matryoshka<\/a>).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Using SAEs as a tool, whether as part of a broader project investigating weird phenomena in model biology, or as a baseline/approach on some downstream task. The key is that the project’s motivation should <i>not <\/i>just be “what if we used SAEs for X?” unless there’s a good argument<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I particularly recommend them for tasks where you don’t know exactly what you’re looking for, e.g. trying to explore some mysterious phenomena<\/p><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Note that I am putting this after stage 2 because I think that <strong>for initial throwaway projects you should <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;be stressing about novelty and avoiding fads<\/strong>&nbsp;- your goal is just to learn. But as we move into stage 3 you should start to be a bit more mindful about choosing more exciting/impactful projects where possible.<\/p><p>Also, take these as nudges and recommendations, not as instructions. If there's a direction you believe in that fits the things I'm critiquing, maybe I'm just wrong, maybe your thing is an exception, go wild, see what happens.<\/p><h3 data-internal-id=\"What_s_New_In_Mech_Interp_\">What’s New In Mech Interp?<\/h3><p>OK, so those are my hot takes on what not to do. What should you do? I think that some really cool new opportunities have opened up in mech interp over the last year, and newcomers may not have come across these. Here are some of the key themes in my favorite papers over the last year, that I’d love to see readers build on:<\/p><ol><li><strong>Downstream tasks<\/strong>. If our North Star is to be able to do useful things with interpretability, a great way to test our techniques is on real-world tasks, to see if they beat baselines or enable us to do a new thing<ol><li>Even if we can't directly test on the tasks we most care about, if models can let us do new things in <i>some<\/i>&nbsp;domains, that's reasonable evidence that we're learning something real in a way that is hard to fake.<\/li><li>One of my favourite examples is <strong>auditing games<\/strong>: <a href=\"https://arxiv.org/abs/2503.10965\">Marks et al<\/a>. is a fantastic paper that trained a model to have a hidden goal and then had different teams compete to find this goal, some with interpretability techniques, some without. Hypothesis: If interpretability does something real, then interp teams should win.<ol><li>This study itself was a bit ambiguous (the mech interp teams were 1st and 3rd, a black box team was 2nd), but I think this is a fantastic way to test interpretability techniques, and has a ton of potential for follow-on work evaluating interpretability in different kinds of games.<\/li><li>The general idea is just that there is a model with some known hidden property and different techniques are used to see which can elicit it. Things like having different teams, allowed to use different techniques, etc. is just one implementation.<\/li><\/ol><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref16\"><strong>Model organisms<\/strong>: The auditing games paper was made possible by the fact that they were able to make a model with a hidden goal<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-noteref\" id=\"fnrefcdmsagzbqkp\"><sup><a href=\"#fncdmsagzbqkp\">[16]<\/a><\/sup><\/span>, a model organism to study. In general, we’re collecting techniques like <a href=\"https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/\">synthetic document fine-tuning<\/a>&nbsp;to make really interesting model organisms.<\/p><ol><li><p data-internal-id=\"ftnt_ref16\">This kind of thing has a lot of potential! If we want to make a lie detector, a core challenge is that we don’t know how to test if it works or not. But if we can insert beliefs or deceptive behaviours into a model, many more projects become possible<\/p><\/li><li><p data-internal-id=\"ftnt_ref16\">A great intro project is playing around with open source model organisms, e.g. from <a href=\"https://arxiv.org/abs/2505.14352\">Cywinski et al<\/a><\/p><\/li><\/ol><\/li><li><strong>Practice on the real AGI Safety problems<\/strong>: Historically, interpretability could only practice on very dull toy problems like <a href=\"https://arxiv.org/abs/2301.05217\">modular addition<\/a>. But we now have models that exhibit complex behaviors that seem genuinely relevant to safety concerns, and we can just study them directly, making it far easier to make real progress.<ol><li>E.g. <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">Rajamanoharan et al<\/a>&nbsp;debunking assumed self-preservation, and <a href=\"https://www.apolloresearch.ai/research/deception-probes\">Goldowsky-Dill et al<\/a>&nbsp;probing for deception<\/li><li>Weird behaviours: models can <a href=\"https://www.apolloresearch.ai/research/deception-probes\">insider trade then lie about it<\/a>, <a href=\"https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations\">tell when they’re being evaluated<\/a>&nbsp;(and act differently), <a href=\"https://arxiv.org/abs/2412.14093\">fake alignment<\/a>, <a href=\"https://metr.org/blog/2025-06-05-recent-reward-hacking/\">reward hack<\/a>, and more.<\/li><\/ol><\/li><li><strong>Real-World Uses of Interpretability<\/strong>: Model interpretability-based techniques are starting to have genuine uses in frontier language models!<ol><li><a href=\"https://arxiv.org/abs/1610.01644\">Linear probes<\/a>, one of the simplest possible techniques, are a highly competitive way to <a href=\"https://alignment.anthropic.com/2025/cheap-monitors/\">cheaply monitor systems<\/a>&nbsp;for things like users trying to make bioweapons.<\/li><li>I find it incredibly cool that interpretability can actually be useful, and kind of embarrassing that only a decade-old technique seems very helpful. Someone should do something about that. Maybe that someone could be you!<\/li><li>This needs a very different kind of research: careful evaluation, comparison to strong baselines, and refinement of methods<\/li><\/ol><\/li><li><strong>Attribution graph-based circuit analysis<\/strong>. The core problem with trying to analyze circuits in terms of things like a model's attention heads and layers is that often these things don't actually have a clear meaning. <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\">Attribution graphs<\/a>&nbsp;use techniques like <a href=\"https://arxiv.org/abs/2406.11944\">transcoders<\/a>, popularized in <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/biology.html\">Anthropic's model biology<\/a>&nbsp;work, to approximate models with a computational graph with meaningful nodes.<ol><li><p data-internal-id=\"ftnt_ref17\">See this <a href=\"https://www.neuronpedia.org/graph/info\">cross-org blog post<\/a>&nbsp;for the ongoing follow-on work across the community, and an open problems list I co-wrote!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-noteref\" id=\"fnrefp0f0m03b55r\"><sup><a href=\"#fnp0f0m03b55r\">[17]<\/a><\/sup><\/span><\/p><\/li><li>You can make and analyse your own attribution graphs on <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia<\/a><\/li><\/ol><\/li><li><strong>Understanding model failures<\/strong>: Models often do weird things. If we were any good at interpretability, we should be able to understand these. Recently, we’ve seen signs of life!<ol><li><a href=\"https://transluce.org/observability-interface\">Meng et al<\/a>&nbsp;on why some models think 9.8 &lt; 9.11<\/li><li><p data-internal-id=\"ftnt_ref18\">A line of work studying <a href=\"https://www.emergent-misalignment.com/\">emergent misalignment<\/a>&nbsp;- why training models on narrowly evil tasks like writing insecure code turns them into Nazis - has found some insights. <a href=\"https://arxiv.org/abs/2506.19823\">Wang et al<\/a>&nbsp;found this was driven by sparse autoencoder latents<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-noteref\" id=\"fnrefg12d8d1lqu\"><sup><a href=\"#fng12d8d1lqu\">[18]<\/a><\/sup><\/span>&nbsp;associated with movie villains, and in <a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">Turner et al<\/a>&nbsp;we found that the model <i>could <\/i>have learned the narrow solution, but this was in some sense less “efficient” and “stable”<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref20\"><strong>Automated interpretability<\/strong>: Using LLMs to automate interpretability. We saw signs of life on this from Bills et al and <a href=\"https://arxiv.org/abs/2404.14394\">Shaham et al<\/a>, but LLMs are actually good now! It’s now possible to make basic interpretability agents that can do things like <a href=\"https://alignment.anthropic.com/2025/automated-auditing/\">solve auditing games<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-noteref\" id=\"fnref0td6a2gxwht\"><sup><a href=\"#fn0td6a2gxwht\">[19]<\/a><\/sup><\/span>. And interpretability agents are the worst they’ll ever be<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-noteref\" id=\"fnref5bdglmkdzr\"><sup><a href=\"#fn5bdglmkdzr\">[20]<\/a><\/sup><\/span>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref22\"><strong>Reasoning model interpretability<\/strong>: All current frontier models are reasoning models—models that are trained with reinforcement learning to think<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-noteref\" id=\"fnrefwuxdh4f7kh\"><sup><a href=\"#fnwuxdh4f7kh\">[21]<\/a><\/sup><\/span>&nbsp;for a while before producing an answer. In my opinion, this requires a major rethinking of many existing interpretability approaches<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-noteref\" id=\"fnref3qxoen8tddk\"><sup><a href=\"#fn3qxoen8tddk\">[22]<\/a><\/sup><\/span>, and calls for exploring new paradigms. IMO this is currently being neglected by the field, but will become a big deal.<\/p><ol><li><p data-internal-id=\"ftnt_ref22\">In <a href=\"http://thought-anchors.com\">Bogdan et al<\/a>, we explored what a possible paradigm could look like. Notably, there are far more interesting and sophisticated black box techniques with reasoning models, like resampling the second half of the chain of thought, or every time the model says a specific kind of sentence, deleting and regenerating that sentence.<\/p><\/li><\/ol><\/li><\/ol><h3 data-internal-id=\"A_Pragmatic_Vision_for_Mech_Interp\">A Pragmatic Vision for Mech Interp<\/h3><p>Attentive readers may notice that the list above focuses on work to do with understanding the more qualitative high-level properties of models, and not ambitious reverse engineering. This is largely because, in my opinion, the former has gone great, while we have not seen much progress towards the fundamental blockers on the latter.<\/p><p>I used to be very excited about ambitious reverse engineering, but I currently think that the dream of completely reverse engineering a model down to something human understandable seems basically doomed. My interpretation of the research so far is that models have some human understandable high-level structure that drives important actions, and a very long tail of increasingly niche and irrelevant heuristics and biases. For pragmatic purposes, these can be largely ignored, but not if we want things like guarantees, or to claim that we have understood most of a model. I think that trying to understand as much as we can is still a reasonable proxy for getting to the point of being pragmatically useful, but think it’s historically been too great a focus of the field, and many other approaches seem more promising if our ultimate goals are pragmatic.<\/p><p>In some ways, this has actually made me more optimistic about interpretability ultimately being useful for AGI safety! Ambitious reverse engineering would be awesome but was always a long shot. But I think we've seen some real results for pragmatic approaches to mechanistic interpretability, and feel fairly confident we are going to be able to do genuinely useful things that are hard to achieve with other methods.<\/p><h2 data-internal-id=\"Stage_3__Working_Up_To_Full_Research_Projects\">Stage 3: Working Up To Full Research Projects<\/h2><p>Once you have a few mini-projects done, you should start being more ambitious. You want to think about gaining the deeper (medium/slow) skills, and exploring ideation and distillation.<\/p><p>However, you should still expect projects to often fail, and want to lean into breadth over depth and avoid getting bogged down in an unsuccessful project you can’t bear to give up on. To resolve this tension, I recommend <strong>working in 1-2 week sprints<\/strong>. At the end of each sprint, reflect and make a deliberate decision: <strong>continue, or pivot?<\/strong>&nbsp;The default should be to pivot unless the project feels truly promising. It’s great to give up on things, if it means you spend your time even better! But if it’s going great, by all means continue.<\/p><p>This strategy should mean that you eventually end up working on something longer-term when you find something <i>good<\/i>, but don't just get bogged down in the first ambitious idea you tried.<\/p><p>I recommend reviewing the list of skills earlier and just for each one, reflecting for a bit on how on top of it you think you feel and how you could intentionally practice it in your next project. Then after each sprint, before deciding whether to pivot, take an hour or two to do a post-mortem: what did you learn, what progress did you make on different skills, and what would you do differently next time? Your goal is to learn, and you learn much better if you make time to actually process your accumulated data!<\/p><h3 data-internal-id=\"Key_Research_Mindsets\">Key Research Mindsets<\/h3><p>One way to decompose your learning is to think about research mindsets: the traits and mindsets a good researcher needs to have, that cut across many of these stages. See <a href=\"https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking\">my blog post on the topic for more<\/a>, but here's a brief view of how I'm currently thinking about it.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\"><strong>Skepticism/Truth-seeking:<\/strong>&nbsp;The default state of the world is that your research is false, because doing research is hard. Your north star should always be to find <i>true <\/i>insights<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-noteref\" id=\"fnreflm5ixkfuzk\"><sup><a href=\"#fnlm5ixkfuzk\">[23]<\/a><\/sup><\/span><\/p><ol><li><p data-internal-id=\"ftnt_ref23\">It generally doesn't come naturally to people to constantly aggressively think about all the ways their work could be false and make a good faith effort to test it. You can learn to do better than this, but it often takes practice.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">This is crucial in understanding, somewhat important in exploration, and crucial in distillation.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">A common mistake is to grasp at straws to find a “positive” result, thinking that nothing else is worth sharing.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\">In my opinion, negative or inconclusive results that are well-analyzed are much better than a poorly supported positive result. I’ll often think well of someone willing to release nuanced negative results, and poorly of someone who pretends their results are better than they are.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Prioritization:<\/strong>&nbsp;Your time is scarce. Research involves making a bunch of decisions that are essentially searching through a high-dimensional space. The difference between a great and a mediocre researcher is being able to make these decisions well.<ol><li>If you have a good mentor, you can lean on them for this at first, but you will need to learn how to do this yourself eventually.<\/li><li>This is absolutely crucial in exploration and ideation, but fairly important throughout.<\/li><li>A good way to learn this one is to reflect on decisions you've made after the fact, eg in a sprint post-mortem, and think about how you could have made them better, and what generalisable lessons to take to the future<\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\"><strong>Productivity<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-noteref\" id=\"fnrefidab8074tka\"><sup><a href=\"#fnidab8074tka\">[24]<\/a><\/sup><\/span><strong>:<\/strong>&nbsp;The best researchers I've worked with get more than twice as much done as the merely good ones. Part of this is good research taste and making good prioritization decisions, but part of this is just being good at getting shit done.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Now, this doesn't necessarily mean pushing yourself until the point of burnout by working really long hours. Or cutting corners and being sloppy. This is about productivity integrated over the long term.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">For example, sometimes the most productive thing to do is to hold off on starting work, set a 5 minute timer, brainstorm possible things to do next, and then pick the best idea<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">This takes many forms, and the highest priority for you:<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Know when to write good code without bugs, to avoid wasting time debugging later, and when to write a hacky thing that just works.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know the right keyboard shortcuts to move fast when coding.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know when to ask for help and have people who can help you get unblocked where appropriate.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be good at managing your time and tasks so that once you've decided what the highest priority thing to work on is, you in fact go and work on it.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be able to make time to achieve deep focus on the key problems.<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">Exercise: Occasionally <strong>audit your time<\/strong>. Use a tool like <a href=\"http://toggl.com\">Toggl<\/a>&nbsp;for a day or two to log what you're doing, then reflect: where did time go? What was inefficient? How could I do this 10% faster next time?<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">The goal isn't to feel guilty, but to spot opportunities for improvement, like making a utility function for a tedious task.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Knowing the literature<\/strong>: At this point, there’s a lot of accumulated wisdom (and a lot of BS) in prior papers, in mech interp and beyond.<ol><li>This cuts across all stages:<ol><li>In ideation, you don’t want to accidentally reinvent the wheel. And often great ideas are inspired by prior work<\/li><li>In exploration, you want to be able to spot connections, borrow interesting techniques, etc<\/li><li>In understanding, you want to know the right standards of proof to check for, the best techniques to use, alternative hypotheses (that may have been raised in other works), etc<\/li><li><p data-internal-id=\"ftnt_ref25\">In distillation, when writing a paper you’re expected to be able to contextualise it relative to existing work (i.e. write a related work section<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-noteref\" id=\"fnrefwpekmwudkpd\"><sup><a href=\"#fnwpekmwudkpd\">[25]<\/a><\/sup><\/span>) which is important for other researchers knowing whether to care. And if you don’t know the standard methods of proof, key baselines everyone will ask about, key gotchas to check for etc, no one will believe your work.<\/p><\/li><\/ol><\/li><li>LLMs are an incredibly useful tool here. GPT-5 thinking or Claude 4 with web search are both pretty useful tools here, as are the slower but more comprehensive deep research tools (Note that Google's is available for free, as of the time of writing)<ol><li>I recommend using these regularly and creatively throughout a project.<\/li><li>You don't necessarily need to go and read the works that get surfaced, but even just having LLM summaries can get you more awareness of what's out there, and over time you'll build this into deeper knowledge.<\/li><\/ol><\/li><li>Of course, when there <i>does<\/i>&nbsp;seem to be a very relevant paper to your work, you should go do a deep dive and read it properly, not just relying on LLM summaries.<\/li><li>Don’t stress - deep knowledge of the literature takes time to build. But you want to ensure you’re on an upwards gradient here, rather than assuming the broader literature is useless<\/li><li><p data-internal-id=\"ftnt_ref26\">On the flip side, many papers <i>are <\/i>highly misleading/outright false, so please don’t just critically believe them!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-noteref\" id=\"fnref1bau7vsh9tk\"><sup><a href=\"#fn1bau7vsh9tk\">[26]<\/a><\/sup><\/span><\/p><\/li><\/ol><\/li><\/ol><p>Okay, so how does this all tie back to the stages of research? Now you're going to be thinking about all four. We'll start by talking about how to deepen your existing skills with exploration and understanding, and then we'll talk about what practicing ideation and actually writing up your work should look like.<\/p><h3 data-internal-id=\"Deepening_Your_Skills\">Deepening Your Skills<\/h3><p>You’ll still be exploring and understanding, but with a greater focus on rigor and the slower skills. In addition to the thoughts when discussing mindsets above, here’s some more specific advice<\/p><ul><li><strong>Deeper Exploration<\/strong>&nbsp;is about internalizing the mindset of maximising productivity, which here means maximising information gain per unit time. Always ask, \"Am I learning something?\"<ul><li><i>Avoid Rabbit Holes:<\/i>&nbsp;A common mistake is finding one random anomaly and zooming in on it. Knowing when to pivot is crucial. Set a timer every hour or two to zoom out and ask if you’re making progress.<ul><li>I recommend any time you notice yourself feeling a bit stuck or distracted or off track, setting a five minute timer and thinking about what could I be doing next, what should I be doing next, and am I doing the most important thing?<\/li><\/ul><\/li><li><i>Avoid Spreading Yourself Too Thin:<\/i>&nbsp;Doing lots of things superficially means none of them will be interesting.<\/li><li>If you have spent more than five hours without learning something new, you should probably try a different approach<ul><li>And if you have spent more than two days without learning something new, you should seriously consider pivoting and doing something else.<\/li><\/ul><\/li><li>To practice prioritization, be intentional about your decisions: write down <i>why<\/i>&nbsp;you think an experiment is the right call, and later reflect on whether you were right. This makes your intuitions explicit and easier to update.<\/li><\/ul><\/li><li><strong>Deeper Understanding<\/strong>&nbsp;is about practicing skepticism and building a bulletproof case. Red-team your results relentlessly.<ul><li>Some experiments are much more impactful and informative than others! Don't just do the first experiment that pops into your head. Think about the key ways the hypothesis <i>could <\/i>be false, and how you could test that. Or about whether a skeptic could explain away a positive experimental results<ul><li>A useful exercise is imagining you're talking to a really obnoxious skeptic who keeps complaining that they don't believe you and coming up with arguments for why your thing is wrong. What could you do such that they don't have a leg to stand on?<\/li><\/ul><\/li><li>Of course, there's also an element of prioritization. Sometimes a shallow case that could be wrong is the right thing to aim for, if you’re working on an unimportant side claim/something that seems super plausible on priors, at which point you should just move on and do something else more interesting.<\/li><li>Exercise: To practice spotting subtle illusions, try red-teaming papers you read, thinking about potential flaws, and ideally run the experiments yourself.<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Doing_Good_Science\">Doing Good Science<\/p><ul><li><strong>Avoid cherry-picking<\/strong>: Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling.<ul><li>When you write up work, always include some randomly selected examples, especially if you present extensive qualitative analysis of specific things. It's fine to put this in the appendix if space is scarce, but it should be there.<\/li><\/ul><\/li><li><strong>Use baselines<\/strong>: A common mistake is for people to try to show a technique works by demonstrating it gets 'decent' results, rather than showing it achieves better results than plausible alternatives that people might have used or are standard in the field. If you want people to e.g. use your cool steering vector results you need to show it beats changing the system prompt.<\/li><li><strong>Don’t sandbag your baselines<\/strong>: Similarly, it's easy to put in much more effort finding good hyperparameters for your technique than for your baselines. Try to make sure you're achieving comparable results with your baselines that prior work in the field has.<\/li><li><strong>Do ablations on your fancy method<\/strong>: It's easy for people to have a fancy method with lots of moving parts, when many actually are unnecessary. You should always try removing one part and see if the method breaks. Do this for each part.<ul><li>For example, the <a href=\"https://arxiv.org/abs/2403.03218v1\">original unlearning method<\/a>&nbsp;in the <a href=\"https://arxiv.org/abs/2403.03218\">RMU paper<\/a>&nbsp;claimed it was based on finding a meaningful steering vector, until follow-up work found that it was just about adding a vector with really high norm that broke the model, and a random vector performed just as well.<\/li><\/ul><\/li><li><strong>(Informally) pre-register claims<\/strong>: It's important to clearly track which experimental results were obtained before versus after you formulated your claim. Post-hoc analysis (interpreting results after they're seen) is inherently less impressive than predictions confirmed by pre-specified experiments<\/li><li><strong>Be reproducible<\/strong>: Where practical, share your code, data and models.<ul><li>If you have time, make sure that it runs on a fresh machine and include a helpful readme that links to key model weights and datasets.<\/li><li><p data-internal-id=\"ftnt_ref27\">This both means others can check if your work is true and makes it more likely people will believe and build on your work<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-noteref\" id=\"fnrefadytzr5d7y\"><sup><a href=\"#fnadytzr5d7y\">[27]<\/a><\/sup><\/span>&nbsp;because they can see replications that are more likely to exist and because it's now low friction.<\/p><\/li><\/ul><\/li><li><strong>Simplicity:<\/strong>&nbsp;Bias towards trying the simple, obvious methods first. Fancy techniques can be a trap. Good research is pragmatic, not about showing off.<ul><li>If you’re designing a fancy technique/experiment, each new detail is one more thing that can break<\/li><li>If trying to explain something mysterious, novice researchers often neglect simple, dumb hypotheses like “maybe MLP0 is incredibly important on <i>every <\/i>input, and there’s nothing special going on with my prompt”<\/li><\/ul><\/li><li><strong>Be qualitative <\/strong><i><strong>and <\/strong><\/i><strong>quantitative<\/strong>: One of the major drivers of progress of modern machine learning is being quantitative, having benchmarks and showing that a technique increases numbers on them. One of the key drivers of progress in mech interp is an openness to qualitative research: summary statistics lose a ton of information. What can we learn by actually looking deeply into what's happening?<ul><li>In my opinion, the best research tries to get the best of both worlds. It tries to understand what's happening via qualitative analysis and then validates it with more quantitative methods. If your paper only does one, it’s probably missing out<\/li><\/ul><\/li><li><strong>Read your data<\/strong>: A fantastic use of time, especially during the exploration phase, is just actually reading the data you're working with, or model chains of thought and responses.<ul><li>Often, the quality of the data is a crucial driver of the results of your experiments. Often, it is quite bad.<\/li><li>Sometimes most of the work of a project is in noticing flaws in your data and making a better data set. Time figuring this out is extremely well spent.<\/li><li>Ditto, include random examples of the data in an appendix for readers to do spot checks of their own.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref28\"><strong>Don’t reinvent the wheel<\/strong>: &nbsp;A common mistake in mech interp is doing something that's already been done<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-noteref\" id=\"fnrefva7mhfkrhm\"><sup><a href=\"#fnva7mhfkrhm\">[28]<\/a><\/sup><\/span>. We have LLM-powered literature reviews now. You have way less of an excuse. Check first!<\/p><\/li><li><strong>Excitement is evidence of bullshit<\/strong>: Generally, most true results are not exciting, but a fair amount of false results are. So from a Bayesian perspective, if a result is exciting and cool, it’s even more likely to be false than normal!<ul><li>Resist the impulse to get really excited! The correct attitude to exciting results is deep skepticism until you have tried really hard to falsify it and run out of ideas.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Practicing_Ideation\">Practicing Ideation<\/h3><p>Okay, so you want to actually come up with good research ideas to work on. What does this look like? I recommend breaking this down into <strong>generating ideas<\/strong>&nbsp;and then <strong>evaluating <\/strong>them to find the best ones.<\/p><p>To generate ideas, I'd often start with just taking a blank doc, blocking out at least an hour, and then just writing down as many ideas as you can come up with. Aim for quantity over quality. Go for at least 20.<\/p><p>There are other things you can do to help with generation:<\/p><ul><li>Throughout your previous sprints, every time you had an idle curiosity or noticed something weird, write it down in one massive long-running doc.<\/li><li>Likewise, when reading papers, note down confusions, curiosities, obviousnesses to do.<\/li><\/ul><p>Okay, so now you have a big list. What does finding the best ones look like?<\/p><ul><li>Ideally, if you have a mentor or at least collaborators, you can just ask them to rate them.<ul><li>If you do this, rate them yourself privately out of 10 before you look at their responses. Compare them and every time you have substantially different numbers, talk to the mentor and try to figure out why your intuitions disagree. This is a great source of supervised data for research taste.<\/li><\/ul><\/li><li>Even if you don’t have a mentor, I think that just going through, rating each idea yourself based on gut feel and sorting is as good a way to prune down a long list as any<\/li><li>For the top few, I recommend trying to answer a few questions about them.<ul><li>What would success look like here?<\/li><li>How surprised would I be if I did this for a month and nothing interesting had happened?<\/li><li>What skills does this require? Do I have them/could I easily gain them?<\/li><li>What models, data, computational resources, etc. does this require?<\/li><li>How does this compare to what the most relevant prior work did? Can I check for prior work and see if anything relevant comes up?<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Research_Taste_Exercises\">Research Taste Exercises<\/p><p>Gaining research taste is slow because the feedback loops are long. You can accelerate it with exercises that give you faster, proxy feedback. (Credit to <a href=\"https://colah.github.io/notes/taste/\">Chris Olah for inspiration here<\/a>)<\/p><ul><li>If you have a mentor, query their taste for fast data and try to imitate it. Concretely:<ul><li>Before each meeting, write a list of questions, then try to write up predictions for what the mentor will say, then actually ask the mentor, see what happens, and compare. If there are discrepancies, chat to the mentor and try to understand why.<\/li><li>Likewise, if the mentor makes a suggestion or asks a question you didn't expect, try to ask questions about where the thought came from.<\/li><li><p data-internal-id=\"ftnt_ref29\">Regularly paraphrase back to the mentor in your own words what you think they're saying, and then ask them to correct anything you're wrong about<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-noteref\" id=\"fnreftt0owz8koks\"><sup><a href=\"#fntt0owz8koks\">[29]<\/a><\/sup><\/span><\/p><\/li><\/ul><\/li><li><strong>Learning from papers as \"offline data\":<\/strong>&nbsp;When you read a paper, don't just passively consume it. Read the introduction, then stop. Try to predict what methods they used and what their key results will be. Then, continue reading and see how your predictions compare. Analyze why the authors made different choices. This trains your intuition on a much larger and faster dataset than your own research.<\/li><\/ul><p>It’s also worth dwelling on what research taste actually is. <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">See my post<\/a>&nbsp;for more, but I break it down as follows:<\/p><ol><li><strong>Intuition (System 1):<\/strong>&nbsp;This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.<\/li><li><strong>Conceptual Framework (System 2)<\/strong>: This is deep domain knowledge and understanding of underlying principles.<\/li><li><strong>Strategic Big Picture<\/strong>: Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?<\/li><\/ol><h3 data-internal-id=\"Write_up_your_work_\">Write up your work!<\/h3><p>At this stage, you should be thinking seriously about how to write up your work. Often, writing up work is the first time you really understand what a project has been about, or you identify key limitations, or experiments you forgot to do. You should check out <a href=\"https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers\">my blog post on writing ML papers<\/a>&nbsp;for much more detailed thoughts (which also apply to high-effort blog posts!) but I'll try to summarize them below.<\/p><p data-internal-id=\"Why_aim_for_public_output_\">Why aim for public output?<\/p><p data-internal-id=\"ftnt_ref30\">If producing something public is intimidating, for now, you can start by just writing up a private Google Doc and maybe share it with some friends or collaborators. But I heavily encourage people to aim for public output where they can. Generally, your research will not matter if no one reads it. The goal of research is to contribute to the sum of human<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-noteref\" id=\"fnrefe3252d8idmr\"><sup><a href=\"#fne3252d8idmr\">[30]<\/a><\/sup><\/span>&nbsp;knowledge. And if no one understands what you did, then it doesn't matter.<\/p><p>Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, <strong>public research output is your best credential<\/strong>. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc. It doesn’t matter if you wrote it in a prestigious PhD program or as a random independent researcher, if it’s good enough then people care.<\/p><p>There are a few options for what this can look like:<\/p><ul><li>A blog post (e.g. on a personal blog or LessWrong) - the simplest and least formal kind<\/li><li><p data-internal-id=\"ftnt_ref31\">An Arxiv paper - much more legible than a blog post, and honestly not much extra effort if you have a high-quality blog post<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-noteref\" id=\"fnref8354hd0flji\"><sup><a href=\"#fn8354hd0flji\">[31]<\/a><\/sup><\/span><\/p><\/li><li><p data-internal-id=\"ftnt_ref32\">A workshop paper<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-noteref\" id=\"fnref9oppcf0ftrh\"><sup><a href=\"#fn9oppcf0ftrh\">[32]<\/a><\/sup><\/span>&nbsp;(i.e. something you submit for peer review to a workshop, typically part of a major ML conference, the bar is much lower than for a conference paper)<\/p><\/li><li><p data-internal-id=\"ftnt_ref34\">A conference paper (the equivalent of top journals in ML, there’s a reasonably high quality bar<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-noteref\" id=\"fnreffmh579omuc6\"><sup><a href=\"#fnfmh579omuc6\">[33]<\/a><\/sup><\/span>, but also a <i>lot <\/i>of noise<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-noteref\" id=\"fnreff09vsa4w37e\"><sup><a href=\"#fnf09vsa4w37e\">[34]<\/a><\/sup><\/span>)<\/p><\/li><\/ul><p>If this all seems overwhelming, starting out with blog posts is fine, but I think people generally overestimate the bar for arxiv or workshop papers - if you think you learned something cool in a project, this is totally worth turning into a paper!<\/p><p data-internal-id=\"How_to_write_stuff_up_\">How to write stuff up?<\/p><p>The core of a paper is the narrative. Readers will not take away more than a few sentences worth of content. Your job is to make sure these are the right handful of sentences and make sure the reader is convinced of them.<\/p><p>You want to distill your paper down into one to three key claims (your contribution), the evidence you provide that the contribution is true, the motivation for why a reader should care about them, and work all of this into a coherent narrative.<\/p><p><strong>Iterate<\/strong>: I'm a big fan of writing things iteratively. You first figure out the contribution and narrative. You then write a condensed summary, the abstract (in a blog post, this should be a TL;DR/executive summary - also very important!). You then write a bullet point outline of the paper: what points you want to cover, what evidence you want to provide, how you intend to build up to that evidence, how you want to structure and order things, etc. If you have mentors or collaborators, the bullet point outline is often the best time to get feedback. Or the narrative formation stage, if you have an engaged mentor. Then write the introduction, and make sure you’re happy with that. Then (or even before the intro) make the figures - figures are incredibly important! Then flesh it out into prose. People spend a <i>lot<\/i>&nbsp;more time reading the abstract and the intro than the main body, especially when you account for all the people who read the abstract and then stop. So you should spend a lot more time per unit word on those.<\/p><p><strong>LLMs<\/strong>: I think LLMs are a really helpful writing tool. They're super useful for getting feedback, especially if writing in an unfamiliar style like an academic ML paper may be for you. Remember to use anti-sycophanty prompts so you get real feedback. However, it's often quite easy to tell when you're reading LLM written slop. So use them as a tool, but don't just have them write the damn thing for you. But if you e.g. have writer’s block, having an LLM help you brainstorm or produce a first draft for inspiration, and can be very helpful.<\/p><p data-internal-id=\"Common_mistakes\">Common mistakes<\/p><ul><li><strong>The reader does not have context<\/strong>: Your paper will be clear in your head, because you have just spent weeks to months steeped in this research project. The reader has not. You will overestimate how clear things are to the reader, and so you should be massively erring in the other direction and spelling everything out as blatantly as possible.<ul><li><strong>This is an incredibly common mistake<\/strong>&nbsp;- assume it will happen to you<\/li><li>The main solution is to get feedback from people with enough research context that they can actually engage and who are also willing to give you substantial negative feedback.<ul><li>Notice the feeling of surprise when people are confused by something you thought was clear. Try to understand why they were confused and iterate on fixing it until it's clear.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Writing is not an afterthought<\/strong>: People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute.<\/li><li><strong>Acknowledge limitations<\/strong>: There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing<\/li><li><strong>Good writing is simple<\/strong>: There's a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they feel impressive. I think this is a highly ineffective strategy<\/li><li><strong>Remember to motivate things<\/strong>: It will typically not be obvious to the reader why your paper matters or is interesting. They do not have the context you do. It is your job to convince them, ideally in the abstract or perhaps intro, why they should care about your work, lest they just give up and stop reading.<\/li><\/ul><p data-internal-id=\"h.sk0e3iwce7ck\">&nbsp;<\/p><h2 data-internal-id=\"Mentorship__Collaboration_and_Sharing_Your_Work\">Mentorship, Collaboration and Sharing Your Work<\/h2><p>A common theme in the above is that it's incredibly useful to have a mentor, or at least collaborators. Here I'll try to unpack that and give advice about how to go about finding one.<\/p><p>Though it's also worth saying that many mentors are not actually great researchers and may have bad research taste or research taste that's not very well suited to mech interp. What you do about this is kind of up to you.<\/p><h3 data-internal-id=\"So_what_does_a_research_mentor_actually_do_\">So what does a research mentor actually do?<\/h3><p>A good mentor is an incredible accelerator. Dysfunctional as academia is, there is a reason it works under the apprenticeship-like system of PhD students and supervisors. When I started supervising, I was very surprised at how much of a difference a weekly check in could make! Here’s my best attempt to breakdown how a good mentor can add value:<\/p><ul><li><strong>Suggest research ideas<\/strong>&nbsp;when you're starting out, letting you bypass the hardest skill (ideation) to focus on execution.<\/li><li><strong>Help you prioritize<\/strong>&nbsp;which experiments to run, lending you their more experienced judgment, so you get more done.<\/li><li><p data-internal-id=\"ftnt_ref35\"><strong>When to pivot<\/strong>: if your research direction isn’t working out, having a mentor to pressure you to pivot can be extremely valuable<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-noteref\" id=\"fnref7ruxx269r2s\"><sup><a href=\"#fn7ruxx269r2s\">[35]<\/a><\/sup><\/span><\/p><\/li><li><strong>Provide supervised data for research taste<\/strong>: For the slow/very-slow skills like coming up with research ideas, and prioritization, a <i>far <\/i>faster way to gain them at first is by learning to mimic your mentor’s.<\/li><li><strong>Act as an interface to the literature<\/strong>: pointing you to the relevant work before you've built up deep knowledge yourself. Flagging standard baselines, standard metrics, relevant techniques, prior work so you don’t reinvent the wheel, etc.<\/li><li><strong>Red-team your results<\/strong>, helping you spot subtle interpretability illusions and flaws in your reasoning that you're too close to see.<\/li><li><strong>Point out skills you're missing<\/strong>&nbsp;that you didn't even notice were skills. Generally guide your learning and help you prioritize<\/li><li><strong>Walk you through communicating your work<\/strong>, helping you distill your findings and present them clearly to the world.<\/li><li><strong>Motivation/accountability<\/strong>: Many find it extremely helpful to have someone, even if very hands-off, who they present work to, so they feel motivated and accountable (especially if they e.g. want to impress the mentor, want a job, etc. Of course, these also increase stress!)<ul><li>To those prone to analysis paralysis, being able to defer to a mentor on uncertain decisions can be highly valuable<\/li><\/ul><\/li><li><strong>References<\/strong>: Having a mentor who can vouch for your skill is very helpful, especially if they know people who may be hiring you in future.<\/li><\/ul><h3 data-internal-id=\"Advice_on_finding_a_mentor\">Advice on finding a mentor<\/h3><p>Here are some suggested ways to get some mentorship while transitioning into the field. I discuss higher commitment ways, like doing a PhD or getting a research job, below.<\/p><p>Note: whatever you do to find a mentor, having evidence that you can do research yourself, that is, public output that demonstrates ability to self-motivate and put in effort, and ideally demonstrates actually interesting research findings, is incredibly helpful and should be a priority.<\/p><p data-internal-id=\"Mentoring_programs\">Mentoring programs<\/p><p>I think mentoring programs like <a href=\"http://matsprogram.org\">MATS<\/a>&nbsp;are an incredibly useful way into the field, you typically do a full-time, several month program where you write a paper, with weekly check-ins with a more experienced researcher. Your experience will vary wildly depending on mentor quality, but at least for my MATS scholars, often people totally new to mech interp can publish a top conference paper in a few months. See my MATS application doc for a bunch more details.<\/p><p>There’s <strong>a wide range of backgrounds<\/strong>&nbsp;among people who do them and get value - people totally new to a field, people with 1+ years of interpretability research experience who want to work with a more experienced mentor, young undergrads, mid-career professionals (including a handful of professors), and more.<\/p><p><a href=\"http://matsprogram.org\">MATS 9.0<\/a>&nbsp;applications are open, due <strong>Oct 2 2025<\/strong>, and <a href=\"http://tinyurl.com/neel-mats-app\">mine<\/a>&nbsp;close on <strong>Sept 12<\/strong>.<\/p><p>Other programs (which I think are generally lower quality than MATS, but often still worth applying to depending on the mentor)<\/p><ul><li><i>Full-time/In-person:<\/i><a href=\"https://www.matsprogram.org/\">&nbsp;MATS<\/a>, <a href=\"https://www.pivotal-research.org/fellowship\">Pivotal<\/a>, <a href=\"https://www.lasrlabs.org/\">LASR<\/a>, <a href=\"https://pibbss.ai/fellowship/\">PIBBSS<\/a><\/li><li><i>Part-time/Remote:<\/i><a href=\"https://www.cambridgeaisafety.org/mars\">&nbsp;<\/a><a href=\"https://sparai.org/\">SPAR<\/a>, <a href=\"https://www.cambridgeaisafety.org/mars\">MARS<\/a><\/li><\/ul><p data-internal-id=\"Cold_emails\">Cold emails<\/p><p>You can also take matters into your own hands and try to convince someone to be your mentor. Reaching out to people, ideally via a warm introduction, but even just via a cold email, can be highly effective. However, I get lots of cold emails and I think many are not very effective, so here's some advice:<\/p><ul><li><strong>Don't just email the most prominent people<\/strong>. A lot of people will just email the most prominent people in the field and ask for mentorship. This is a bad plan! These people are very busy and they also get lots of emails. I just reflexively respond to any email requesting mentorship with “please apply to my MATS cohort”.<ul><li>However, there are lots of less prominent people who can provide a bunch of useful mentorship. These people are much more likely to be excited to get a cold email, to have time to engage, potentially even the spare capacity to properly mentor a project.<\/li><li>I think that many people who've recently joined my team or people who worked on a great paper with me during MATS are able to add a lot of value to people new to the field. And I would recommend reaching out to them!<ul><li>For example, Josh Engels, a new starter on my team, said he would happily receive more cold emails (as of early Sept 2025).<\/li><li>As a general heuristic, email first authors of papers, not fancy last authors.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Start small<\/strong>: Don't email someone you've never interacted with before asking if they want to kind of officially mentor you on some project. That's a big commitment.<ul><li>It's much better to be like, I'd be interested in having a chat about your paper or my work building on your paper.<\/li><li>Or just asking if they're down to have a chat giving you feedback on some project ideas, etc.<\/li><li>And if this goes well, it may organically turn into a more long-term mentoring relationship!<\/li><\/ul><\/li><li><strong>Proof of work<\/strong>: Demonstrate that you are actually interested in this person specifically, not just spamming tons of people.<ul><li>Show that you've engaged with their work, say something intelligent about it, have some questions.<ul><li>In the era of LLMs, this is less of a costly signal that you've actually taken an interest in this person specifically than it used to be, admittedly<\/li><li>But linking to some research you did building on their work I think is still reasonably costly, and very flattering to people.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Prioritize aggressively<\/strong>. Assume the reader will stop reading at any moment, so put your most critical and impressive information first.<\/li><li><strong>Explain who you are<\/strong>: If you're emailing someone who gets more emails than they have capacity to respond to, they're going to be prioritizing. A key input into this is just who you are, what have you done, have you done something interesting that shows promise, do you have relevant credentials, etc. I personally find it very helpful if people just say the most impressive things about them in the first sentence or two.<ul><li>To do this without seeming arrogant, you could try: \"I'm sure you must get many of these emails. So to help you prioritise, here's some key info about me\"<\/li><\/ul><\/li><li>Use <strong>bolding<\/strong>&nbsp;for key phrases to make your email easily skimmable.<\/li><li><strong>Be concise<\/strong>. One thing I would often appreciate is a short blurb summarizing your request with a link to a longer document for details if I'm interested.<\/li><li><strong>Quick requests<\/strong>: Generally, my flow when reading emails is that I will either immediately respond or never look at it again. I'm a lot more likely to immediately respond if I can do so quickly. If you do want to email a busy person, have a clear, concrete question up front that they might be able to help with.<\/li><\/ul><h3 data-internal-id=\"Community___collaborators\">Community &amp; collaborators<\/h3><p>Much easier than finding a mentor is finding collaborators, other people to work on the same project with, or just other people also trying to learn more about mech interp, who you can chat with and give each other feedback:<\/p><ul><li><strong>In-Person:<\/strong>&nbsp;Local AI Safety hubs (London, Bay Area, etc.), University groups, ML conferences (e.g., the<a href=\"http://mechinterpworkshop.com/\">&nbsp;NeurIPS Mech Interp workshop<\/a>&nbsp;I co-organize), EAG/EAGx conferences.<ul><li>If you’re a student, see if there’s a lab at your university that has some people interested in interpretability. There may be interested PhD students even if no professor works on it<\/li><\/ul><\/li><li><strong>Online<\/strong>: These are also good places to meet people! I recommend sharing work for feedback, or just asking about who’s interested in what you’re interested in, and trying to DM the people who engage/seem interested, and seeing what happens<ul><li><a href=\"https://www.neelnanda.io/osmi-slack-invite\">Open Source Mechanistic Interpretability Slack<\/a><\/li><li><a href=\"https://discord.gg/nHS4YxmfeM\">Eleuther Discord<\/a>&nbsp;(interpretability-general)<\/li><li><a href=\"https://discord.gg/ysVfhCfCKw\">Mech Interp Discord<\/a><\/li><\/ul><\/li><\/ul><p><strong>Staying up to date<\/strong>: Another common question is how to stay up to date with the field. Honestly, I think that people new to the field should not worry that much about this. Most new papers are irrelevant, including the ones that there is hype around. But it's good to stay a little bit in the loop. Note that the community has substantial parts both in academia and outside, which are often best kept up with in different ways.<\/p><ul><li>LessWrong and the AlignmentForum are a reasonable place to keep up to date with the less academic half<\/li><li>Twitter is a confusing, chaotic place that is an okay place to keep up with both. It's a bit unclear who the right people to follow.<ul><li><a href=\"http://x.com/ch402\">Chris Olah<\/a>&nbsp;doesn't tweet much, but it's high quality when he does.<\/li><li><a href=\"http://x.com/neelnanda5\">I will tweet<\/a>&nbsp;about all of my interpretability work and sometimes others.<\/li><\/ul><\/li><\/ul><h2 data-internal-id=\"Careers\">Careers<\/h2><h3 data-internal-id=\"Where_to_apply\">Where to apply<\/h3><ul><li>Anthropic’s interpretability team roles: <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020159008\">research scientist<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020305008\">research engineer<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4009173008\">research manager<\/a><\/li><li><a href=\"https://openai.com/careers/research-engineer-scientist-interpretability\">OpenAI's interpretability team roles<\/a><\/li><li>My team at Google DeepMind will hopefully be <a href=\"https://deepmind.google/about/careers/#open-roles\">hiring in early 2026<\/a>! Watch this space<\/li><li><a href=\"https://transluce.org/\">Transluce<\/a>&nbsp;-- a nonprofit research lab<\/li><li><a href=\"https://www.goodfire.ai/\">Goodfire<\/a>&nbsp;-- a mech interp startup that are <a href=\"https://www.goodfire.ai/careers\">hiring a bunch<\/a>.<ul><li>They <a href=\"https://www.goodfire.ai/blog/announcing-our-50m-series-a\">recently raised a $50 million Series A<\/a>&nbsp;and as of the time of writing are trying to both have people focused on products, and people focused on more fundamental research<\/li><\/ul><\/li><li>The UK government's AI Security Institute's interpretability team (<a href=\"https://www.aisi.gov.uk/careers#open-roles\">not currently hiring<\/a>)<\/li><\/ul><p data-internal-id=\"Applying_for_grants\">Applying for grants<\/p><p>For people trying to get into mech interp via the safety community, there are some funders around open to giving career transition grants to people trying to upskill in a new field like mech interp. Probably the best place I know of is <a href=\"https://www.openphilanthropy.org/career-development-and-transition-funding/\">Open Philanthropy's Early Career Funding.<\/a><\/p><p data-internal-id=\"Explore_Other_AI_Safety_Areas\">Explore Other AI Safety Areas<\/p><p>Mech interp isn't the only game in town! There’s other important areas of safety like Evals, AI Control, and Scalable Oversight, the latter two in particular seem neglected compared to mech interp. The<a href=\"https://arxiv.org/pdf/2504.01849\">&nbsp;GDM AGI Safety Approach<\/a>&nbsp;gives an overview of different parts of the field. If you’re doing this for safety reasons, I’d check if there’s other, more neglected subfields, that also appeal to you!<\/p><h3 data-internal-id=\"What_do_hiring_managers_look_for\">What do hiring managers look for<\/h3><p>Leaving aside things that apply to basically all roles, like whether this person has a good personality fit (which often just means looking out for red flags), here’s my sense of what hiring managers in interpretability are often looking for.<\/p><p>A useful mental model is that from a hiring manager's perspective, they're making an uncertain bet with little information in a somewhat adversarial environment. Each applicant wants to present themselves as the perfect fit. This means managers need to rely on signals that are hard to fake. But it’s quite difficult to get that much info on a person before you actually go and work with them a bunch.<\/p><p>Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly. If your research track record is good enough, interviews may just act as a check for red flags and to verify that you can actually write code and run experiments well.<\/p><p>Key skills:<\/p><ul><li><strong>Research Skills:<\/strong>&nbsp;A track record of completing end-to-end projects is the best signal. Papers are a great way to show this.<ul><li><strong>Research taste<\/strong>: The ability to come up with great research ideas <i>and<\/i>&nbsp;drive them to completion is rare and very valuable.<\/li><li><strong>Experiment design<\/strong>: Can they design good experiments and make their research ideas concrete and convert them into actions?<\/li><\/ul><\/li><li><strong>Conceptual Understanding of Mech Interp:<\/strong>&nbsp;Do you get the key ideas and know the literature?<\/li><li><p data-internal-id=\"ftnt_ref36\"><strong>Productivity and Conscientiousness:<\/strong>&nbsp;This is a very hard one to interview for, but incredibly important. A public track record of doing interesting things is a good signal, as are strong references from trusted sources<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-noteref\" id=\"fnrefslnwemz4grq\"><sup><a href=\"#fnslnwemz4grq\">[36]<\/a><\/sup><\/span>.<\/p><\/li><li><strong>Engineering Skills:<\/strong>&nbsp;Can you work fluently in a Python notebook? Can you write experiment code fast and well? Can you get things done? Do you understand the standard gotchas?<\/li><li><strong>Deep engineering skill<\/strong>: Beyond hacking together experiments, can you navigate large, complex codebases, write maintainable code, design complex software projects, etc?<ul><li>This is much more important if doing research inside a larger lab or tech company than as an independent researcher or academic.<\/li><li>One of the most common reasons we don't hire seemingly promising researchers onto my team is because they lack sufficiently strong engineering skills.<\/li><li>Obviously, LLMs are substantially changing the game when it comes to engineering skills, but I think deep engineering skills will be much harder to automate than shallow ones, unfortunately.<\/li><li>Unfortunately, I don’t have great advice on how to gain these other than working in larger and more complex codebases and learning how to cope. Pair programming with more experienced programmers can be a great way to transfer tacit knowledge<\/li><\/ul><\/li><li><strong>Skepticism<\/strong>: Can you constructively engage with research and critically evaluate it? In particular, can you do this to your own research? Good researchers need to be able to do work that is true.<\/li><\/ul><h3 data-internal-id=\"Should_you_do_a_PhD_\">Should you do a PhD?<\/h3><p>I don't have a PhD (and think I would have had a far less successful career if I had tried to get one) so I'm somewhat biased. But it's a common question. Here are the strongest arguments I’ve heard in favour:<\/p><ul><li>You get extremely high <strong>autonomy<\/strong>. If you want to spend years going deep on a niche topic that no industry lab would fund, a PhD is one of the only ways to do it.<\/li><li>It's a great environment to cultivate the ability to <strong>set your own research agenda<\/strong>. This is a crucial and difficult skill that is harder to learn in industry, where agendas are often set from the top down (though this varies a lot between team).<\/li><\/ul><p>And here are the reasons I think it's often a bad idea:<\/p><ul><li>The opportunity cost is immense. You could spend 4-6 years gaining direct, relevant experience in an industry lab.<\/li><li>Academic incentives can be misaligned with doing impactful research, e.g. pressure to publish meaning you’re discouraged from admitting to the limitations of your work.<\/li><li>The quality of supervision varies wildly, and a bad supervisor can make your life miserable.<\/li><li>Quality of life: The pay is generally terrible, which may or may not matter to you, and you may only get places in a different city/country than you’d prefer.<\/li><\/ul><p>But with all those caveats in mind, it’s definitely the right option for some! My overall take:<\/p><ul><li>The key thing that matters is mentorship, being in an environment where you are working with a better researcher, and learning from them.<ul><li>PhDs are often a good way of getting this. But if you can gain this by another way, plausibly you should go to that instead. PhDs have a lot of downsides too.<\/li><\/ul><\/li><li>Generally, the variance between supervisors and between managers in industry will dominate the academia versus industry differences, and thus you should pay a lot of attention to who exactly would be managing you.<ul><li>For a PhD, try to speak to your potential supervisor’s students in a private setting. If they say pretty bad things, that's a good reason not to go for the supervisor.<\/li><li>A common mistake is optimising for the most prestigious and famous supervisor when you often want to go for the ones who will have the most time for you, which anti-correlates.<\/li><\/ul><\/li><li>A common mistake is people feeling they need to <i>finish<\/i>&nbsp;PhDs. But if you sincerely believe that the point of a PhD is to be a learning environment, then why would the formal end of the PhD be the optimal time to leave? It's all kind of arbitrary.<ul><li>IMO, at least every six months, you should seriously evaluate what other opportunities you have, try applying for some things and be emotionally willing leave if a better opportunity comes along (taking into account switching costs).<ul><li>Note that often you can just take a year's leave of absence and resume at will.<\/li><\/ul><\/li><\/ul><\/li><\/ul><p data-internal-id=\"Relevant_Academic_Labs\">Relevant Academic Labs<\/p><p>I’m a big fan of the work coming out of these two, they seem like great places to work:<\/p><ul><li>David Bau (Northeastern)<\/li><li>Martin Wattenberg &amp; Fernanda Viegas (Harvard)<\/li><\/ul><p>Other labs that seem like good places to do interpretability research (note that this is not trying to be a comprehensive list!):<\/p><ul><li>Yonatan Belinkov (Technion)<\/li><li>Jacob Andreas (MIT)<\/li><li>Jacob Steinhardt (Berkeley)<\/li><li>Ellie Pavlick (Brown)<\/li><li>Victor Veitch (UChicago)<\/li><li>Robert West (EPFL)<\/li><li>Roger Grosse (Toronto)<\/li><li>Mor Geva (Tel Aviv)<\/li><li>Sarah Wiegreffe (Maryland)<\/li><li>Aaron Mueller (Boston University)<\/li><\/ul><p><i>Thanks a lot to Arthur Conmy, Paul Bogdan, Bilal Chughtai, Julian Minder, Callum McDougall, Josh Engels, Clement Dumas, Bart Bussmann for valuable feedback<\/i><\/p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-endnote\" id=\"fnnifk1wb1jum\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"nifk1wb1jum\"><sup><strong><a href=\"#fnrefnifk1wb1jum\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Note that I mean a full working month here. So something like 200 working hours. If you're only able to do this part-time, it's fine to take longer. If you're really focused on it, or have a head-start, then move on faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-endnote\" id=\"fnue9pdw6v8rj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"ue9pdw6v8rj\"><sup><strong><a href=\"#fnrefue9pdw6v8rj\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;If you want something even more approachable, one of my past MATS scholars recommends getting GPT-5 thinking to produce coding exercises (eg a Python script with empty functions, and good tests), for an easier way in.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-endnote\" id=\"fnhh6mwdeo4zm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"hh6mwdeo4zm\"><sup><strong><a href=\"#fnrefhh6mwdeo4zm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;It’s fine for this coding to need a bunch of LLM help and documentation/tutorial looking up, this isn’t a memory test. The key thing is being able to correctly explain the core of each technique to a friend/LLM.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-endnote\" id=\"fnsxyjce3nii\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"sxyjce3nii\"><sup><strong><a href=\"#fnrefsxyjce3nii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Note: This curriculum aims to get you started on <i>independent research<\/i>. This is often good enough for academic labs, but the engineering bar for most industry labs is significantly higher, as you’ll need to work in a large complex codebase with hundreds of other researchers. But those skills take much longer to gain.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-endnote\" id=\"fnkte6u8splw\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"kte6u8splw\"><sup><strong><a href=\"#fnrefkte6u8splw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;You want to exclude the first token of the prompt when collecting activations, it’s a weird attention sink and often has high norm/is anomalous in many ways<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-endnote\" id=\"fn2ob115pcmet\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"2ob115pcmet\"><sup><strong><a href=\"#fnref2ob115pcmet\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Gotcha: Remember to try a bunch of coefficients for the vector when adding it. This is a crucial hyper-parameter and steered model behaviour varies a lot depending on its value<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-endnote\" id=\"fn1b9r0ass7sd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1b9r0ass7sd\"><sup><strong><a href=\"#fnref1b9r0ass7sd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Mixture of expert models, where there are many parameters, and only a fraction light up for each token, are a pain for interpretability research. Larger models means you'll need to get more/larger GPUs which is expensive and unwieldy. Favor working with dense models where possible.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-endnote\" id=\"fnbzop9pji3nl\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bzop9pji3nl\"><sup><strong><a href=\"#fnrefbzop9pji3nl\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;You can download then upload the PDF to the model, or just select all and copy and paste from the PDF to the chat window. No need to correct the formatting issues, LLMs are great at ignoring weird formatting artifacts<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-endnote\" id=\"fn207k0k5nobb\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"207k0k5nobb\"><sup><strong><a href=\"#fnref207k0k5nobb\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;<a href=\"http://repo2txt.com\">repo2txt.com<\/a>&nbsp;is a useful tool for concatenating a Github repo into a single txt file<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-endnote\" id=\"fn979wnkvgpa4\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"979wnkvgpa4\"><sup><strong><a href=\"#fnref979wnkvgpa4\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;If you would like other perspectives, check out <a href=\"https://arxiv.org/abs/2501.16496\">Open Problems in Mechanistic Interpretability<\/a>&nbsp;(broad lit review from many leading researchers, recent), or <a href=\"https://transformer-circuits.pub/2023/interpretability-dreams/index.html\">Interpretability Dreams<\/a>&nbsp;(from Anthropic, 2 years old)<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-endnote\" id=\"fn3zw26zes9dx\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3zw26zes9dx\"><sup><strong><a href=\"#fnref3zw26zes9dx\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;And for reasons we’ll discuss later, now feel much more pessimistic about the ambitious reverse engineering direction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-endnote\" id=\"fn7cxhc64szn8\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7cxhc64szn8\"><sup><strong><a href=\"#fnref7cxhc64szn8\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Even if you already have a research background in another field, mechanistic interpretability is sufficiently different that you should expect to need to relearn at least some of your instincts. This stage remains very relevant to you, though you can hopefully learn faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-endnote\" id=\"fn9wj0u0qz3q\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9wj0u0qz3q\"><sup><strong><a href=\"#fnref9wj0u0qz3q\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;The rest of this piece will be framed around approaching learning research like this and why I think it is a reasonable process. Obviously, there is not one true correct way to learn research! When I e.g. critique something as a “mistake”, interpret this as “I often see people do this and think it’s suboptimal for them”, not “there does not exist a way of learning research where this is a good idea<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-endnote\" id=\"fnxw1ra5pqnd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"xw1ra5pqnd\"><sup><strong><a href=\"#fnrefxw1ra5pqnd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;My term for associated knowledge, understanding, intuition, etc.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-endnote\" id=\"fntq4gws0zq69\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tq4gws0zq69\"><sup><strong><a href=\"#fnreftq4gws0zq69\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Read <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">my thoughts on SAEs here<\/a>. There’s still useful work to be done, but it’s an oversubscribed area, and our bar should be higher. They are a useful tool, but not as promising as I once hoped.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-endnote\" id=\"fncdmsagzbqkp\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"cdmsagzbqkp\"><sup><strong><a href=\"#fnrefcdmsagzbqkp\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This was using a technique called synthetic document fine-tuning (and some other creativity on top), which basically lets you insert false beliefs into a model by generating a bunch of fictional documents where those beliefs are true and fine-tuning the model on them.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-endnote\" id=\"fnp0f0m03b55r\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"p0f0m03b55r\"><sup><strong><a href=\"#fnrefp0f0m03b55r\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>We chose problems we’re excited to see worked on, while trying to avoid fad-like dynamics<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-endnote\" id=\"fng12d8d1lqu\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"g12d8d1lqu\"><sup><strong><a href=\"#fnrefg12d8d1lqu\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Latents refer to the hidden units of the SAE. These were originally termed “features”, but that term is also used to mean “the interpretable concept the latent refers to”, so I use a different term to minimise confusion.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-endnote\" id=\"fn0td6a2gxwht\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"0td6a2gxwht\"><sup><strong><a href=\"#fnref0td6a2gxwht\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;One of my MATS scholars make a working GPT-5 model diffing agent in a day<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-endnote\" id=\"fn5bdglmkdzr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"5bdglmkdzr\"><sup><strong><a href=\"#fnref5bdglmkdzr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This is the one line in the post <i>without <\/i>a “as of early Sept 2025” disclaimer, this feels pretty evergreen<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-endnote\" id=\"fnwuxdh4f7kh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wuxdh4f7kh\"><sup><strong><a href=\"#fnrefwuxdh4f7kh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note: \"think\" or \"chain of thought\" are terrible terms. It's far more useful to think of the chain of thought as a scratchpad that a model with very limited short-term memory can choose to use or ignore.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-endnote\" id=\"fn3qxoen8tddk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3qxoen8tddk\"><sup><strong><a href=\"#fnref3qxoen8tddk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Reasoning models break a lot of standard interpretability techniques because now the computational graph goes through the discrete, non-differentiable, and random operation of sampling thousands of times. Most interpretability techniques focus on studying a single forward pass.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-endnote\" id=\"fnlm5ixkfuzk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"lm5ixkfuzk\"><sup><strong><a href=\"#fnreflm5ixkfuzk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Not just, e.g., ones you can publish on.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-endnote\" id=\"fnidab8074tka\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"idab8074tka\"><sup><strong><a href=\"#fnrefidab8074tka\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>I called this moving fast in the blog post, but I think that may have confused some people.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-endnote\" id=\"fnwpekmwudkpd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wpekmwudkpd\"><sup><strong><a href=\"#fnrefwpekmwudkpd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Though often this is done well with just a good introduction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-endnote\" id=\"fn1bau7vsh9tk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1bau7vsh9tk\"><sup><strong><a href=\"#fnref1bau7vsh9tk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And having a well-known researcher as co-author is not sufficient evidence to avoid this, alas. I’m sure at least one paper I’ve co-authored in the past year or two is substantially false<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-endnote\" id=\"fnadytzr5d7y\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"adytzr5d7y\"><sup><strong><a href=\"#fnrefadytzr5d7y\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>It's strongly in your interests for people to build on your work because that makes your original work look better, in addition to being just pretty cool to see people engage deeply with your stuff.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-endnote\" id=\"fnva7mhfkrhm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"va7mhfkrhm\"><sup><strong><a href=\"#fnrefva7mhfkrhm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that deliberately reproducing work, or trying to demonstrate the past work is shoddy, is completely reasonable. You just need to not <i>accidentally<\/i>&nbsp;reinvent the wheel.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-endnote\" id=\"fntt0owz8koks\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tt0owz8koks\"><sup><strong><a href=\"#fnreftt0owz8koks\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>This is generally a good thing to do regardless of whether you’re focused on research taste or not!<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-endnote\" id=\"fne3252d8idmr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"e3252d8idmr\"><sup><strong><a href=\"#fnrefe3252d8idmr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And, nowadays, LLM knowledge too I guess?<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-endnote\" id=\"fn8354hd0flji\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"8354hd0flji\"><sup><strong><a href=\"#fnref8354hd0flji\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you’ll need someone who’s written several Arxiv papers to endorse you. cs.LG is the typical category for ML papers.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-endnote\" id=\"fn9oppcf0ftrh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9oppcf0ftrh\"><sup><strong><a href=\"#fnref9oppcf0ftrh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you can submit something to a workshop <i>and <\/i>to a conference, so long as the workshop is “non-archival”<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-endnote\" id=\"fnfmh579omuc6\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"fmh579omuc6\"><sup><strong><a href=\"#fnreffmh579omuc6\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>A conference paper is a fair bit more effort, and you generally want to be working with someone who understands the academic conventions and shibboleths and the various hoops you should be jumping through. But I think this can be a nice thing to aim for, especially if you're starting out and need credentials, though mech interp cares less about peer review than most academic subfields.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-endnote\" id=\"fnf09vsa4w37e\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"f09vsa4w37e\"><sup><strong><a href=\"#fnreff09vsa4w37e\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>See <a href=\"https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/\">this NeurIPS experiment<\/a>&nbsp;showing that half the spotlight papers would be rejected by an independent reviewing council<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-endnote\" id=\"fn7ruxx269r2s\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7ruxx269r2s\"><sup><strong><a href=\"#fnref7ruxx269r2s\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This is one of the most valuable things I do for my MATS scholars, IMO.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-endnote\" id=\"fnslnwemz4grq\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"slnwemz4grq\"><sup><strong><a href=\"#fnrefslnwemz4grq\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp; Unfortunately, standard reference culture, especially in the US, is to basically lie, and the amount of lying varies between contexts, rendering references mostly useless unless from a cultural context the hiring manager understands or ideally from people they know and trust. This is one of the reasons that doing AI safety mentoring programs like MATS can be extremely valuable, because often your mentor will know people who might then go on to hire you, which makes you a lower risk hire from their perspective.<\/p><\/div><\/li><\/ol>",
                "wordCount": 16572,
                "htmlHighlight": "<p><strong>Note<\/strong>: If you’ll forgive the shameless self-promotion, <strong>applications for <\/strong><a href=\"http://tinyurl.com/neel-mats-app\"><strong>my MATS stream<\/strong><\/a><strong>&nbsp;are open until<\/strong>&nbsp;<strong>Sept 12<\/strong>. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.<\/p><p><i>Last updated Sept 2 2025<\/i><\/p><h2 data-internal-id=\"TL_DR\">TL;DR<\/h2><ul><li>This post is about the mindset and process I recommend if you want to <i>do<\/i>&nbsp;mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.<ul><li>Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.<\/li><li><strong>Learn the minimum viable basics, then do research.<\/strong>&nbsp;Mech interp is an empirical science<\/li><\/ul><\/li><li>Three stages:<ul><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learn the ropes<\/strong><\/a><strong>&nbsp;(≤1 month)<\/strong>&nbsp;learn the essentials, go breadth-first;<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Learn with research mini-projects<\/strong><\/a>&nbsp;practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full projects<\/strong><\/a>, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.<\/li><\/ul><\/li><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Stage 1:<\/strong><\/a><strong>&nbsp;Learning the Ropes<\/strong><ul><li><strong>Breadth over depth; get a good baseline not perfection<\/strong><\/li><li><strong>Learn the basics<\/strong>: <a href=\"#Machine_Learning___Transformer_Basics\">Code a transformer from scratch<\/a>, <a href=\"#Mechanistic_Interpretability_Techniques\">key mech interp techniques<\/a>, <a href=\"#Using_LLMs_for_Learning\">the landscape of the field<\/a>, <a href=\"#Machine_Learning___Transformer_Basics\">linear algebra intuitions<\/a>, <a href=\"#Mechanistic_Interpretability_Coding___Tooling\">how to write mech interp code<\/a>&nbsp;(<a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">ARENA is your friend<\/a>)<\/li><li><strong>Get your hands dirty<\/strong>: Do <i>not<\/i>&nbsp;just read things. Mech interp is a fundamentally empirical science<\/li><li><strong>Move on after a month<\/strong>. Don’t expect to feel “done” or to have covered <i>all <\/i>of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real<\/li><li><a href=\"#Using_LLMs_for_Learning\"><strong>Use LLMs extensively<\/strong><\/a>&nbsp;- they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)<\/li><\/ul><\/li><li><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><strong>Unpacking the research process<\/strong><\/a>:<ul><li><a href=\"#Unpacking_the_Research_Process\">Many skills<\/a>, categorise them by the feedback loops.<ul><li>Fast skills (minutes-hours) like write/run/debug experiments<\/li><li>Slow (weeks) like how to prioritise and when to pivot<\/li><li>Very slow (months) like generating good research ideas<\/li><\/ul><\/li><li><strong>Do <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;try to learn all skills at once<\/strong>. Focus on fast/medium skills first, then slowly expand<\/li><li><a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">4 phases of research<\/a>: finding an idea (<strong>ideatio<\/strong><\/li><\/ul><\/li><\/ul>... ",
                "plaintextDescription": "Note: If you’ll forgive the shameless self-promotion, applications for my MATS stream are open until Sept 12. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.\n\nLast updated Sept 2 2025\n\n\nTL;DR\n * This post is about the mindset and process I recommend if you want to do mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.\n   * Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.\n   * Learn the minimum viable basics, then do research. Mech interp is an empirical science\n * Three stages:\n   * Learn the ropes (≤1 month) learn the essentials, go breadth-first;\n   * Learn with research mini-projects practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;\n   * Work up to full projects, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.\n * Stage 1: Learning the Ropes\n   * Breadth over depth; get a good baseline not perfection\n   * Learn the basics: Code a transformer from scratch, key mech interp techniques, the landscape of the field, linear algebra intuitions, how to write mech interp code (ARENA is your friend)\n   * Get your hands dirty: Do not just read things. Mech interp is a fundamentally empirical science\n   * Move on after a month. Don’t expect to feel “done” or to have covered all of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real\n   * Use LLMs extensively - they’re not perfect, but are better at mech interp than you right now! They’re a crucial lea"
            }
        }
    </script>
    <script>
        window.__APOLLO_FOREIGN_STATE__ = {}
    </script>
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "DiscussionForumPosting",
            "url": "https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher",
            "text": "<p><strong>Note<\/strong>: If you’ll forgive the shameless self-promotion, <strong>applications for <\/strong><a href=\"http://tinyurl.com/neel-mats-app\"><strong>my MATS stream<\/strong><\/a><strong>&nbsp;are open until<\/strong>&nbsp;<strong>Sept 12<\/strong>. I help people write a mech interp paper, often accept promising people new to mech interp, and alumni often have careers as mech interp researchers. If you’re interested in this post I recommend applying! The application should be educational whatever happens: you spend a weekend doing a small mech interp research project, and show me what you learned.<\/p><p><i>Last updated Sept 2 2025<\/i><\/p><h2 data-internal-id=\"TL_DR\">TL;DR<\/h2><ul><li>This post is about the mindset and process I recommend if you want to <i>do<\/i>&nbsp;mechanistic interpretability research. I aim to give a clear sense of direction, so give opinionated advice and concrete recommendations.<ul><li>Mech interp is high-leverage, impactful, and learnable on your own with short feedback loops and modest compute.<\/li><li><strong>Learn the minimum viable basics, then do research.<\/strong>&nbsp;Mech interp is an empirical science<\/li><\/ul><\/li><li>Three stages:<ul><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learn the ropes<\/strong><\/a><strong>&nbsp;(≤1 month)<\/strong>&nbsp;learn the essentials, go breadth-first;<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Learn with research mini-projects<\/strong><\/a>&nbsp;practice basic research skills with 1-5 day mini projects, focus on fast feedback loop skills;<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full projects<\/strong><\/a>, do 1-2 week research sprints, continue the best ones. Explore deeper skills and the mindset of a great researcher.<\/li><\/ul><\/li><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Stage 1:<\/strong><\/a><strong>&nbsp;Learning the Ropes<\/strong><ul><li><strong>Breadth over depth; get a good baseline not perfection<\/strong><\/li><li><strong>Learn the basics<\/strong>: <a href=\"#Machine_Learning___Transformer_Basics\">Code a transformer from scratch<\/a>, <a href=\"#Mechanistic_Interpretability_Techniques\">key mech interp techniques<\/a>, <a href=\"#Using_LLMs_for_Learning\">the landscape of the field<\/a>, <a href=\"#Machine_Learning___Transformer_Basics\">linear algebra intuitions<\/a>, <a href=\"#Mechanistic_Interpretability_Coding___Tooling\">how to write mech interp code<\/a>&nbsp;(<a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">ARENA is your friend<\/a>)<\/li><li><strong>Get your hands dirty<\/strong>: Do <i>not<\/i>&nbsp;just read things. Mech interp is a fundamentally empirical science<\/li><li><strong>Move on after a month<\/strong>. Don’t expect to feel “done” or to have covered <i>all <\/i>of the ropes, learn more when needed. You won’t stumble across great research insights without starting to do something real<\/li><li><a href=\"#Using_LLMs_for_Learning\"><strong>Use LLMs extensively<\/strong><\/a>&nbsp;- they’re not perfect, but are better at mech interp than you right now! They’re a crucial learning tool (when used right!)<\/li><\/ul><\/li><li><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><strong>Unpacking the research process<\/strong><\/a>:<ul><li><a href=\"#Unpacking_the_Research_Process\">Many skills<\/a>, categorise them by the feedback loops.<ul><li>Fast skills (minutes-hours) like write/run/debug experiments<\/li><li>Slow (weeks) like how to prioritise and when to pivot<\/li><li>Very slow (months) like generating good research ideas<\/li><\/ul><\/li><li><strong>Do <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;try to learn all skills at once<\/strong>. Focus on fast/medium skills first, then slowly expand<\/li><li><a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">4 phases of research<\/a>: finding an idea (<strong>ideation<\/strong>) -&gt; building intuition and hunches (<strong>exploration<\/strong>) -&gt; testing hypotheses (<strong>understanding<\/strong>) -&gt; refining and writing up (<strong>distillation<\/strong>)<\/li><\/ul><\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Stage 2:<\/strong><\/a><strong>&nbsp;Mini projects<\/strong>&nbsp;(1-5 days each for 2-4 weeks)<ul><li><a href=\"#Practicing_Exploration\">Exploration mindset<\/a>: <strong>Maximise information gain per unit time<\/strong>, learn how to get unstuck. You don't need a plan, so long as you're learning<\/li><li><a href=\"#Practicing_Understanding\">Understanding mindset<\/a>: <strong>Every research result is false until proven otherwise<\/strong>. The more exciting a result is, the more likely it is to be false. Be your own greatest critic<\/li><li>Idea quality (ideation) and write-ups (distillation) aren't the priority yet; <strong>taste and prioritization are learned by doing things<\/strong>.<\/li><li>Having good research ideas takes forever to learn, <strong>to choose early projects, cheat<\/strong>! <a href=\"#Choose_A_Project\">Pick well scoped projects<\/a>, eg extending a paper (ideas)<\/li><li><a href=\"#Using_LLMs_for_Research_Code\"><strong>Use LLMs extensively<\/strong><\/a><strong>&nbsp;<\/strong>- they should speed up your research/coding a <i>lot<\/i>&nbsp;(if you know how to use them properly!)<\/li><\/ul><\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Stage 3:<\/strong><\/a><strong>&nbsp;Towards full projects<\/strong><ul><li><strong>Work in 1-2 week sprints<\/strong>, post-mortem after each, pivot to another project unless it's going <i>great<\/i><\/li><li><a href=\"#Deepening_Your_Skills\"><strong>Slower skills<\/strong><\/a><strong>&nbsp;and <\/strong><a href=\"#Key_Research_Mindsets\"><strong>key mindsets<\/strong><\/a>: careful skepticism, awareness of the literature, prioritization, high productivity<\/li><li><a href=\"#Doing_Good_Science\"><strong>Do good science<\/strong><\/a><strong>, not flashy science<\/strong>&nbsp;- be honest about limitations, give proof you're not cherry picking, read your data, do the simple things that work, use real baselines.<\/li><li><a href=\"#Write_up_your_work_\"><strong>Write-up<\/strong><\/a><strong>&nbsp;your work<\/strong>! Distill it into a narrative, then iteratively expand it to a write-up<ul><li><strong>Good public work is <\/strong><a href=\"#Why_aim_for_public_output_\"><strong>your best credential<\/strong><\/a>&nbsp;- for careers, PhDs, finding mentors, etc<\/li><li><strong>Writing is not an afterthought<\/strong>&nbsp;- make time for it. <a href=\"#Common_mistakes\">The reader will understand less than you think<\/a><\/li><\/ul><\/li><li><strong>Practice <\/strong><a href=\"#Practicing_Ideation\"><strong>generating research ideas<\/strong><\/a>. If possible, try to imitation learn <a href=\"#Research_Taste_Exercises\">a mentor's research taste.<\/a><ul><li><a href=\"#Avoiding_Fads\">Avoid fads<\/a>, and think about <a href=\"#What_s_New_In_Mech_Interp_\">what’s new and exciting in mech interp<\/a><\/li><\/ul><\/li><\/ul><\/li><li><a href=\"#Advice_on_finding_a_mentor\"><strong>Proactively reach out to mentors<\/strong><\/a>&nbsp;Everything is <i>much<\/i>&nbsp;easier with a good mentor. Cold email, apply for mentoring programs, etc.<ul><li>Reach out to researchers who'll have time, not the most famous<\/li><\/ul><\/li><li><strong>Careers:<\/strong>&nbsp;If you want to work in the field, apply for things! <a href=\"#Where_to_apply\">Jobs<\/a>, <a href=\"#Mentoring_programs\">mentoring programs<\/a>, <a href=\"#Applying_for_grants\">funding<\/a>, <a href=\"#Relevant_Academic_Labs\">academic labs<\/a>.<ul><li>Bonus thoughts: <a href=\"#What_do_hiring_managers_look_for\">what do hiring managers look for<\/a>, <a href=\"#So_what_does_a_research_mentor_actually_do_\">what does a good research mentor actually do<\/a>, and <a href=\"#Should_you_do_a_PhD_\">should you do a PhD<\/a>?<\/li><\/ul><\/li><li>I also give various thoughts on how I'm thinking about the field nowadays, and what I’ve changed my mind about. I separate these from the practical advice, so you can take it or leave it.<ul><li>Covering: <a href=\"#Interlude__What_is_mech_interp_\">how I currently define the field<\/a>, why I'm <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">pessimistic on ambitious reverse engineering, and excited about more pragmatic approaches<\/a>, <a href=\"#What_s_New_In_Mech_Interp_\">what recent work I am<i>&nbsp;<\/i>excited about<\/a>&nbsp;and recommend building on.<\/li><li>And if any of that worldview appeals, you may want to apply to work with me via <a href=\"http://tinyurl.com/neel-mats-app\">MATS, due Sept 12<\/a>!<\/li><\/ul><\/li><\/ul><h2 data-internal-id=\"Introduction\">Introduction<\/h2><p>Mechanistic interpretability (mech interp) is, in my incredibly biased opinion, one of the most exciting research areas out there. We have these incredibly complex AI models that we don't understand, yet there are tantalizing signs of real structure inside them. Even partial understanding of this structure opens up a world of possibilities, yet is neglected by 99% of machine learning researchers. There’s so much to do!<\/p><p>I think mech interp is an unusually easy field to learn about on your own: there’s a lot of educational materials, you don’t need too much compute, and there’s short feedback loops. But if you're new, it can feel pretty intimidating to get started. This is my updated guide on how to skill up, get involved, reach the point where you can do actual research, and some advice on how to go from there to a career/academic role in the field.<\/p><p>This guide is deliberately highly opinionated. My goal is to convey a productive mindset and concrete steps that I think will work well, and give a sense of direction, rather than trying to give a fully broad overview or perfect advice. (And many of the links are to my own work because that's what I know best. Sorry!)<\/p><h3 data-internal-id=\"High_Level_Framing\">High-Level Framing<\/h3><p>My core philosophy for getting into mech interp is this: learn the absolute minimal basics as quickly as possible, and then immediately transition to learning by doing research.<\/p><p>The goal is not to read every paper before you touch research. When doing research you'll notice gaps and go back to learn more. But being grounded in a project will give you vastly more direction to guide your learning, and contextualise why anything you’re learning actually matters. You just want enough grounding to start a project with some understanding of what you’re doing.<\/p><p>Don't stress about the research quality at first, or having the perfect project idea. Key skills, like <a href=\"https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/Ldrss6o3tiKT6NdMm\">research taste<\/a>&nbsp;and the ability to prioritize, take time to develop. Gaining experience—even messy experience—will teach you the basics like how to run and interpret experiments, which in turn help you learn the high-level skills.<\/p><p>I break this down into three stages:<\/p><ol><li><a href=\"#Stage_1__Learning_the_Ropes\"><strong>Learning the ropes<\/strong><\/a>, where you work through the basics breadth first, and after at most a month, move on to stage 2<\/li><li><a href=\"#Stage_2__Practicing_Research_with_Mini_Projects\"><strong>Practicing research with mini-projects<\/strong><\/a>. Work on throwaway, 1-5 day research projects. Focus on practicing the basic research skills with the fastest feedback loops, don’t stress about having the best ideas, or writing them up. After 2-4 weeks, move on to stage 3<\/li><li><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><strong>Work up to full-projects<\/strong><\/a>: work in 1-2 week sprints. After each, do a post-mortem and pivot to something else, <i>unless <\/i>it was going great and has momentum. Eventually, you should end up working on something longer-term. Start thinking about the deeper skills and research mindsets, practice having good ideas, and prioritize making good public write-ups of sprints that went well<\/li><\/ol><h2 data-internal-id=\"Stage_1__Learning_the_Ropes\">Stage 1: Learning the Ropes<\/h2><p>Your goal here is learning the basics: how to write experiments with a mech interp library, understanding the key concepts, getting the lay of the land.<\/p><p data-internal-id=\"ftnt_ref1\">Your aim is learning enough that the rest of your learning can be done via doing research, <i>not<\/i>&nbsp;finishing learning. Prioritize ruthlessly. <strong>After max 1 month<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-noteref\" id=\"fnrefnifk1wb1jum\"><sup><a href=\"#fnnifk1wb1jum\">[1]<\/a><\/sup><\/span><strong>, move on to stage 2<\/strong>. I’ve flagged which parts of this I think are essential, vs just nice to have.<\/p><p><strong>Do not just read papers <\/strong>- a common mistake among academic types is to spend months reading as many papers as they can get their hands on before writing code. Don’t do it. Mech interp is an empirical science, getting your hands dirty gives key context for your learning. Intersperse reading papers with doing coding tutorials or small research explorations. See <a href=\"https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">my research walkthroughs<\/a>&nbsp;for an idea of what tiny exploratory projects can look like.<\/p><p>LLMs are a key tool - see <a href=\"#h.ab01gbohcxm5\">the section below<\/a>&nbsp;for advice on using them well<\/p><h3 data-internal-id=\"Machine_Learning___Transformer_Basics\"><strong>Machine Learning &amp; Transformer Basics<\/strong><\/h3><p><i>Assuming you already know basic Python and introductory ML concepts.<\/i><\/p><ul><li><strong>Maths:<\/strong><ul><li><strong>Linear Algebra is King (Essential):<\/strong>&nbsp;You need to think in vectors and matrices fluently. This is by far the highest value set of generic math you should learn to do mech interp or ML research.<ul><li><i>Resource:<\/i>&nbsp;3Blue1Brown's<a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\">&nbsp;Essence of Linear Algebra<\/a>.<\/li><li><strong>Highly recommended<\/strong>: Put <a href=\"https://transformer-circuits.pub/2021/framework/index.html\">A Mathematical Framework For Transformer Circuits<\/a>&nbsp;in the context window and have the LLM generate exercises to test your intuitions about transformer internals.<\/li><li>LLMs are great for checking whether linear algebra actually clicks. Try summarizing what you've learned and the links between different concepts and ask an LLM whether you are correct. For example:<ul><li>Ensure you understand SVD and why it works<\/li><li>What does changing basis mean and why does it matter<\/li><li>Key ways a low rank and full rank matrix differ<\/li><\/ul><\/li><\/ul><\/li><li><strong>Other Bits:<\/strong>&nbsp;Basic probability, info theory, optimization, vector calculus.<ul><li>Use an LLM tutor to quiz your understanding on the parts most relevant to transformers<\/li><\/ul><\/li><li>Generally don’t bother learning other areas of maths (unless doing it for fun!)<\/li><\/ul><\/li><li><strong>Practical ML with PyTorch: (Essential)<\/strong><ul><li><p data-internal-id=\"ftnt_ref2\">Code a simple Transformer (like GPT-2) from scratch. ARENA Chapter 1.1 is a great coding tutorial<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-noteref\" id=\"fnrefue9pdw6v8rj\"><sup><a href=\"#fnue9pdw6v8rj\">[2]<\/a><\/sup><\/span><\/p><ul><li><p data-internal-id=\"ftnt_ref2\">This builds intuitions for mech interp <i>and <\/i>on using PyTorch.<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">I have two video tutorials on this, starting from the basics - <a href=\"https://www.youtube.com/watch?v=bOYE6E8JrtU&amp;list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz\">start here<\/a>&nbsp;if you’re not sure what to do!<\/p><\/li><li><p data-internal-id=\"ftnt_ref2\">And use LLMs to fill in any background things you’re missing, like PyTorch basics<\/p><\/li><\/ul><\/li><\/ul><\/li><li><strong>Cloud GPUs:<\/strong><ul><li>You’ll need to be able to run language models, which (typically) needs a GPU<\/li><li>You can start with Google Colab to get started fast, but it’ll be very constraining to use long-term. Learn to rent and use a cloud GPU.<ul><li>Newer Macbook Pros, or computers with powerful gaming GPUs may also be able to run LLMs locally<\/li><\/ul><\/li><li><i>Resource:<\/i>&nbsp;ARENA has a<a href=\"https://arena-appendix.streamlit.app/cloud-gpus\">&nbsp;<\/a><a href=\"https://arena-chapter0-fundamentals.streamlit.app/#vm-setup-instructions\">guide<\/a>. I like<a href=\"http://runpod.io/\">&nbsp;<\/a><a href=\"http://runpod.io\">runpod.io<\/a>&nbsp;as a provider;<a href=\"http://vast.ai/\">&nbsp;vast.ai<\/a>&nbsp;is cheaper.<\/li><li>nnsight also lets you do some <a href=\"https://nnsight.net/notebooks/tutorials/get_started/start_remote_access/\">interpretability on certain models they host themselves<\/a>, including LLaMA 3 405B, which can be a great way to work with larger models.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Mechanistic_Interpretability_Techniques\">Mechanistic Interpretability Techniques<\/h3><p>A lot of mech interp research looks like knowing the right technique to apply and in what context. This is a key thing to prioritise getting your head around when starting out. You’ll learn this with a mix of reading educational materials and doing coding tutorials like ARENA (discussed in next sub-section).<\/p><ul><li><a href=\"https://arxiv.org/abs/2405.00208\">Ferrando et al<\/a>&nbsp;is a good <strong>overview<\/strong>&nbsp;of the key techniques - it’s long enough that you shouldn’t prioritise reading it in full, but it’s a great reference<ul><li>Put it in a LLM context window and ask questions, or to write you exercises<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Essential<\/strong>: Make sure you understand these <strong>core techniques<\/strong>, well enough that you can code it up yourself on a simple model like GPT-2 Small<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-noteref\" id=\"fnrefhh6mwdeo4zm\"><sup><a href=\"#fnhh6mwdeo4zm\">[3]<\/a><\/sup><\/span>:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Activation Patching<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Linear Probes<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Using Sparse Autoencoders (SAEs) (you only need to write code that uses an SAE, not trains one)<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Max Activating Dataset Examples<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Nice-to-have:<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Steering Vectors<\/p><\/li><li><p data-internal-id=\"ftnt_ref3\">Direct Logit Attribution (DLA) (a simpler version is called logit lens)<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref3\"><strong>Key exercise<\/strong>: Describe each technique to an LLM with Ferrando et al in the context window and ask for feedback. Iterate until you get it all right.<\/p><ul><li><p data-internal-id=\"ftnt_ref3\">Use an anti-sycophancy prompt to get real feedback, by pretending someone else wrote your answer, e.g. “I saw someone claim this, it seems pretty off to me, can you help me give them direct but constructive feedback on what they missed? [insert your description]”<\/p><\/li><\/ul><\/li><\/ul><\/li><li>Remember that there’s a bunch of valuable <strong>black-box interpretability <\/strong>techniques! (ie that don’t use the model’s internals) You can often correctly guess a model’s algorithm by reading its chain of thought. Careful variation of the prompt is a powerful way to causally test hypotheses.<ul><li>They’re an additional tool. Often the correct first step in an investigation is just talking to the model and bunch and observing its behaviour. Don’t be a purist and dismiss them as “not rigorous” - they have uses and flaws, just like any other technique.<ul><li>One <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">project I supervised<\/a>&nbsp;on interpreting “self-preservation” in frontier models started with simple black-box techniques, and it just worked, we never needed anything fancier.<\/li><\/ul><\/li><li>Understand fancier black-box techniques like <a href=\"https://arxiv.org/abs/2312.12321\">token forcing<\/a>&nbsp;(aka prefill attacks) where you put words in a model’s mouth.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Mechanistic_Interpretability_Coding___Tooling\">Mechanistic Interpretability Coding &amp; Tooling<\/h3><ul><li><p data-internal-id=\"ftnt_ref4\"><strong>Goal:<\/strong>&nbsp;Get comfortable running experiments and \"playing\" with model internals. Get the engineering basics down<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-noteref\" id=\"fnrefsxyjce3nii\"><sup><a href=\"#fnsxyjce3nii\">[4]<\/a><\/sup><\/span>. Get your hands dirty<strong>.<\/strong><\/p><\/li><li><strong>ARENA<\/strong>: ARENA has <a href=\"https://arena-chapter1-transformer-interp.streamlit.app/\">a set of fantastic coding tutorials by Callum McDougall<\/a>, you should just go do these. But there’s tons, so <strong>prioritize ruthlessly<\/strong>.<ul><li><strong>Essential<\/strong><i><strong>:<\/strong><\/i><strong>&nbsp;Chapter 1.2<\/strong>&nbsp;(Interpretability Basics – prioritize the first 3 sections on tooling, direct observation, and patching).<\/li><li><i>Recommended: <\/i>1.4.1 (Causal Interventions &amp; Activation Patching – this is a core technique).<\/li><li><i>Worthwhile<\/i>: 1.3.2 (Sparse Autoencoders (SAEs) – Skim or Skip section 1, the key thing to get from the rest is an intuition for what SAEs are, strengths and weaknesses, and how to use an open source SAE. Don’t worry about training them).<\/li><\/ul><\/li><li><strong>Tooling <\/strong>(<strong>Essential<\/strong>)<strong>:<\/strong>&nbsp;Get proficient with at least one mech interp library, this is what you’ll use to run experiments.<ul><li><a href=\"https://github.com/TransformerLensOrg/TransformerLens\">TransformerLens<\/a>: best for small models &lt;=9B where you want to write more complex interpretability experiments, or work with many models at once.<ul><li>As of early Sept 2025, TransformerLens <a href=\"https://github.com/TransformerLensOrg/TransformerLens/releases/tag/v3.0.0a5\">v3<\/a>&nbsp;is in alpha, works well with large models and is far more flexible.<\/li><\/ul><\/li><li><a href=\"http://nnsight.net/\">nnsight<\/a>: More performant, works well on larger models, it’s just a wrapper around standard LLM libraries like HuggingFace transformers<\/li><\/ul><\/li><li><strong>LLM APIs<\/strong>: Learn how to use an LLM API to call an LLM programmatically. This is super useful for measuring qualitative things about some data, and for generating synthetic datasets<ul><li>I like <a href=\"http://openrouter.ai\">openrouter.ai<\/a>&nbsp;which lets you access almost all the important LLMs from a single place. GPT5 and Gemini are reasonably priced and good defaults, they have a range of sizes<ul><li>Cerebras and Groq have <i>way <\/i>higher throughput than normal providers, and serve a handful of open source models, they may be worth checking out.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref6\">Exercise: Make a happiness steering vector (for e.g. GPT-2 Small) by having an LLM via an API generate 32 happy prompts and 32 sad prompts, and taking the difference in mean activations<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-noteref\" id=\"fnrefkte6u8splw\"><sup><a href=\"#fnkte6u8splw\">[5]<\/a><\/sup><\/span>&nbsp;(e.g. the residual stream at the middle layer). Add this vector to the model’s residual stream<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-noteref\" id=\"fnref2ob115pcmet\"><sup><a href=\"#fn2ob115pcmet\">[6]<\/a><\/sup><\/span>&nbsp;while generating responses to some example prompts, and use an LLM API to rate how happy they seem, and see this score go up when steering.<\/p><\/li><\/ul><\/li><li><strong>Open source LLMs<\/strong>: You’ll want to work a lot with open source LLMs, as the thing you’re trying to interpret. The best open source LLM changes a lot<ul><li><p data-internal-id=\"ftnt_ref7\">As of early Sept 2025, Qwen3 is a good default model family. Each model has reasoning and non-reasoning mode, there’s a good range of sizes, and most are dense<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-noteref\" id=\"fnref1b9r0ass7sd\"><sup><a href=\"#fn1b9r0ass7sd\">[7]<\/a><\/sup><\/span>&nbsp;<\/p><ul><li><p data-internal-id=\"ftnt_ref7\">Gemma 3 and LLaMA 3.3 are decent non-reasoning models. I’ve heard bad things about gpt-oss and LLaMA 4<\/p><\/li><\/ul><\/li><li><i>Gotcha: <\/i>The different open source LLMs often have different tokenizations and formats for chat or reasoning tokens. Using the wrong token format can only somewhat degrade performance and may be hard to notice while corrupting your results - keep an eye out, try hard to find where this might be documented, and sanity check by e.g. comparing to official evals<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Understanding_the_literature\">Understanding the literature<\/h3><p>Your priority is to understand the concepts and the basics, but you want a sense for the landscape of the field, so you should practice reading at least some papers.<\/p><ul><li>Remember, <strong>breadth over depth<\/strong>. Skim things, get a sense of what's out there, and only dive into the things that are most interesting.<ul><li>You should be heavily using <strong>LLMs<\/strong>&nbsp;here. Give them something you're considering reading and get a summary, ask questions about the work, summarise your understanding to it and ask for feedback (with an anti-sycophancy prompt).<ul><li>If you aren't able to verify yourself, cross-reference by asking multiple LLMs and making sure they all say consistent things.<\/li><\/ul><\/li><\/ul><\/li><li>Here’s <a href=\"https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite\">a list of my favourite papers<\/a>&nbsp;(as of mid 2024) with summaries and opinions<ul><li>Do <i>not <\/i>try to read all of these in full. Skim summaries, skim abstracts, pick a few to explore deeper with an LLM, <i>then<\/i>&nbsp;decide if you want to read the full paper.<\/li><li><a href=\"https://www.youtube.com/@neelnanda2469\">My YouTube Channel<\/a>:<a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T&amp;index=1\">&nbsp;<\/a><a href=\"https://www.youtube.com/watch?v=KV5gbOmHbjU&amp;list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&amp;pp=gAQB\">Paper walkthroughs<\/a>, <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">recordings of myself doing research<\/a>, and talks.<\/li><\/ul><\/li><li><a href=\"https://arxiv.org/abs/2501.16496\">Open Problems In Mechanistic Interpretability<\/a>&nbsp;is a decent recent literature review, that a lot of top mech interp people were involved in<ul><li>Be warned that the paper basically consists of a bunch of opinionated and disagreeable researchers writing their own sections and often having strong takes. Don’t defer to it too much, but it's a good way to quickly assess what's out there.<\/li><\/ul><\/li><li><strong>Deep dives<\/strong>: You should read at least one paper carefully and in full. This is a useful skill that you will use in research projects where there’s a handful of extremely relevant papers to your project<ul><li>This is much more than just reading the words! You should write out a summary, try to understand the surrounding context with LLM help, be able to describe why the paper exists, the motivation, the problem it's trying to solve, etc.<\/li><li>Aim for a barbell strategy: put minimal effort into most papers and a lot of effort into a few.<\/li><\/ul><\/li><li><strong>LLMs<\/strong>: LLMs are a super useful tool for exploring the literature, but easy to shoot yourself in the foot with.<ul><li>As a search engine over the literature (especially with some lit reviews in context, or a starting paper), basically doing a lit review, finding relevant work for a question you have, etc.<ul><li><p data-internal-id=\"ftnt_ref8\">As a tool to help you skim a paper - put the paper in the context window<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-noteref\" id=\"fnrefbzop9pji3nl\"><sup><a href=\"#fnbzop9pji3nl\">[8]<\/a><\/sup><\/span>&nbsp;then get a summary, ask it questions, etc<\/p><\/li><li>If you’re concerned about hallucinations, you can&nbsp;ask it to support answers with quotes (and verify these are real and make sense), or give its answer to another LLM and ask for harsh critique of all the inaccuracies. Honestly, I often don’t bother though, frontier reasoning models are pretty good now.<\/li><\/ul><\/li><li>As a tool to help with deep dives - you need to actually read the paper, but I recommend having the LLM chat open as you read with the paper in the context and asking it questions, for context, etc every time you get confused.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Using_LLMs_for_Learning\">Using LLMs for Learning<\/h3><p><i>Note: I expect this section to go out of date fast! Written early Sept 2025<\/i><\/p><p>LLMs are a super useful tool for learning, especially in a new field. While they struggle to beat experts, they often beat novices. If you aren’t using them regularly throughout this process, I’d guess you’re leaving a bunch of value on the table.<\/p><p>But LLMs have weird flaws and strengths, and it’s worth being intentional about how you use them:<\/p><ul><li><strong>Use a good model<\/strong>:&nbsp;The best paid models are way better than e.g. free ChatGPT. Don't be a cheapskate; if you can, get a $20/month subscription, it makes a big difference. Gemini 2.5 Pro, Claude 4.1 Opus with extended thinking, and GPT-5 Thinking are all reasonable. (do <i>not <\/i>use non-thinking GPT-5 or anything older like GPT-4o, reasoning models are a big upgrade)<ul><li>If you can’t get a subscription, Gemini 2.5 Pro is also available for free, and is the best.<\/li><li>Use Gemini 2.5 Pro via <a href=\"https://aistudio.google.com/prompts/new_chat\">AI Studio<\/a>, it’s way better than the main Gemini interface&nbsp;and has much nicer rate limits for free users. Always use compare mode (the button in the header with two arrows) to see two responses in parallel from Pro<\/li><li>See <a href=\"https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher?commentId=jDzbZGnjWDMsNjDPQ\">thoughts<\/a> from my MATS alum Paul Bogdan comparing different LLMs for learning, and why he currently prefers Gemini<\/li><\/ul><\/li><li><strong>System Prompts:<\/strong>&nbsp;System prompts make a big difference - be concrete and specific about what you want, and how you want it done.<ul><li>LLMs are good at this: I'll just ramble at one about what the task is, my criteria, the failure modes I don't want, and then it’ll just write the prompt for me<\/li><li>If the prompt doesn’t work, tell the LLM what it did wrong, and see if it can rewrite the prompt for you.<\/li><\/ul><\/li><li><strong>Merge perspectives<\/strong>:<ul><li>Ask a Q to multiple different frontier LLMs, give LLM B’s response to LLM A and ask it to assess the strengths and weaknesses then merge.<ul><li>If a point is in both original responses, it’s probably not a hallucination<\/li><\/ul><\/li><li>If you want to fact check an LLM’s answer, give it to another LLM with an anti-sycophancy prompt<\/li><\/ul><\/li><li><strong>Anti-Sycophancy Prompts:<\/strong>&nbsp;LLMs are bad at giving critical feedback. Frame your request so the sycophantic thing to do is to be critical, by pretending someone else wrote the thing you want feedback on.<ul><li><i>\"A friend wrote this explanation and asked for brutally honest feedback. They'll be offended if I hold back. Please help me give them the most useful feedback.\"<\/i><\/li><li><i>\"I saw someone claiming this, but it seems pretty dumb to me. What do you think?\"<\/i><\/li><li><i>“Some moron wrote this thing, and I find this really annoying. Please write me a brutal but truthful response”<\/i><\/li><\/ul><\/li><li><strong>Learn actively, not passively:<\/strong><ul><li><strong>Summarize <\/strong>your understanding back to the LLM in your own words and ask for critical feedback. Do this every time you read a paper or learn about a new concept<\/li><li>Try having it teach you <strong>socratically<\/strong>. Note: you can probably design a better system prompt than the official “study mode”<\/li><li>Ask the LLM to <strong>generate exercises<\/strong>&nbsp;to test your understanding, including maths and coding exercises as appropriate.<ul><li>Gemini can make multiple choice quizzes, which some enjoy<\/li><li>Coding exercises can be requested with accompanying tests, and template code with blank functions for you to fill out, a la the ARENA tutorials.<\/li><\/ul><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref9\"><strong>Context engineering:<\/strong>&nbsp;Modern LLMs are much more useful with relevant info in context. If you give them the paper in question, or source code of the relevant library<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-noteref\" id=\"fnref207k0k5nobb\"><sup><a href=\"#fn207k0k5nobb\">[9]<\/a><\/sup><\/span>, they’ll be far more helpful.<\/p><ul><li><p data-internal-id=\"ftnt_ref9\">See <a href=\"https://drive.google.com/drive/u/0/folders/1GfrgKJwndk-twnJ8K7Ba-TE9i_8wBWAU\">this folder<\/a>&nbsp;for a bunch of saved context files for mech interp queries. If you don’t know what you need, just use <a href=\"https://drive.google.com/file/d/18cF3lkU17_elUSv0zk8KSVejM1jGfNnz/view?usp=drive_link\">this default file<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref9\">I recommend Gemini 2.5 Pro (1M context window) via<a href=\"http://aistudio.google.com/\">&nbsp;aistudio.google.com<\/a>; the UI is better. Always turn compare mode on, you get two answers in parallel<\/p><\/li><\/ul><\/li><li><strong>Voice dictation<\/strong>: If you dictate to your LLM, via free speech-to-text software, and run it with no editing, it’ll understand fine. I personally find this much easier, especially when brain-dumping.<ul><li><a href=\"http://superwhisper.com\">Superwhisper<\/a>&nbsp;on Mac is great; Superwhisper is not currently available on Windows, but Windows users can use <a href=\"https://wisprflow.ai/\">Whispr Flow<\/a>.<\/li><\/ul><\/li><li><strong>Coding<\/strong>: LLM tools like Cursor are great for coding, but <i>not <\/i>if your goal is to learn. For things like ARENA, only let yourself use browser-based LLMs, and only use them as a tutor. Don’t copy and paste code, your goal is to learn not complete exercises.<\/li><\/ul><h2 data-internal-id=\"Interlude__What_is_mech_interp_\">Interlude: What is mech interp?<\/h2><p><i>Feel free to skip to the<\/i>&nbsp;<i>“<\/i><a href=\"#The_Big_Picture__Learning_the_Craft_of_Research\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p data-internal-id=\"ftnt_ref10\">At this point it’s worth reflecting on what mech interp actually <i>is<\/i>. What are we even doing here? There isn't a consensus definition on how exactly to define mechanistic interpretability, and different researchers will give very different takes. But <i>my<\/i>&nbsp;working definition is as follows<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-noteref\" id=\"fnref979wnkvgpa4\"><sup><a href=\"#fn979wnkvgpa4\">[10]<\/a><\/sup><\/span>.<\/p><ul><li><strong>Interpretability<\/strong>&nbsp;is the study of understanding models, gaining insight into their behavior, the cognition inside of them, why and how they work, etc. This is the important part and the heart of the field.<\/li><li><strong>Mechanistic<\/strong>&nbsp;means using the internals of the model, the weights and activations<\/li><li>So <strong>mechanistic interpretability <\/strong>is any approach to understanding the model that uses its internals.<ul><li>This is distinct from some other worthwhile directions, like <strong>black box interpretability<\/strong>, understanding models without using the internals, and <strong>model internals<\/strong>, using the internals of the model for other things like steering vectors.<\/li><\/ul><\/li><\/ul><p><strong>Why this definition?<\/strong>&nbsp;To do impactful research, it's often good to find the directions that other people are missing. I think of most of machine learning as non-mechanistic non-interpretability. 99% of ML research just looks at the inputs and outputs to models, and treats its north star as controlling their behavior. Progress is defined by making a number go up, not to explain why it works. This has been very successful, but IMO leaves a lot of value on the table. Mechanistic interpretability is about doing better than this, and has achieved a bunch of cool stuff, like <a href=\"https://arxiv.org/abs/2310.16410\">teaching grandmasters how to play chess better by interpreting AlphaZero<\/a>.<\/p><p><strong>Why care?<\/strong>&nbsp;Obviously, our goal is not “do things if and only if they fit the above definition”, but I find it a useful one. To discuss this, let’s first consider our actual goals here. To me, <strong>the ultimate goal is to make human-level AI systems (or beyond) safer<\/strong>. I do mech interp because I think we’ll find enough understanding of what happens inside a model to be pragmatically useful here (also, because mech interp is fun!): to better understand how they work, detect if they're lying to us, detect and diagnose unexpected failure modes, etc. But people’s goals vary, e.g. real-world usefulness today, aesthetic beauty, or scientific insight. It’s worth thinking about what yours are.<\/p><p>Some implications of this framing worth laying out:<\/p><ul><li>My ultimate <strong>north star is pragmatism<\/strong>&nbsp;- achieve enough understanding to be (reliably) useful. Subgoals like “completely reverse engineer the model” are just means to an end.<ul><li>One of my big shifts in research prioritization in recent years is concluding that <strong>reverse engineering is not the right aim<\/strong>. Instead, I think we should just more directly try to do pragmatic work that enables us to do useful things using internals. I discuss this shift more <a href=\"#A_Pragmatic_Vision_for_Mech_Interp\">later on<\/a>.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref11\">This is a <strong>broad definition<\/strong>. Historically, the field has focused on more specific agendas, like ambitious reverse engineering of models. But I think we shouldn’t limit ourselves, there’s many other important and neglected directions and the field is large enough to cover a lot of ground<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-noteref\" id=\"fnref3zw26zes9dx\"><sup><a href=\"#fn3zw26zes9dx\">[11]<\/a><\/sup><\/span><\/p><\/li><li>It’s about <strong>understanding<\/strong>, not just using internals - model internals methods like steering vectors can be useful for shaping a model’s behaviour, but compete with many powerful methods like prompting and fine-tuning. Very few areas of ML can achieve understanding<\/li><li><strong>Don’t be a purist<\/strong>&nbsp;- using internals is a means to an end. If black-box methods are the right tool, use them<\/li><\/ul><h2 data-internal-id=\"The_Big_Picture__Learning_the_Craft_of_Research\">The Big Picture: Learning the Craft of Research<\/h2><p data-internal-id=\"ftnt_ref12\">So, you've gone through the tutorials, you understand the core concepts, and you can write some basic experimental code. Now comes the hard part: learning how to actually do mech interp research<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-noteref\" id=\"fnref7cxhc64szn8\"><sup><a href=\"#fn7cxhc64szn8\">[12]<\/a><\/sup><\/span>.<\/p><p>This is an inherently difficult thing to learn, of course. But IMO people often misunderstand what they need to do here, try to learn everything at once, or more generally make life unnecessarily hard for themselves. The key is to break the process down, understand the different skills involved, and focus on <strong>learning the pieces with the fastest feedback loops first<\/strong>.<\/p><p data-internal-id=\"ftnt_ref13\">I suggest breaking this down into two stages<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-noteref\" id=\"fnref9wj0u0qz3q\"><sup><a href=\"#fn9wj0u0qz3q\">[13]<\/a><\/sup><\/span>.<\/p><p><strong>Stage 2<\/strong>: working on a bunch of throwaway mini projects of 1-5 days each. Don't stress about choosing the best projects or producing public output. The goal is to learn the skills with the fastest feedback loops.<\/p><p><strong>Stage 3: <\/strong>After a few weeks of these, start to be more ambitious: paying more attention to how you choose your projects, gaining the subtler skills, and how to write things up. I still recommend working iteratively, in one to two week sprints, but ending up with longer-term projects if things go well.<\/p><p>Note: Unlike stage 1 to 2, the transition from stages two to three should be fairly gradual as you take on larger projects and become more ambitious. A good default would be after three to four weeks in stage two, but you don’t need to have a big formal shift.<\/p><p><strong>Mentorship<\/strong>: A good mentor is a major accelerator, and finding one should be a major priority for you. In the careers section, I provide advice on <a href=\"#Advice_on_finding_a_mentor\">how to go about finding a good mentor<\/a>, and <a href=\"#So_what_does_a_research_mentor_actually_do_\">how concretely they can add value<\/a>. In the rest of the post I'll write most of it assuming you do not have a mentor and then flag the ways to use a mentor where appropriate.<\/p><h3 data-internal-id=\"Unpacking_the_Research_Process\">Unpacking the Research Process<\/h3><p>I find it helpful to think of research as a cycle of four distinct stages. Read <a href=\"https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand\">my blog post on the research proces<\/a>&nbsp;for full details, but in brief:<\/p><ul><li><strong>Ideation:<\/strong>&nbsp;You choose a research problem or a general domain to focus on.<\/li><li><strong>Exploration:<\/strong>&nbsp;You may not have a specific hypothesis yet; you’re just trying to figure out the right questions to ask, and build deeper intuition for the domain. Your north star is to gain information and surface area.<\/li><li><strong>Understanding:<\/strong>&nbsp;This begins when you have a concrete hypothesis, and some intuitive understanding of the domain. Your north star is to convince yourself that the hypothesis is true or false.<\/li><li><strong>Distillation:<\/strong>&nbsp;Once you’re convinced, your north star is to compress your findings into concise, rigorous truth that you can communicate to the world - create enough experimental evidence to convince others, write it up clearly, and share it.<\/li><\/ul><p>Underpinning these stages is a host of skills, best separated by how quickly you can apply them and get feedback. We learn by doing things and getting feedback, so you’ll learn the fast ones much more quickly. I put a rough list and categorization below.<\/p><p>My general advice is <strong>to prioritize learning these in order of feedback loops<\/strong>. If it seems like you need a slow skill to get started, like the taste to choose a good research problem, find a way to cheat rather than stressing about not having that skill (e.g. doing an incremental extension to a paper, getting one from a mentor, etc).<\/p><ul><li><strong>Fast Loop (minutes-hours):<\/strong><ul><li>Planning and writing experiment code<ul><li><strong>Medium<\/strong>: Designing great experiments<\/li><li><strong>Medium<\/strong>: Knowing when to write hacky vs. quality code.<\/li><\/ul><\/li><li>Running/debugging experiments<ul><li><strong>Medium/Slow<\/strong>: Spotting and fixing subtle bugs (e.g., you got your tokenization subtly wrong, you didn’t search hyper-parameters well enough, etc)<\/li><\/ul><\/li><li>Interpreting the results of a single experiment.<ul><li><strong>Medium<\/strong>: Understanding whether your results support your conclusions<\/li><li><strong>Slow<\/strong>: Spotting subtle interpretability illusions where your results don't actually support your claims<\/li><\/ul><\/li><\/ul><\/li><li><strong>Medium Loop (days):<\/strong><ul><li>Developing a conceptual understanding of mech interp<ul><li><strong>Slow<\/strong>: Noticing and fixing your own subtle confusions<\/li><li><strong>Slow<\/strong>: Build a deep knowledge of the literature<\/li><\/ul><\/li><li>Knowing how to explore without getting stuck<\/li><li>Writing up results<ul><li><strong>Slow<\/strong>: Communicating your work in a way that’s genuinely clear to people.<\/li><li><strong>Slow<\/strong>: Communicating why your work is <i>interesting<\/i>&nbsp;to people<\/li><\/ul><\/li><\/ul><\/li><li><strong>Slow Loop (weeks):<\/strong><ul><li>Prioritizing which experiment to do next<\/li><li>Knowing when to continue with a research direction or pivot to another angle of attack/another project<\/li><li>Identifying bad research ideas, <i>without <\/i>doing a project on them first<\/li><\/ul><\/li><li><strong>Very Slow Loop (months):<\/strong><ul><li>Coming up with good research ideas. This is the core of \"research taste.\"<\/li><\/ul><\/li><\/ul><p>Your progression should be simple: First, focus on the fast/medium skills behind exploration and understanding with throwaway projects. Then, graduate to end-to-end projects where you can intentionally practice the deeper skills, and practice ideation and distillation too.<\/p><h3 data-internal-id=\"What_is_research_taste_\">What is research taste?<\/h3><p>A particularly important and fuzzy type of skill is called research taste. I basically think of this as the bundle of intuitions you get with enough research experience that let you do things like come up with good ideas, predict if an idea is promising, have conviction in good research directions, etc. Check out <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">my post on the topic<\/a>&nbsp;for more thoughts.<\/p><p>I broadly think you should just ignore it for now, find ways to compensate for not having much yet, and focus on learning the fast-medium skills, and this will give you a much better base for learning it. In particular, it's much faster to learn with a mentor, so if you don't have a mentor at the start, you should prioritize other things.<\/p><p>But you want to learn it eventually, so it's good to be mindful of it throughout, and look for opportunities to practice and learn lessons. I recommend treating it as a nice-to-have but not stressing about it<\/p><p>Note, one important trap here is that having good taste can often manifest as having confidence and conviction in some research direction. But often novice researchers develop this confidence and conviction significantly <i>before <\/i>they develop the ability to not be confident in bad ideas. It’s often a good learning experience to once or twice pursue a thing you feel really convinced is going to be epic and then discover you're wrong, so it's not that bad an outcome, especially in stage 2 (mini-projects) but be warned.<\/p><h2 data-internal-id=\"Stage_2__Practicing_Research_with_Mini_Projects\">Stage 2: Practicing Research with Mini-Projects<\/h2><p>With that big picture in mind, let's get our hands dirty. You want to do a series of ~1-5 day mini-projects, for maybe 2-4 weeks. The goal right now is to learn the craft, not to produce groundbreaking research.<\/p><p>Focus on practicing exploration and understanding and gaining the fast/medium skills, leave aside ideation and distillation for now. If you produce something cool and want to write it up, great! But that’s a nice-to-have, not a priority.<\/p><p>Once you finish a mini-project, remember to do a post-mortem. Spend at least an hour analyzing: what did you do? What did you try? What worked? What didn't? What mistakes did you make? What would you do differently if doing this again? And how can you integrate this into your research strategy going forwards?<\/p><h3 data-internal-id=\"Choose_A_Project\">Choose A Project<\/h3><p>Some suggested starter projects<\/p><ul><li><strong>Replicate and Extend a Paper:<\/strong>&nbsp;A classic for a reason. Replicate a key result, then extend it. Suggestions:<ul><li><a href=\"https://arxiv.org/abs/2406.11717\">Refusal is mediated by a single direction<\/a><ul><li>Extending papers can vary a lot in difficulty. For example, applying the method to study refusal on a new model is easy as you can reuse the same data, while applying it to a new concept is harder.<\/li><li>Skills: practicing activation patching and steering vectors.<\/li><\/ul><\/li><li><a href=\"http://thought-anchors.com\">Thought Anchors<\/a>: apply these reasoning model interpretability methods to new types of prompts, or explore some prompts using the linked interface, or see if you can improve on the methods/invent your own.<ul><li>Skills: reasoning model interpretability, using LLM APIs, and working with modern models<\/li><\/ul><\/li><li>Replicate the truth probes in <a href=\"https://arxiv.org/abs/2310.06824\">Geometry of Truth<\/a>&nbsp;on a more modern model and try applying them in more interesting settings. How well do they generalise? Can you break them? If so, can you fix this?<ul><li>Skills: probing, supervised learning, dataset creation<\/li><\/ul><\/li><\/ul><\/li><li><strong>Play around with something interesting:<\/strong><ul><li>Use <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia's attribution graphs<\/a>&nbsp;to form a hypothesis about Gemma 2B, then use other methods (e.g. prompting) to verify it.<ul><li>Skills: Attribution graphs, scientific mindset, prompting<\/li><\/ul><\/li><li>Play with <a href=\"https://huggingface.co/collections/bcywinski/gemma-2-9b-it-taboo-6826efbb186dfce0616dd174\">Bartosz Cywiński's taboo models<\/a>&nbsp;that have a secret word programmed in and test as many methods as you can to find it.<ul><li>If you’re feeling ambitious: train your own models with a more complex secret, and try to interpret those.<\/li><li>Skills: Logit lens, SAEs, black box methods<\/li><\/ul><\/li><li>Explore <a href=\"https://github.com/clarifying-EM/model-organisms-for-EM\">the models<\/a>&nbsp;from the <a href=\"https://www.emergent-misalignment.com/\">emergent<\/a>&nbsp;<a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">misalignment<\/a>&nbsp;<a href=\"https://openai.com/index/emergent-misalignment/\">papers<\/a>.<ul><li>Skills: steering vectors, SAEs, maybe fine-tuning<\/li><\/ul><\/li><li>Pick some prompts from <a href=\"https://arxiv.org/abs/2503.08679\">Chain-of-Thought Reasoning In The Wild Is Not Always Faithful<\/a>&nbsp;and try to gain a deeper understanding of what’s happening<ul><li>Skills: Open ended exploration, using whichever tools seem appropriate<\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Those cover two kinds of starter projects:<\/p><ul><li><strong>Understanding-heavy<\/strong>, where you take a well-known domain and try to test a hypothesis there (e.g. extending a paper you’ve read closely)<ul><li>Note that you still want to do <i>some<\/i><\/li><\/ul><\/li><li><strong>Exploration-heavy<\/strong>, where you take some phenomena (a technique, a model, a phenomena, etc) play around with it, and try to understand what’s going on.<ul><li>Exploration-heavy projects are often a less familiar style, so make sure to do some of those!<\/li><\/ul><\/li><\/ul><p>Common mistakes:<\/p><ul><li>People often get hung up on finding the “best” project. Sadly, that’s not going to happen. Instead, just do something and see what happens - better ideas and inspiration come with time.<\/li><li>Don't get too attached to your first project. It was probably badly chosen! These are throwaway projects, just move on once you’re not learning as much.<\/li><li>Conversely, don't flit between ideas so much that you never build your \"getting unstuck\" toolkit.<\/li><li>Avoid compute-heavy and/papers (e.g., training cross-layer transcoders) or highly technical papers (e.g., Sparse Feature Circuits).<\/li><\/ul><h3 data-internal-id=\"Practicing_Exploration\">Practicing Exploration<\/h3><p>The idea of exploration as a phase in itself often trips up people new to mech interp. They feel like they always need to have a plan, a clear thing they're doing at any given point, etc. In my experience, you will often spend more than half of a project trying to figure out what the hell is happening and what you think your plan is. This is totally fine!<\/p><p data-internal-id=\"ftnt_ref14\">You don't need a plan. It's okay to be confused. However, this does <i>not <\/i>mean you should just screw around. Your North Star: gain information and surface area<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-noteref\" id=\"fnrefxw1ra5pqnd\"><sup><a href=\"#fnxw1ra5pqnd\">[14]<\/a><\/sup><\/span>&nbsp;on the problem. Your job is to take actions that maximise information gained per unit time. If you've learned nothing in 2 hours, pivot to another approach. If 2-3 approaches were dead ends, it’s fine to just pick another problem.<\/p><p>I have <a href=\"https://www.youtube.com/watch?v=LP_NTmMvp10&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T\">several research walkthroughs on my YouTube channel<\/a>&nbsp;that I think demonstrates the mindset of exploration. What I think is an appropriate speed to be moving. E.g. I think you should aim to make a new plot every few minutes (or faster!) if experiments don't take too long to run.<\/p><p>A common difficulty is feeling “stuck” and not knowing what to do. IMO, this is largely a skill issue. Here's my recommended toolkit when this happens:<\/p><ul><li>Use \"gain surface area\" techniques, things that can surface new ideas and connections and just give you raw data to work with: look at the model's output/chain-of-thought, change the prompt, probe for a concept, look at an SAE/attribution graph, read examples from your dataset, try logit lens or steering, etc.<\/li><li>Set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5-minute timer<\/a>&nbsp;and brainstorm things you're curious about or directions to try.<\/li><li>If you’re confused/curious about something, set a <a href=\"https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers\">5 minute timer<\/a>&nbsp;and brainstorm what could be happening.<\/li><\/ul><p>Other advice:<\/p><ul><li>Before any &gt;30 minute experiment, stop and brainstorm alternatives. Is this <i>really<\/i>&nbsp;the fastest way to gain information?<\/li><li>It's totally fine to pause for half a day to go learn some key background knowledge.<\/li><li>Get in the habit of keeping a research log of your findings and a \"highlights\" doc for the really cool stuff.<ul><li>If applicable, it can be cool to have your research log be a slack/discord channel<\/li><\/ul><\/li><li>Remember: when exploring and thinking through how to explain mysterious phenomena, most of your probability mass should be on \"something I haven't thought of yet.\"<\/li><li>Practice following your curiosity, but be aware that it’ll often lead you astray at first. When it does, pay attention! What can you learn from this?<\/li><\/ul><h3 data-internal-id=\"Practicing_Understanding\">Practicing Understanding<\/h3><p>If exploration goes well, you'll start to form hunches about the problem. E.g. thinking that you are successfully (linearly) probing for some concept. Or that you found a direction that mediates refusal. Or that days of the week are represented as a circle in a 2D subspace.<\/p><p>Once you have this, you want to go to figure out if it's actually true. Be warned, the feeling of “being really convinced that it's true” is very different from actually being true. Part of being a good researcher is being good enough at testing and falsifying your pet hypotheses that, when you fail to falsify one, there’s a good chance that it's true. But you're probably not there yet.<\/p><p>Note: While I find it helpful to think of these as discrete stages, often you'll be flitting back and forth. A great way to explore is coming up with guesses and micro-hypotheses about what's going on, running a quick experiment to test them, and integrating the results into your understanding of the problem, going back to the drawing board.<\/p><p>Your North Star: convince yourself a hypothesis is true or false. The key mindset is skepticism. Advice:<\/p><ul><li>Before testing a hypothesis, set a five-minute timer and brainstorm, \"What are the ways this could be false?\"<\/li><li>Alternatively, write out the best possible case for your hypothesis and see where the argument feels weak.<ul><li>Try using an LLM with an anti-sycophancy prompt (\"My friend wrote this and wants brutal feedback...\") to red-team your arguments - it probably won’t work, but might be helpful<\/li><\/ul><\/li><li>Or set a 5 minute timer and brainstorm alternative explanations for your observations<\/li><\/ul><p>You then want to convert these flaws and alternative hypotheses into concrete experiments. <strong>Experiment design is a deep skill<\/strong>. Honestly, I'm not sure how to teach it other than through experience. But one recommendation is to pay close attention to the experiments in papers you admire and analyze what made them so clever and effective. I also recommend that, every time you feel like you’ve (approximately) proven or falsified a hypothesis, adding them to a running doc of “things I believe to be true” with hypotheses, experiments, and results.<\/p><h3 data-internal-id=\"Using_LLMs_for_Research_Code\">Using LLMs for Research Code<\/h3><p>In my opinion, coding is one of the domains where LLMs are most obviously useful. It was very striking to me how much better my math scholars were six months ago than 12 months ago, and I think a good chunk of this is attributable by them having much better LLMs to use. If you are not using LLMs as a core part of your coding workflow, I think you're making a mistake.<\/p><ul><li><strong>Use<\/strong><a href=\"http://cursor.com/\"><strong>&nbsp;Cursor<\/strong><\/a><strong>:<\/strong>&nbsp;It's VS Code with fantastic AI integration. Make sure to add the docs for libraries with @&nbsp;so the AI has context. The $20/month plan is worth it, if possible, and there’s a <a href=\"https://cursor.com/students\">free student version<\/a>.<ul><li>Claude Code is tempting but bad for learning and iteration. I’d use it for throwaway things and first drafts - if the draft has a bunch of bugs, go read the code yourself/throw it away and start again. Cursor facilitates reading the AI’s code better than Claude code does IMO<\/li><\/ul><\/li><li><strong>A caveat:<\/strong>&nbsp;If learning a new library (like in ARENA), first try writing things yourself. Use the LLM when stuck, not to replace the learning process.<\/li><li>Later on, when thinking about writing up results, if key experiments were mostly vibe-coded, I recommend re-implementing them by hand to make sure no dumb LLM bugs slipped in.<\/li><\/ul><h2 data-internal-id=\"Interlude__What_s_New_In_Mechanistic_Interpretability_\">Interlude: What’s New In Mechanistic Interpretability?<\/h2><p><i>Feel free to skip to the “<\/i><a href=\"#Stage_3__Working_Up_To_Full_Research_Projects\"><i>what should I do next<\/i><\/a><i>” part<\/i><\/p><p>Things move fast in mechanistic interpretability. Newcomers to the field who've kept up from afar are often pretty out of date. Here's what I think you need to know, again, filtered through my own opinions and biases.<\/p><h3 data-internal-id=\"Avoiding_Fads\">Avoiding Fads<\/h3><p>This interlude is particularly important because <strong>the field often has fads<\/strong>: lines of research that are very popular for a year or so, make some progress and find many limitations, and then the field moves on. But if you’re new, and catching up on the literature, you might not realise. I often see people new to the field working on older things, that I don’t think are too productive to work on any more. Historical fads include:<\/p><ul><li>Interpreting toy models trained on algorithmic tasks (e.g. my <a href=\"https://arxiv.org/abs/2301.05217\">grokking work<\/a>)<ul><li>I no longer recommend working on this, as I think we basically know that “sometimes models trained on algorithmic tasks are interpretable”, and they’re sufficiently artificial and divorced from real models that I am pessimistic about deeper and more specific insights generalising<\/li><\/ul><\/li><li>Circuit analysis via causal interventions on model components (e.g. the <a href=\"https://arxiv.org/abs/2211.00593\">IOI paper<\/a>)<ul><li>This is slightly more complicated. I think that's worth learning about, and techniques like activation and attribution patching are genuinely useful.<\/li><li>But the core problem is that once you got a sparse subgraph of a model responsible for a task, there wasn't really a “what next?”. This didn't tend to result in deeper insight because the nodes (eg layers or maybe attention heads) weren't monosemantic, and it was often more complicated than naive stories suggested but we didn’t have the tools to dig deeper.<\/li><li>It was pretty cool to see that this was possible at all, but there have been more than enough works in this area that the bar for a novel contribution is now much higher.<\/li><li>Simply identifying a circuit is no longer enough; you need to use that circuit to reveal a deeper, non-obvious property of the model. I recommend exploring <a href=\"https://www.neuronpedia.org/graph/info\">attribution-graph style approaches<\/a><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">We're at the tail end of a fad of incremental <a href=\"https://transformer-circuits.pub/2023/monosemantic-features\">sparse autoencoder research<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-noteref\" id=\"fnreftq4gws0zq69\"><sup><a href=\"#fntq4gws0zq69\">[15]<\/a><\/sup><\/span>&nbsp;(i.e. focusing on simple uses and refinements of the basic technique)<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Calling this one a fad is probably more controversial (if only because it's more recent).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">The <i>specific<\/i>&nbsp;thing I am critiquing is the spate of papers, including ones I was involved in, that are about incremental improvements to the sparse autoencoder architecture, or initial demonstrations that you can apply SAEs to do things, or picking some downstream task and seeing what SAEs do on it.<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I think this made some sense when it seemed like SAEs could be a total gamechanger for the field, and where we were learning things from each new such paper. I think this moment has passed; I do not think they were a gamechanger in the way that I hoped they might be. See <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">more of my thoughts here<\/a>.<\/p><\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref15\">I am <i>not<\/i>&nbsp;discouraging work on the following:<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">Attribution graph-based circuit analysis, which I don't think has played out yet - see <a href=\"https://www.neuronpedia.org/graph/info\">a recent overview of that sub-field I co-wrote<\/a>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Trying meaningfully different approaches to dictionary learning (eg <a href=\"https://arxiv.org/abs/2506.20790\">SPD<\/a>&nbsp;or <a href=\"https://arxiv.org/abs/2505.17769\">ITDA<\/a>), or things targeted to fix conceptual limitations of current techniques (eg <a href=\"https://arxiv.org/abs/2503.17547\">Matryoshka<\/a>).<\/p><\/li><li><p data-internal-id=\"ftnt_ref15\">Using SAEs as a tool, whether as part of a broader project investigating weird phenomena in model biology, or as a baseline/approach on some downstream task. The key is that the project’s motivation should <i>not <\/i>just be “what if we used SAEs for X?” unless there’s a good argument<\/p><ul><li><p data-internal-id=\"ftnt_ref15\">I particularly recommend them for tasks where you don’t know exactly what you’re looking for, e.g. trying to explore some mysterious phenomena<\/p><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><\/ul><p>Note that I am putting this after stage 2 because I think that <strong>for initial throwaway projects you should <\/strong><i><strong>not<\/strong><\/i><strong>&nbsp;be stressing about novelty and avoiding fads<\/strong>&nbsp;- your goal is just to learn. But as we move into stage 3 you should start to be a bit more mindful about choosing more exciting/impactful projects where possible.<\/p><p>Also, take these as nudges and recommendations, not as instructions. If there's a direction you believe in that fits the things I'm critiquing, maybe I'm just wrong, maybe your thing is an exception, go wild, see what happens.<\/p><h3 data-internal-id=\"What_s_New_In_Mech_Interp_\">What’s New In Mech Interp?<\/h3><p>OK, so those are my hot takes on what not to do. What should you do? I think that some really cool new opportunities have opened up in mech interp over the last year, and newcomers may not have come across these. Here are some of the key themes in my favorite papers over the last year, that I’d love to see readers build on:<\/p><ol><li><strong>Downstream tasks<\/strong>. If our North Star is to be able to do useful things with interpretability, a great way to test our techniques is on real-world tasks, to see if they beat baselines or enable us to do a new thing<ol><li>Even if we can't directly test on the tasks we most care about, if models can let us do new things in <i>some<\/i>&nbsp;domains, that's reasonable evidence that we're learning something real in a way that is hard to fake.<\/li><li>One of my favourite examples is <strong>auditing games<\/strong>: <a href=\"https://arxiv.org/abs/2503.10965\">Marks et al<\/a>. is a fantastic paper that trained a model to have a hidden goal and then had different teams compete to find this goal, some with interpretability techniques, some without. Hypothesis: If interpretability does something real, then interp teams should win.<ol><li>This study itself was a bit ambiguous (the mech interp teams were 1st and 3rd, a black box team was 2nd), but I think this is a fantastic way to test interpretability techniques, and has a ton of potential for follow-on work evaluating interpretability in different kinds of games.<\/li><li>The general idea is just that there is a model with some known hidden property and different techniques are used to see which can elicit it. Things like having different teams, allowed to use different techniques, etc. is just one implementation.<\/li><\/ol><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref16\"><strong>Model organisms<\/strong>: The auditing games paper was made possible by the fact that they were able to make a model with a hidden goal<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-noteref\" id=\"fnrefcdmsagzbqkp\"><sup><a href=\"#fncdmsagzbqkp\">[16]<\/a><\/sup><\/span>, a model organism to study. In general, we’re collecting techniques like <a href=\"https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/\">synthetic document fine-tuning<\/a>&nbsp;to make really interesting model organisms.<\/p><ol><li><p data-internal-id=\"ftnt_ref16\">This kind of thing has a lot of potential! If we want to make a lie detector, a core challenge is that we don’t know how to test if it works or not. But if we can insert beliefs or deceptive behaviours into a model, many more projects become possible<\/p><\/li><li><p data-internal-id=\"ftnt_ref16\">A great intro project is playing around with open source model organisms, e.g. from <a href=\"https://arxiv.org/abs/2505.14352\">Cywinski et al<\/a><\/p><\/li><\/ol><\/li><li><strong>Practice on the real AGI Safety problems<\/strong>: Historically, interpretability could only practice on very dull toy problems like <a href=\"https://arxiv.org/abs/2301.05217\">modular addition<\/a>. But we now have models that exhibit complex behaviors that seem genuinely relevant to safety concerns, and we can just study them directly, making it far easier to make real progress.<ol><li>E.g. <a href=\"https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the\">Rajamanoharan et al<\/a>&nbsp;debunking assumed self-preservation, and <a href=\"https://www.apolloresearch.ai/research/deception-probes\">Goldowsky-Dill et al<\/a>&nbsp;probing for deception<\/li><li>Weird behaviours: models can <a href=\"https://www.apolloresearch.ai/research/deception-probes\">insider trade then lie about it<\/a>, <a href=\"https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations\">tell when they’re being evaluated<\/a>&nbsp;(and act differently), <a href=\"https://arxiv.org/abs/2412.14093\">fake alignment<\/a>, <a href=\"https://metr.org/blog/2025-06-05-recent-reward-hacking/\">reward hack<\/a>, and more.<\/li><\/ol><\/li><li><strong>Real-World Uses of Interpretability<\/strong>: Model interpretability-based techniques are starting to have genuine uses in frontier language models!<ol><li><a href=\"https://arxiv.org/abs/1610.01644\">Linear probes<\/a>, one of the simplest possible techniques, are a highly competitive way to <a href=\"https://alignment.anthropic.com/2025/cheap-monitors/\">cheaply monitor systems<\/a>&nbsp;for things like users trying to make bioweapons.<\/li><li>I find it incredibly cool that interpretability can actually be useful, and kind of embarrassing that only a decade-old technique seems very helpful. Someone should do something about that. Maybe that someone could be you!<\/li><li>This needs a very different kind of research: careful evaluation, comparison to strong baselines, and refinement of methods<\/li><\/ol><\/li><li><strong>Attribution graph-based circuit analysis<\/strong>. The core problem with trying to analyze circuits in terms of things like a model's attention heads and layers is that often these things don't actually have a clear meaning. <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\">Attribution graphs<\/a>&nbsp;use techniques like <a href=\"https://arxiv.org/abs/2406.11944\">transcoders<\/a>, popularized in <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/biology.html\">Anthropic's model biology<\/a>&nbsp;work, to approximate models with a computational graph with meaningful nodes.<ol><li><p data-internal-id=\"ftnt_ref17\">See this <a href=\"https://www.neuronpedia.org/graph/info\">cross-org blog post<\/a>&nbsp;for the ongoing follow-on work across the community, and an open problems list I co-wrote!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-noteref\" id=\"fnrefp0f0m03b55r\"><sup><a href=\"#fnp0f0m03b55r\">[17]<\/a><\/sup><\/span><\/p><\/li><li>You can make and analyse your own attribution graphs on <a href=\"https://www.neuronpedia.org/gemma-2-2b/graph\">Neuronpedia<\/a><\/li><\/ol><\/li><li><strong>Understanding model failures<\/strong>: Models often do weird things. If we were any good at interpretability, we should be able to understand these. Recently, we’ve seen signs of life!<ol><li><a href=\"https://transluce.org/observability-interface\">Meng et al<\/a>&nbsp;on why some models think 9.8 &lt; 9.11<\/li><li><p data-internal-id=\"ftnt_ref18\">A line of work studying <a href=\"https://www.emergent-misalignment.com/\">emergent misalignment<\/a>&nbsp;- why training models on narrowly evil tasks like writing insecure code turns them into Nazis - has found some insights. <a href=\"https://arxiv.org/abs/2506.19823\">Wang et al<\/a>&nbsp;found this was driven by sparse autoencoder latents<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-noteref\" id=\"fnrefg12d8d1lqu\"><sup><a href=\"#fng12d8d1lqu\">[18]<\/a><\/sup><\/span>&nbsp;associated with movie villains, and in <a href=\"https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy\">Turner et al<\/a>&nbsp;we found that the model <i>could <\/i>have learned the narrow solution, but this was in some sense less “efficient” and “stable”<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref20\"><strong>Automated interpretability<\/strong>: Using LLMs to automate interpretability. We saw signs of life on this from Bills et al and <a href=\"https://arxiv.org/abs/2404.14394\">Shaham et al<\/a>, but LLMs are actually good now! It’s now possible to make basic interpretability agents that can do things like <a href=\"https://alignment.anthropic.com/2025/automated-auditing/\">solve auditing games<\/a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-noteref\" id=\"fnref0td6a2gxwht\"><sup><a href=\"#fn0td6a2gxwht\">[19]<\/a><\/sup><\/span>. And interpretability agents are the worst they’ll ever be<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-noteref\" id=\"fnref5bdglmkdzr\"><sup><a href=\"#fn5bdglmkdzr\">[20]<\/a><\/sup><\/span>.<\/p><\/li><li><p data-internal-id=\"ftnt_ref22\"><strong>Reasoning model interpretability<\/strong>: All current frontier models are reasoning models—models that are trained with reinforcement learning to think<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-noteref\" id=\"fnrefwuxdh4f7kh\"><sup><a href=\"#fnwuxdh4f7kh\">[21]<\/a><\/sup><\/span>&nbsp;for a while before producing an answer. In my opinion, this requires a major rethinking of many existing interpretability approaches<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-noteref\" id=\"fnref3qxoen8tddk\"><sup><a href=\"#fn3qxoen8tddk\">[22]<\/a><\/sup><\/span>, and calls for exploring new paradigms. IMO this is currently being neglected by the field, but will become a big deal.<\/p><ol><li><p data-internal-id=\"ftnt_ref22\">In <a href=\"http://thought-anchors.com\">Bogdan et al<\/a>, we explored what a possible paradigm could look like. Notably, there are far more interesting and sophisticated black box techniques with reasoning models, like resampling the second half of the chain of thought, or every time the model says a specific kind of sentence, deleting and regenerating that sentence.<\/p><\/li><\/ol><\/li><\/ol><h3 data-internal-id=\"A_Pragmatic_Vision_for_Mech_Interp\">A Pragmatic Vision for Mech Interp<\/h3><p>Attentive readers may notice that the list above focuses on work to do with understanding the more qualitative high-level properties of models, and not ambitious reverse engineering. This is largely because, in my opinion, the former has gone great, while we have not seen much progress towards the fundamental blockers on the latter.<\/p><p>I used to be very excited about ambitious reverse engineering, but I currently think that the dream of completely reverse engineering a model down to something human understandable seems basically doomed. My interpretation of the research so far is that models have some human understandable high-level structure that drives important actions, and a very long tail of increasingly niche and irrelevant heuristics and biases. For pragmatic purposes, these can be largely ignored, but not if we want things like guarantees, or to claim that we have understood most of a model. I think that trying to understand as much as we can is still a reasonable proxy for getting to the point of being pragmatically useful, but think it’s historically been too great a focus of the field, and many other approaches seem more promising if our ultimate goals are pragmatic.<\/p><p>In some ways, this has actually made me more optimistic about interpretability ultimately being useful for AGI safety! Ambitious reverse engineering would be awesome but was always a long shot. But I think we've seen some real results for pragmatic approaches to mechanistic interpretability, and feel fairly confident we are going to be able to do genuinely useful things that are hard to achieve with other methods.<\/p><h2 data-internal-id=\"Stage_3__Working_Up_To_Full_Research_Projects\">Stage 3: Working Up To Full Research Projects<\/h2><p>Once you have a few mini-projects done, you should start being more ambitious. You want to think about gaining the deeper (medium/slow) skills, and exploring ideation and distillation.<\/p><p>However, you should still expect projects to often fail, and want to lean into breadth over depth and avoid getting bogged down in an unsuccessful project you can’t bear to give up on. To resolve this tension, I recommend <strong>working in 1-2 week sprints<\/strong>. At the end of each sprint, reflect and make a deliberate decision: <strong>continue, or pivot?<\/strong>&nbsp;The default should be to pivot unless the project feels truly promising. It’s great to give up on things, if it means you spend your time even better! But if it’s going great, by all means continue.<\/p><p>This strategy should mean that you eventually end up working on something longer-term when you find something <i>good<\/i>, but don't just get bogged down in the first ambitious idea you tried.<\/p><p>I recommend reviewing the list of skills earlier and just for each one, reflecting for a bit on how on top of it you think you feel and how you could intentionally practice it in your next project. Then after each sprint, before deciding whether to pivot, take an hour or two to do a post-mortem: what did you learn, what progress did you make on different skills, and what would you do differently next time? Your goal is to learn, and you learn much better if you make time to actually process your accumulated data!<\/p><h3 data-internal-id=\"Key_Research_Mindsets\">Key Research Mindsets<\/h3><p>One way to decompose your learning is to think about research mindsets: the traits and mindsets a good researcher needs to have, that cut across many of these stages. See <a href=\"https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking\">my blog post on the topic for more<\/a>, but here's a brief view of how I'm currently thinking about it.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\"><strong>Skepticism/Truth-seeking:<\/strong>&nbsp;The default state of the world is that your research is false, because doing research is hard. Your north star should always be to find <i>true <\/i>insights<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-noteref\" id=\"fnreflm5ixkfuzk\"><sup><a href=\"#fnlm5ixkfuzk\">[23]<\/a><\/sup><\/span><\/p><ol><li><p data-internal-id=\"ftnt_ref23\">It generally doesn't come naturally to people to constantly aggressively think about all the ways their work could be false and make a good faith effort to test it. You can learn to do better than this, but it often takes practice.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">This is crucial in understanding, somewhat important in exploration, and crucial in distillation.<\/p><\/li><li><p data-internal-id=\"ftnt_ref23\">A common mistake is to grasp at straws to find a “positive” result, thinking that nothing else is worth sharing.<\/p><ol><li><p data-internal-id=\"ftnt_ref23\">In my opinion, negative or inconclusive results that are well-analyzed are much better than a poorly supported positive result. I’ll often think well of someone willing to release nuanced negative results, and poorly of someone who pretends their results are better than they are.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Prioritization:<\/strong>&nbsp;Your time is scarce. Research involves making a bunch of decisions that are essentially searching through a high-dimensional space. The difference between a great and a mediocre researcher is being able to make these decisions well.<ol><li>If you have a good mentor, you can lean on them for this at first, but you will need to learn how to do this yourself eventually.<\/li><li>This is absolutely crucial in exploration and ideation, but fairly important throughout.<\/li><li>A good way to learn this one is to reflect on decisions you've made after the fact, eg in a sprint post-mortem, and think about how you could have made them better, and what generalisable lessons to take to the future<\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\"><strong>Productivity<\/strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-noteref\" id=\"fnrefidab8074tka\"><sup><a href=\"#fnidab8074tka\">[24]<\/a><\/sup><\/span><strong>:<\/strong>&nbsp;The best researchers I've worked with get more than twice as much done as the merely good ones. Part of this is good research taste and making good prioritization decisions, but part of this is just being good at getting shit done.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Now, this doesn't necessarily mean pushing yourself until the point of burnout by working really long hours. Or cutting corners and being sloppy. This is about productivity integrated over the long term.<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">For example, sometimes the most productive thing to do is to hold off on starting work, set a 5 minute timer, brainstorm possible things to do next, and then pick the best idea<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">This takes many forms, and the highest priority for you:<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">Know when to write good code without bugs, to avoid wasting time debugging later, and when to write a hacky thing that just works.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know the right keyboard shortcuts to move fast when coding.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Know when to ask for help and have people who can help you get unblocked where appropriate.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be good at managing your time and tasks so that once you've decided what the highest priority thing to work on is, you in fact go and work on it.<\/p><\/li><li><p data-internal-id=\"ftnt_ref24\">Be able to make time to achieve deep focus on the key problems.<\/p><\/li><\/ol><\/li><li><p data-internal-id=\"ftnt_ref24\">Exercise: Occasionally <strong>audit your time<\/strong>. Use a tool like <a href=\"http://toggl.com\">Toggl<\/a>&nbsp;for a day or two to log what you're doing, then reflect: where did time go? What was inefficient? How could I do this 10% faster next time?<\/p><ol><li><p data-internal-id=\"ftnt_ref24\">The goal isn't to feel guilty, but to spot opportunities for improvement, like making a utility function for a tedious task.<\/p><\/li><\/ol><\/li><\/ol><\/li><li><strong>Knowing the literature<\/strong>: At this point, there’s a lot of accumulated wisdom (and a lot of BS) in prior papers, in mech interp and beyond.<ol><li>This cuts across all stages:<ol><li>In ideation, you don’t want to accidentally reinvent the wheel. And often great ideas are inspired by prior work<\/li><li>In exploration, you want to be able to spot connections, borrow interesting techniques, etc<\/li><li>In understanding, you want to know the right standards of proof to check for, the best techniques to use, alternative hypotheses (that may have been raised in other works), etc<\/li><li><p data-internal-id=\"ftnt_ref25\">In distillation, when writing a paper you’re expected to be able to contextualise it relative to existing work (i.e. write a related work section<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-noteref\" id=\"fnrefwpekmwudkpd\"><sup><a href=\"#fnwpekmwudkpd\">[25]<\/a><\/sup><\/span>) which is important for other researchers knowing whether to care. And if you don’t know the standard methods of proof, key baselines everyone will ask about, key gotchas to check for etc, no one will believe your work.<\/p><\/li><\/ol><\/li><li>LLMs are an incredibly useful tool here. GPT-5 thinking or Claude 4 with web search are both pretty useful tools here, as are the slower but more comprehensive deep research tools (Note that Google's is available for free, as of the time of writing)<ol><li>I recommend using these regularly and creatively throughout a project.<\/li><li>You don't necessarily need to go and read the works that get surfaced, but even just having LLM summaries can get you more awareness of what's out there, and over time you'll build this into deeper knowledge.<\/li><\/ol><\/li><li>Of course, when there <i>does<\/i>&nbsp;seem to be a very relevant paper to your work, you should go do a deep dive and read it properly, not just relying on LLM summaries.<\/li><li>Don’t stress - deep knowledge of the literature takes time to build. But you want to ensure you’re on an upwards gradient here, rather than assuming the broader literature is useless<\/li><li><p data-internal-id=\"ftnt_ref26\">On the flip side, many papers <i>are <\/i>highly misleading/outright false, so please don’t just critically believe them!<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-noteref\" id=\"fnref1bau7vsh9tk\"><sup><a href=\"#fn1bau7vsh9tk\">[26]<\/a><\/sup><\/span><\/p><\/li><\/ol><\/li><\/ol><p>Okay, so how does this all tie back to the stages of research? Now you're going to be thinking about all four. We'll start by talking about how to deepen your existing skills with exploration and understanding, and then we'll talk about what practicing ideation and actually writing up your work should look like.<\/p><h3 data-internal-id=\"Deepening_Your_Skills\">Deepening Your Skills<\/h3><p>You’ll still be exploring and understanding, but with a greater focus on rigor and the slower skills. In addition to the thoughts when discussing mindsets above, here’s some more specific advice<\/p><ul><li><strong>Deeper Exploration<\/strong>&nbsp;is about internalizing the mindset of maximising productivity, which here means maximising information gain per unit time. Always ask, \"Am I learning something?\"<ul><li><i>Avoid Rabbit Holes:<\/i>&nbsp;A common mistake is finding one random anomaly and zooming in on it. Knowing when to pivot is crucial. Set a timer every hour or two to zoom out and ask if you’re making progress.<ul><li>I recommend any time you notice yourself feeling a bit stuck or distracted or off track, setting a five minute timer and thinking about what could I be doing next, what should I be doing next, and am I doing the most important thing?<\/li><\/ul><\/li><li><i>Avoid Spreading Yourself Too Thin:<\/i>&nbsp;Doing lots of things superficially means none of them will be interesting.<\/li><li>If you have spent more than five hours without learning something new, you should probably try a different approach<ul><li>And if you have spent more than two days without learning something new, you should seriously consider pivoting and doing something else.<\/li><\/ul><\/li><li>To practice prioritization, be intentional about your decisions: write down <i>why<\/i>&nbsp;you think an experiment is the right call, and later reflect on whether you were right. This makes your intuitions explicit and easier to update.<\/li><\/ul><\/li><li><strong>Deeper Understanding<\/strong>&nbsp;is about practicing skepticism and building a bulletproof case. Red-team your results relentlessly.<ul><li>Some experiments are much more impactful and informative than others! Don't just do the first experiment that pops into your head. Think about the key ways the hypothesis <i>could <\/i>be false, and how you could test that. Or about whether a skeptic could explain away a positive experimental results<ul><li>A useful exercise is imagining you're talking to a really obnoxious skeptic who keeps complaining that they don't believe you and coming up with arguments for why your thing is wrong. What could you do such that they don't have a leg to stand on?<\/li><\/ul><\/li><li>Of course, there's also an element of prioritization. Sometimes a shallow case that could be wrong is the right thing to aim for, if you’re working on an unimportant side claim/something that seems super plausible on priors, at which point you should just move on and do something else more interesting.<\/li><li>Exercise: To practice spotting subtle illusions, try red-teaming papers you read, thinking about potential flaws, and ideally run the experiments yourself.<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Doing_Good_Science\">Doing Good Science<\/p><ul><li><strong>Avoid cherry-picking<\/strong>: Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling.<ul><li>When you write up work, always include some randomly selected examples, especially if you present extensive qualitative analysis of specific things. It's fine to put this in the appendix if space is scarce, but it should be there.<\/li><\/ul><\/li><li><strong>Use baselines<\/strong>: A common mistake is for people to try to show a technique works by demonstrating it gets 'decent' results, rather than showing it achieves better results than plausible alternatives that people might have used or are standard in the field. If you want people to e.g. use your cool steering vector results you need to show it beats changing the system prompt.<\/li><li><strong>Don’t sandbag your baselines<\/strong>: Similarly, it's easy to put in much more effort finding good hyperparameters for your technique than for your baselines. Try to make sure you're achieving comparable results with your baselines that prior work in the field has.<\/li><li><strong>Do ablations on your fancy method<\/strong>: It's easy for people to have a fancy method with lots of moving parts, when many actually are unnecessary. You should always try removing one part and see if the method breaks. Do this for each part.<ul><li>For example, the <a href=\"https://arxiv.org/abs/2403.03218v1\">original unlearning method<\/a>&nbsp;in the <a href=\"https://arxiv.org/abs/2403.03218\">RMU paper<\/a>&nbsp;claimed it was based on finding a meaningful steering vector, until follow-up work found that it was just about adding a vector with really high norm that broke the model, and a random vector performed just as well.<\/li><\/ul><\/li><li><strong>(Informally) pre-register claims<\/strong>: It's important to clearly track which experimental results were obtained before versus after you formulated your claim. Post-hoc analysis (interpreting results after they're seen) is inherently less impressive than predictions confirmed by pre-specified experiments<\/li><li><strong>Be reproducible<\/strong>: Where practical, share your code, data and models.<ul><li>If you have time, make sure that it runs on a fresh machine and include a helpful readme that links to key model weights and datasets.<\/li><li><p data-internal-id=\"ftnt_ref27\">This both means others can check if your work is true and makes it more likely people will believe and build on your work<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-noteref\" id=\"fnrefadytzr5d7y\"><sup><a href=\"#fnadytzr5d7y\">[27]<\/a><\/sup><\/span>&nbsp;because they can see replications that are more likely to exist and because it's now low friction.<\/p><\/li><\/ul><\/li><li><strong>Simplicity:<\/strong>&nbsp;Bias towards trying the simple, obvious methods first. Fancy techniques can be a trap. Good research is pragmatic, not about showing off.<ul><li>If you’re designing a fancy technique/experiment, each new detail is one more thing that can break<\/li><li>If trying to explain something mysterious, novice researchers often neglect simple, dumb hypotheses like “maybe MLP0 is incredibly important on <i>every <\/i>input, and there’s nothing special going on with my prompt”<\/li><\/ul><\/li><li><strong>Be qualitative <\/strong><i><strong>and <\/strong><\/i><strong>quantitative<\/strong>: One of the major drivers of progress of modern machine learning is being quantitative, having benchmarks and showing that a technique increases numbers on them. One of the key drivers of progress in mech interp is an openness to qualitative research: summary statistics lose a ton of information. What can we learn by actually looking deeply into what's happening?<ul><li>In my opinion, the best research tries to get the best of both worlds. It tries to understand what's happening via qualitative analysis and then validates it with more quantitative methods. If your paper only does one, it’s probably missing out<\/li><\/ul><\/li><li><strong>Read your data<\/strong>: A fantastic use of time, especially during the exploration phase, is just actually reading the data you're working with, or model chains of thought and responses.<ul><li>Often, the quality of the data is a crucial driver of the results of your experiments. Often, it is quite bad.<\/li><li>Sometimes most of the work of a project is in noticing flaws in your data and making a better data set. Time figuring this out is extremely well spent.<\/li><li>Ditto, include random examples of the data in an appendix for readers to do spot checks of their own.<\/li><\/ul><\/li><li><p data-internal-id=\"ftnt_ref28\"><strong>Don’t reinvent the wheel<\/strong>: &nbsp;A common mistake in mech interp is doing something that's already been done<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-noteref\" id=\"fnrefva7mhfkrhm\"><sup><a href=\"#fnva7mhfkrhm\">[28]<\/a><\/sup><\/span>. We have LLM-powered literature reviews now. You have way less of an excuse. Check first!<\/p><\/li><li><strong>Excitement is evidence of bullshit<\/strong>: Generally, most true results are not exciting, but a fair amount of false results are. So from a Bayesian perspective, if a result is exciting and cool, it’s even more likely to be false than normal!<ul><li>Resist the impulse to get really excited! The correct attitude to exciting results is deep skepticism until you have tried really hard to falsify it and run out of ideas.<\/li><\/ul><\/li><\/ul><h3 data-internal-id=\"Practicing_Ideation\">Practicing Ideation<\/h3><p>Okay, so you want to actually come up with good research ideas to work on. What does this look like? I recommend breaking this down into <strong>generating ideas<\/strong>&nbsp;and then <strong>evaluating <\/strong>them to find the best ones.<\/p><p>To generate ideas, I'd often start with just taking a blank doc, blocking out at least an hour, and then just writing down as many ideas as you can come up with. Aim for quantity over quality. Go for at least 20.<\/p><p>There are other things you can do to help with generation:<\/p><ul><li>Throughout your previous sprints, every time you had an idle curiosity or noticed something weird, write it down in one massive long-running doc.<\/li><li>Likewise, when reading papers, note down confusions, curiosities, obviousnesses to do.<\/li><\/ul><p>Okay, so now you have a big list. What does finding the best ones look like?<\/p><ul><li>Ideally, if you have a mentor or at least collaborators, you can just ask them to rate them.<ul><li>If you do this, rate them yourself privately out of 10 before you look at their responses. Compare them and every time you have substantially different numbers, talk to the mentor and try to figure out why your intuitions disagree. This is a great source of supervised data for research taste.<\/li><\/ul><\/li><li>Even if you don’t have a mentor, I think that just going through, rating each idea yourself based on gut feel and sorting is as good a way to prune down a long list as any<\/li><li>For the top few, I recommend trying to answer a few questions about them.<ul><li>What would success look like here?<\/li><li>How surprised would I be if I did this for a month and nothing interesting had happened?<\/li><li>What skills does this require? Do I have them/could I easily gain them?<\/li><li>What models, data, computational resources, etc. does this require?<\/li><li>How does this compare to what the most relevant prior work did? Can I check for prior work and see if anything relevant comes up?<\/li><\/ul><\/li><\/ul><p data-internal-id=\"Research_Taste_Exercises\">Research Taste Exercises<\/p><p>Gaining research taste is slow because the feedback loops are long. You can accelerate it with exercises that give you faster, proxy feedback. (Credit to <a href=\"https://colah.github.io/notes/taste/\">Chris Olah for inspiration here<\/a>)<\/p><ul><li>If you have a mentor, query their taste for fast data and try to imitate it. Concretely:<ul><li>Before each meeting, write a list of questions, then try to write up predictions for what the mentor will say, then actually ask the mentor, see what happens, and compare. If there are discrepancies, chat to the mentor and try to understand why.<\/li><li>Likewise, if the mentor makes a suggestion or asks a question you didn't expect, try to ask questions about where the thought came from.<\/li><li><p data-internal-id=\"ftnt_ref29\">Regularly paraphrase back to the mentor in your own words what you think they're saying, and then ask them to correct anything you're wrong about<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-noteref\" id=\"fnreftt0owz8koks\"><sup><a href=\"#fntt0owz8koks\">[29]<\/a><\/sup><\/span><\/p><\/li><\/ul><\/li><li><strong>Learning from papers as \"offline data\":<\/strong>&nbsp;When you read a paper, don't just passively consume it. Read the introduction, then stop. Try to predict what methods they used and what their key results will be. Then, continue reading and see how your predictions compare. Analyze why the authors made different choices. This trains your intuition on a much larger and faster dataset than your own research.<\/li><\/ul><p>It’s also worth dwelling on what research taste actually is. <a href=\"https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research\">See my post<\/a>&nbsp;for more, but I break it down as follows:<\/p><ol><li><strong>Intuition (System 1):<\/strong>&nbsp;This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.<\/li><li><strong>Conceptual Framework (System 2)<\/strong>: This is deep domain knowledge and understanding of underlying principles.<\/li><li><strong>Strategic Big Picture<\/strong>: Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?<\/li><\/ol><h3 data-internal-id=\"Write_up_your_work_\">Write up your work!<\/h3><p>At this stage, you should be thinking seriously about how to write up your work. Often, writing up work is the first time you really understand what a project has been about, or you identify key limitations, or experiments you forgot to do. You should check out <a href=\"https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers\">my blog post on writing ML papers<\/a>&nbsp;for much more detailed thoughts (which also apply to high-effort blog posts!) but I'll try to summarize them below.<\/p><p data-internal-id=\"Why_aim_for_public_output_\">Why aim for public output?<\/p><p data-internal-id=\"ftnt_ref30\">If producing something public is intimidating, for now, you can start by just writing up a private Google Doc and maybe share it with some friends or collaborators. But I heavily encourage people to aim for public output where they can. Generally, your research will not matter if no one reads it. The goal of research is to contribute to the sum of human<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-noteref\" id=\"fnrefe3252d8idmr\"><sup><a href=\"#fne3252d8idmr\">[30]<\/a><\/sup><\/span>&nbsp;knowledge. And if no one understands what you did, then it doesn't matter.<\/p><p>Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, <strong>public research output is your best credential<\/strong>. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc. It doesn’t matter if you wrote it in a prestigious PhD program or as a random independent researcher, if it’s good enough then people care.<\/p><p>There are a few options for what this can look like:<\/p><ul><li>A blog post (e.g. on a personal blog or LessWrong) - the simplest and least formal kind<\/li><li><p data-internal-id=\"ftnt_ref31\">An Arxiv paper - much more legible than a blog post, and honestly not much extra effort if you have a high-quality blog post<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-noteref\" id=\"fnref8354hd0flji\"><sup><a href=\"#fn8354hd0flji\">[31]<\/a><\/sup><\/span><\/p><\/li><li><p data-internal-id=\"ftnt_ref32\">A workshop paper<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-noteref\" id=\"fnref9oppcf0ftrh\"><sup><a href=\"#fn9oppcf0ftrh\">[32]<\/a><\/sup><\/span>&nbsp;(i.e. something you submit for peer review to a workshop, typically part of a major ML conference, the bar is much lower than for a conference paper)<\/p><\/li><li><p data-internal-id=\"ftnt_ref34\">A conference paper (the equivalent of top journals in ML, there’s a reasonably high quality bar<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-noteref\" id=\"fnreffmh579omuc6\"><sup><a href=\"#fnfmh579omuc6\">[33]<\/a><\/sup><\/span>, but also a <i>lot <\/i>of noise<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-noteref\" id=\"fnreff09vsa4w37e\"><sup><a href=\"#fnf09vsa4w37e\">[34]<\/a><\/sup><\/span>)<\/p><\/li><\/ul><p>If this all seems overwhelming, starting out with blog posts is fine, but I think people generally overestimate the bar for arxiv or workshop papers - if you think you learned something cool in a project, this is totally worth turning into a paper!<\/p><p data-internal-id=\"How_to_write_stuff_up_\">How to write stuff up?<\/p><p>The core of a paper is the narrative. Readers will not take away more than a few sentences worth of content. Your job is to make sure these are the right handful of sentences and make sure the reader is convinced of them.<\/p><p>You want to distill your paper down into one to three key claims (your contribution), the evidence you provide that the contribution is true, the motivation for why a reader should care about them, and work all of this into a coherent narrative.<\/p><p><strong>Iterate<\/strong>: I'm a big fan of writing things iteratively. You first figure out the contribution and narrative. You then write a condensed summary, the abstract (in a blog post, this should be a TL;DR/executive summary - also very important!). You then write a bullet point outline of the paper: what points you want to cover, what evidence you want to provide, how you intend to build up to that evidence, how you want to structure and order things, etc. If you have mentors or collaborators, the bullet point outline is often the best time to get feedback. Or the narrative formation stage, if you have an engaged mentor. Then write the introduction, and make sure you’re happy with that. Then (or even before the intro) make the figures - figures are incredibly important! Then flesh it out into prose. People spend a <i>lot<\/i>&nbsp;more time reading the abstract and the intro than the main body, especially when you account for all the people who read the abstract and then stop. So you should spend a lot more time per unit word on those.<\/p><p><strong>LLMs<\/strong>: I think LLMs are a really helpful writing tool. They're super useful for getting feedback, especially if writing in an unfamiliar style like an academic ML paper may be for you. Remember to use anti-sycophanty prompts so you get real feedback. However, it's often quite easy to tell when you're reading LLM written slop. So use them as a tool, but don't just have them write the damn thing for you. But if you e.g. have writer’s block, having an LLM help you brainstorm or produce a first draft for inspiration, and can be very helpful.<\/p><p data-internal-id=\"Common_mistakes\">Common mistakes<\/p><ul><li><strong>The reader does not have context<\/strong>: Your paper will be clear in your head, because you have just spent weeks to months steeped in this research project. The reader has not. You will overestimate how clear things are to the reader, and so you should be massively erring in the other direction and spelling everything out as blatantly as possible.<ul><li><strong>This is an incredibly common mistake<\/strong>&nbsp;- assume it will happen to you<\/li><li>The main solution is to get feedback from people with enough research context that they can actually engage and who are also willing to give you substantial negative feedback.<ul><li>Notice the feeling of surprise when people are confused by something you thought was clear. Try to understand why they were confused and iterate on fixing it until it's clear.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Writing is not an afterthought<\/strong>: People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute.<\/li><li><strong>Acknowledge limitations<\/strong>: There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing<\/li><li><strong>Good writing is simple<\/strong>: There's a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they feel impressive. I think this is a highly ineffective strategy<\/li><li><strong>Remember to motivate things<\/strong>: It will typically not be obvious to the reader why your paper matters or is interesting. They do not have the context you do. It is your job to convince them, ideally in the abstract or perhaps intro, why they should care about your work, lest they just give up and stop reading.<\/li><\/ul><p data-internal-id=\"h.sk0e3iwce7ck\">&nbsp;<\/p><h2 data-internal-id=\"Mentorship__Collaboration_and_Sharing_Your_Work\">Mentorship, Collaboration and Sharing Your Work<\/h2><p>A common theme in the above is that it's incredibly useful to have a mentor, or at least collaborators. Here I'll try to unpack that and give advice about how to go about finding one.<\/p><p>Though it's also worth saying that many mentors are not actually great researchers and may have bad research taste or research taste that's not very well suited to mech interp. What you do about this is kind of up to you.<\/p><h3 data-internal-id=\"So_what_does_a_research_mentor_actually_do_\">So what does a research mentor actually do?<\/h3><p>A good mentor is an incredible accelerator. Dysfunctional as academia is, there is a reason it works under the apprenticeship-like system of PhD students and supervisors. When I started supervising, I was very surprised at how much of a difference a weekly check in could make! Here’s my best attempt to breakdown how a good mentor can add value:<\/p><ul><li><strong>Suggest research ideas<\/strong>&nbsp;when you're starting out, letting you bypass the hardest skill (ideation) to focus on execution.<\/li><li><strong>Help you prioritize<\/strong>&nbsp;which experiments to run, lending you their more experienced judgment, so you get more done.<\/li><li><p data-internal-id=\"ftnt_ref35\"><strong>When to pivot<\/strong>: if your research direction isn’t working out, having a mentor to pressure you to pivot can be extremely valuable<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-noteref\" id=\"fnref7ruxx269r2s\"><sup><a href=\"#fn7ruxx269r2s\">[35]<\/a><\/sup><\/span><\/p><\/li><li><strong>Provide supervised data for research taste<\/strong>: For the slow/very-slow skills like coming up with research ideas, and prioritization, a <i>far <\/i>faster way to gain them at first is by learning to mimic your mentor’s.<\/li><li><strong>Act as an interface to the literature<\/strong>: pointing you to the relevant work before you've built up deep knowledge yourself. Flagging standard baselines, standard metrics, relevant techniques, prior work so you don’t reinvent the wheel, etc.<\/li><li><strong>Red-team your results<\/strong>, helping you spot subtle interpretability illusions and flaws in your reasoning that you're too close to see.<\/li><li><strong>Point out skills you're missing<\/strong>&nbsp;that you didn't even notice were skills. Generally guide your learning and help you prioritize<\/li><li><strong>Walk you through communicating your work<\/strong>, helping you distill your findings and present them clearly to the world.<\/li><li><strong>Motivation/accountability<\/strong>: Many find it extremely helpful to have someone, even if very hands-off, who they present work to, so they feel motivated and accountable (especially if they e.g. want to impress the mentor, want a job, etc. Of course, these also increase stress!)<ul><li>To those prone to analysis paralysis, being able to defer to a mentor on uncertain decisions can be highly valuable<\/li><\/ul><\/li><li><strong>References<\/strong>: Having a mentor who can vouch for your skill is very helpful, especially if they know people who may be hiring you in future.<\/li><\/ul><h3 data-internal-id=\"Advice_on_finding_a_mentor\">Advice on finding a mentor<\/h3><p>Here are some suggested ways to get some mentorship while transitioning into the field. I discuss higher commitment ways, like doing a PhD or getting a research job, below.<\/p><p>Note: whatever you do to find a mentor, having evidence that you can do research yourself, that is, public output that demonstrates ability to self-motivate and put in effort, and ideally demonstrates actually interesting research findings, is incredibly helpful and should be a priority.<\/p><p data-internal-id=\"Mentoring_programs\">Mentoring programs<\/p><p>I think mentoring programs like <a href=\"http://matsprogram.org\">MATS<\/a>&nbsp;are an incredibly useful way into the field, you typically do a full-time, several month program where you write a paper, with weekly check-ins with a more experienced researcher. Your experience will vary wildly depending on mentor quality, but at least for my MATS scholars, often people totally new to mech interp can publish a top conference paper in a few months. See my MATS application doc for a bunch more details.<\/p><p>There’s <strong>a wide range of backgrounds<\/strong>&nbsp;among people who do them and get value - people totally new to a field, people with 1+ years of interpretability research experience who want to work with a more experienced mentor, young undergrads, mid-career professionals (including a handful of professors), and more.<\/p><p><a href=\"http://matsprogram.org\">MATS 9.0<\/a>&nbsp;applications are open, due <strong>Oct 2 2025<\/strong>, and <a href=\"http://tinyurl.com/neel-mats-app\">mine<\/a>&nbsp;close on <strong>Sept 12<\/strong>.<\/p><p>Other programs (which I think are generally lower quality than MATS, but often still worth applying to depending on the mentor)<\/p><ul><li><i>Full-time/In-person:<\/i><a href=\"https://www.matsprogram.org/\">&nbsp;MATS<\/a>, <a href=\"https://www.pivotal-research.org/fellowship\">Pivotal<\/a>, <a href=\"https://www.lasrlabs.org/\">LASR<\/a>, <a href=\"https://pibbss.ai/fellowship/\">PIBBSS<\/a><\/li><li><i>Part-time/Remote:<\/i><a href=\"https://www.cambridgeaisafety.org/mars\">&nbsp;<\/a><a href=\"https://sparai.org/\">SPAR<\/a>, <a href=\"https://www.cambridgeaisafety.org/mars\">MARS<\/a><\/li><\/ul><p data-internal-id=\"Cold_emails\">Cold emails<\/p><p>You can also take matters into your own hands and try to convince someone to be your mentor. Reaching out to people, ideally via a warm introduction, but even just via a cold email, can be highly effective. However, I get lots of cold emails and I think many are not very effective, so here's some advice:<\/p><ul><li><strong>Don't just email the most prominent people<\/strong>. A lot of people will just email the most prominent people in the field and ask for mentorship. This is a bad plan! These people are very busy and they also get lots of emails. I just reflexively respond to any email requesting mentorship with “please apply to my MATS cohort”.<ul><li>However, there are lots of less prominent people who can provide a bunch of useful mentorship. These people are much more likely to be excited to get a cold email, to have time to engage, potentially even the spare capacity to properly mentor a project.<\/li><li>I think that many people who've recently joined my team or people who worked on a great paper with me during MATS are able to add a lot of value to people new to the field. And I would recommend reaching out to them!<ul><li>For example, Josh Engels, a new starter on my team, said he would happily receive more cold emails (as of early Sept 2025).<\/li><li>As a general heuristic, email first authors of papers, not fancy last authors.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Start small<\/strong>: Don't email someone you've never interacted with before asking if they want to kind of officially mentor you on some project. That's a big commitment.<ul><li>It's much better to be like, I'd be interested in having a chat about your paper or my work building on your paper.<\/li><li>Or just asking if they're down to have a chat giving you feedback on some project ideas, etc.<\/li><li>And if this goes well, it may organically turn into a more long-term mentoring relationship!<\/li><\/ul><\/li><li><strong>Proof of work<\/strong>: Demonstrate that you are actually interested in this person specifically, not just spamming tons of people.<ul><li>Show that you've engaged with their work, say something intelligent about it, have some questions.<ul><li>In the era of LLMs, this is less of a costly signal that you've actually taken an interest in this person specifically than it used to be, admittedly<\/li><li>But linking to some research you did building on their work I think is still reasonably costly, and very flattering to people.<\/li><\/ul><\/li><\/ul><\/li><li><strong>Prioritize aggressively<\/strong>. Assume the reader will stop reading at any moment, so put your most critical and impressive information first.<\/li><li><strong>Explain who you are<\/strong>: If you're emailing someone who gets more emails than they have capacity to respond to, they're going to be prioritizing. A key input into this is just who you are, what have you done, have you done something interesting that shows promise, do you have relevant credentials, etc. I personally find it very helpful if people just say the most impressive things about them in the first sentence or two.<ul><li>To do this without seeming arrogant, you could try: \"I'm sure you must get many of these emails. So to help you prioritise, here's some key info about me\"<\/li><\/ul><\/li><li>Use <strong>bolding<\/strong>&nbsp;for key phrases to make your email easily skimmable.<\/li><li><strong>Be concise<\/strong>. One thing I would often appreciate is a short blurb summarizing your request with a link to a longer document for details if I'm interested.<\/li><li><strong>Quick requests<\/strong>: Generally, my flow when reading emails is that I will either immediately respond or never look at it again. I'm a lot more likely to immediately respond if I can do so quickly. If you do want to email a busy person, have a clear, concrete question up front that they might be able to help with.<\/li><\/ul><h3 data-internal-id=\"Community___collaborators\">Community &amp; collaborators<\/h3><p>Much easier than finding a mentor is finding collaborators, other people to work on the same project with, or just other people also trying to learn more about mech interp, who you can chat with and give each other feedback:<\/p><ul><li><strong>In-Person:<\/strong>&nbsp;Local AI Safety hubs (London, Bay Area, etc.), University groups, ML conferences (e.g., the<a href=\"http://mechinterpworkshop.com/\">&nbsp;NeurIPS Mech Interp workshop<\/a>&nbsp;I co-organize), EAG/EAGx conferences.<ul><li>If you’re a student, see if there’s a lab at your university that has some people interested in interpretability. There may be interested PhD students even if no professor works on it<\/li><\/ul><\/li><li><strong>Online<\/strong>: These are also good places to meet people! I recommend sharing work for feedback, or just asking about who’s interested in what you’re interested in, and trying to DM the people who engage/seem interested, and seeing what happens<ul><li><a href=\"https://www.neelnanda.io/osmi-slack-invite\">Open Source Mechanistic Interpretability Slack<\/a><\/li><li><a href=\"https://discord.gg/nHS4YxmfeM\">Eleuther Discord<\/a>&nbsp;(interpretability-general)<\/li><li><a href=\"https://discord.gg/ysVfhCfCKw\">Mech Interp Discord<\/a><\/li><\/ul><\/li><\/ul><p><strong>Staying up to date<\/strong>: Another common question is how to stay up to date with the field. Honestly, I think that people new to the field should not worry that much about this. Most new papers are irrelevant, including the ones that there is hype around. But it's good to stay a little bit in the loop. Note that the community has substantial parts both in academia and outside, which are often best kept up with in different ways.<\/p><ul><li>LessWrong and the AlignmentForum are a reasonable place to keep up to date with the less academic half<\/li><li>Twitter is a confusing, chaotic place that is an okay place to keep up with both. It's a bit unclear who the right people to follow.<ul><li><a href=\"http://x.com/ch402\">Chris Olah<\/a>&nbsp;doesn't tweet much, but it's high quality when he does.<\/li><li><a href=\"http://x.com/neelnanda5\">I will tweet<\/a>&nbsp;about all of my interpretability work and sometimes others.<\/li><\/ul><\/li><\/ul><h2 data-internal-id=\"Careers\">Careers<\/h2><h3 data-internal-id=\"Where_to_apply\">Where to apply<\/h3><ul><li>Anthropic’s interpretability team roles: <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020159008\">research scientist<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4020305008\">research engineer<\/a>, <a href=\"https://job-boards.greenhouse.io/anthropic/jobs/4009173008\">research manager<\/a><\/li><li><a href=\"https://openai.com/careers/research-engineer-scientist-interpretability\">OpenAI's interpretability team roles<\/a><\/li><li>My team at Google DeepMind will hopefully be <a href=\"https://deepmind.google/about/careers/#open-roles\">hiring in early 2026<\/a>! Watch this space<\/li><li><a href=\"https://transluce.org/\">Transluce<\/a>&nbsp;-- a nonprofit research lab<\/li><li><a href=\"https://www.goodfire.ai/\">Goodfire<\/a>&nbsp;-- a mech interp startup that are <a href=\"https://www.goodfire.ai/careers\">hiring a bunch<\/a>.<ul><li>They <a href=\"https://www.goodfire.ai/blog/announcing-our-50m-series-a\">recently raised a $50 million Series A<\/a>&nbsp;and as of the time of writing are trying to both have people focused on products, and people focused on more fundamental research<\/li><\/ul><\/li><li>The UK government's AI Security Institute's interpretability team (<a href=\"https://www.aisi.gov.uk/careers#open-roles\">not currently hiring<\/a>)<\/li><\/ul><p data-internal-id=\"Applying_for_grants\">Applying for grants<\/p><p>For people trying to get into mech interp via the safety community, there are some funders around open to giving career transition grants to people trying to upskill in a new field like mech interp. Probably the best place I know of is <a href=\"https://www.openphilanthropy.org/career-development-and-transition-funding/\">Open Philanthropy's Early Career Funding.<\/a><\/p><p data-internal-id=\"Explore_Other_AI_Safety_Areas\">Explore Other AI Safety Areas<\/p><p>Mech interp isn't the only game in town! There’s other important areas of safety like Evals, AI Control, and Scalable Oversight, the latter two in particular seem neglected compared to mech interp. The<a href=\"https://arxiv.org/pdf/2504.01849\">&nbsp;GDM AGI Safety Approach<\/a>&nbsp;gives an overview of different parts of the field. If you’re doing this for safety reasons, I’d check if there’s other, more neglected subfields, that also appeal to you!<\/p><h3 data-internal-id=\"What_do_hiring_managers_look_for\">What do hiring managers look for<\/h3><p>Leaving aside things that apply to basically all roles, like whether this person has a good personality fit (which often just means looking out for red flags), here’s my sense of what hiring managers in interpretability are often looking for.<\/p><p>A useful mental model is that from a hiring manager's perspective, they're making an uncertain bet with little information in a somewhat adversarial environment. Each applicant wants to present themselves as the perfect fit. This means managers need to rely on signals that are hard to fake. But it’s quite difficult to get that much info on a person before you actually go and work with them a bunch.<\/p><p>Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly. If your research track record is good enough, interviews may just act as a check for red flags and to verify that you can actually write code and run experiments well.<\/p><p>Key skills:<\/p><ul><li><strong>Research Skills:<\/strong>&nbsp;A track record of completing end-to-end projects is the best signal. Papers are a great way to show this.<ul><li><strong>Research taste<\/strong>: The ability to come up with great research ideas <i>and<\/i>&nbsp;drive them to completion is rare and very valuable.<\/li><li><strong>Experiment design<\/strong>: Can they design good experiments and make their research ideas concrete and convert them into actions?<\/li><\/ul><\/li><li><strong>Conceptual Understanding of Mech Interp:<\/strong>&nbsp;Do you get the key ideas and know the literature?<\/li><li><p data-internal-id=\"ftnt_ref36\"><strong>Productivity and Conscientiousness:<\/strong>&nbsp;This is a very hard one to interview for, but incredibly important. A public track record of doing interesting things is a good signal, as are strong references from trusted sources<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-noteref\" id=\"fnrefslnwemz4grq\"><sup><a href=\"#fnslnwemz4grq\">[36]<\/a><\/sup><\/span>.<\/p><\/li><li><strong>Engineering Skills:<\/strong>&nbsp;Can you work fluently in a Python notebook? Can you write experiment code fast and well? Can you get things done? Do you understand the standard gotchas?<\/li><li><strong>Deep engineering skill<\/strong>: Beyond hacking together experiments, can you navigate large, complex codebases, write maintainable code, design complex software projects, etc?<ul><li>This is much more important if doing research inside a larger lab or tech company than as an independent researcher or academic.<\/li><li>One of the most common reasons we don't hire seemingly promising researchers onto my team is because they lack sufficiently strong engineering skills.<\/li><li>Obviously, LLMs are substantially changing the game when it comes to engineering skills, but I think deep engineering skills will be much harder to automate than shallow ones, unfortunately.<\/li><li>Unfortunately, I don’t have great advice on how to gain these other than working in larger and more complex codebases and learning how to cope. Pair programming with more experienced programmers can be a great way to transfer tacit knowledge<\/li><\/ul><\/li><li><strong>Skepticism<\/strong>: Can you constructively engage with research and critically evaluate it? In particular, can you do this to your own research? Good researchers need to be able to do work that is true.<\/li><\/ul><h3 data-internal-id=\"Should_you_do_a_PhD_\">Should you do a PhD?<\/h3><p>I don't have a PhD (and think I would have had a far less successful career if I had tried to get one) so I'm somewhat biased. But it's a common question. Here are the strongest arguments I’ve heard in favour:<\/p><ul><li>You get extremely high <strong>autonomy<\/strong>. If you want to spend years going deep on a niche topic that no industry lab would fund, a PhD is one of the only ways to do it.<\/li><li>It's a great environment to cultivate the ability to <strong>set your own research agenda<\/strong>. This is a crucial and difficult skill that is harder to learn in industry, where agendas are often set from the top down (though this varies a lot between team).<\/li><\/ul><p>And here are the reasons I think it's often a bad idea:<\/p><ul><li>The opportunity cost is immense. You could spend 4-6 years gaining direct, relevant experience in an industry lab.<\/li><li>Academic incentives can be misaligned with doing impactful research, e.g. pressure to publish meaning you’re discouraged from admitting to the limitations of your work.<\/li><li>The quality of supervision varies wildly, and a bad supervisor can make your life miserable.<\/li><li>Quality of life: The pay is generally terrible, which may or may not matter to you, and you may only get places in a different city/country than you’d prefer.<\/li><\/ul><p>But with all those caveats in mind, it’s definitely the right option for some! My overall take:<\/p><ul><li>The key thing that matters is mentorship, being in an environment where you are working with a better researcher, and learning from them.<ul><li>PhDs are often a good way of getting this. But if you can gain this by another way, plausibly you should go to that instead. PhDs have a lot of downsides too.<\/li><\/ul><\/li><li>Generally, the variance between supervisors and between managers in industry will dominate the academia versus industry differences, and thus you should pay a lot of attention to who exactly would be managing you.<ul><li>For a PhD, try to speak to your potential supervisor’s students in a private setting. If they say pretty bad things, that's a good reason not to go for the supervisor.<\/li><li>A common mistake is optimising for the most prestigious and famous supervisor when you often want to go for the ones who will have the most time for you, which anti-correlates.<\/li><\/ul><\/li><li>A common mistake is people feeling they need to <i>finish<\/i>&nbsp;PhDs. But if you sincerely believe that the point of a PhD is to be a learning environment, then why would the formal end of the PhD be the optimal time to leave? It's all kind of arbitrary.<ul><li>IMO, at least every six months, you should seriously evaluate what other opportunities you have, try applying for some things and be emotionally willing leave if a better opportunity comes along (taking into account switching costs).<ul><li>Note that often you can just take a year's leave of absence and resume at will.<\/li><\/ul><\/li><\/ul><\/li><\/ul><p data-internal-id=\"Relevant_Academic_Labs\">Relevant Academic Labs<\/p><p>I’m a big fan of the work coming out of these two, they seem like great places to work:<\/p><ul><li>David Bau (Northeastern)<\/li><li>Martin Wattenberg &amp; Fernanda Viegas (Harvard)<\/li><\/ul><p>Other labs that seem like good places to do interpretability research (note that this is not trying to be a comprehensive list!):<\/p><ul><li>Yonatan Belinkov (Technion)<\/li><li>Jacob Andreas (MIT)<\/li><li>Jacob Steinhardt (Berkeley)<\/li><li>Ellie Pavlick (Brown)<\/li><li>Victor Veitch (UChicago)<\/li><li>Robert West (EPFL)<\/li><li>Roger Grosse (Toronto)<\/li><li>Mor Geva (Tel Aviv)<\/li><li>Sarah Wiegreffe (Maryland)<\/li><li>Aaron Mueller (Boston University)<\/li><\/ul><p><i>Thanks a lot to Arthur Conmy, Paul Bogdan, Bilal Chughtai, Julian Minder, Callum McDougall, Josh Engels, Clement Dumas, Bart Bussmann for valuable feedback<\/i><\/p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"nifk1wb1jum\" role=\"doc-endnote\" id=\"fnnifk1wb1jum\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"nifk1wb1jum\"><sup><strong><a href=\"#fnrefnifk1wb1jum\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Note that I mean a full working month here. So something like 200 working hours. If you're only able to do this part-time, it's fine to take longer. If you're really focused on it, or have a head-start, then move on faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"ue9pdw6v8rj\" role=\"doc-endnote\" id=\"fnue9pdw6v8rj\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"ue9pdw6v8rj\"><sup><strong><a href=\"#fnrefue9pdw6v8rj\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;If you want something even more approachable, one of my past MATS scholars recommends getting GPT-5 thinking to produce coding exercises (eg a Python script with empty functions, and good tests), for an easier way in.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"hh6mwdeo4zm\" role=\"doc-endnote\" id=\"fnhh6mwdeo4zm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"hh6mwdeo4zm\"><sup><strong><a href=\"#fnrefhh6mwdeo4zm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;It’s fine for this coding to need a bunch of LLM help and documentation/tutorial looking up, this isn’t a memory test. The key thing is being able to correctly explain the core of each technique to a friend/LLM.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"sxyjce3nii\" role=\"doc-endnote\" id=\"fnsxyjce3nii\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"sxyjce3nii\"><sup><strong><a href=\"#fnrefsxyjce3nii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Note: This curriculum aims to get you started on <i>independent research<\/i>. This is often good enough for academic labs, but the engineering bar for most industry labs is significantly higher, as you’ll need to work in a large complex codebase with hundreds of other researchers. But those skills take much longer to gain.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"5\" data-footnote-id=\"kte6u8splw\" role=\"doc-endnote\" id=\"fnkte6u8splw\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"kte6u8splw\"><sup><strong><a href=\"#fnrefkte6u8splw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;You want to exclude the first token of the prompt when collecting activations, it’s a weird attention sink and often has high norm/is anomalous in many ways<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"6\" data-footnote-id=\"2ob115pcmet\" role=\"doc-endnote\" id=\"fn2ob115pcmet\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"2ob115pcmet\"><sup><strong><a href=\"#fnref2ob115pcmet\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Gotcha: Remember to try a bunch of coefficients for the vector when adding it. This is a crucial hyper-parameter and steered model behaviour varies a lot depending on its value<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"7\" data-footnote-id=\"1b9r0ass7sd\" role=\"doc-endnote\" id=\"fn1b9r0ass7sd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1b9r0ass7sd\"><sup><strong><a href=\"#fnref1b9r0ass7sd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Mixture of expert models, where there are many parameters, and only a fraction light up for each token, are a pain for interpretability research. Larger models means you'll need to get more/larger GPUs which is expensive and unwieldy. Favor working with dense models where possible.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"8\" data-footnote-id=\"bzop9pji3nl\" role=\"doc-endnote\" id=\"fnbzop9pji3nl\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"bzop9pji3nl\"><sup><strong><a href=\"#fnrefbzop9pji3nl\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;You can download then upload the PDF to the model, or just select all and copy and paste from the PDF to the chat window. No need to correct the formatting issues, LLMs are great at ignoring weird formatting artifacts<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"9\" data-footnote-id=\"207k0k5nobb\" role=\"doc-endnote\" id=\"fn207k0k5nobb\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"207k0k5nobb\"><sup><strong><a href=\"#fnref207k0k5nobb\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;<a href=\"http://repo2txt.com\">repo2txt.com<\/a>&nbsp;is a useful tool for concatenating a Github repo into a single txt file<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"10\" data-footnote-id=\"979wnkvgpa4\" role=\"doc-endnote\" id=\"fn979wnkvgpa4\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"979wnkvgpa4\"><sup><strong><a href=\"#fnref979wnkvgpa4\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;If you would like other perspectives, check out <a href=\"https://arxiv.org/abs/2501.16496\">Open Problems in Mechanistic Interpretability<\/a>&nbsp;(broad lit review from many leading researchers, recent), or <a href=\"https://transformer-circuits.pub/2023/interpretability-dreams/index.html\">Interpretability Dreams<\/a>&nbsp;(from Anthropic, 2 years old)<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"11\" data-footnote-id=\"3zw26zes9dx\" role=\"doc-endnote\" id=\"fn3zw26zes9dx\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3zw26zes9dx\"><sup><strong><a href=\"#fnref3zw26zes9dx\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;And for reasons we’ll discuss later, now feel much more pessimistic about the ambitious reverse engineering direction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"12\" data-footnote-id=\"7cxhc64szn8\" role=\"doc-endnote\" id=\"fn7cxhc64szn8\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7cxhc64szn8\"><sup><strong><a href=\"#fnref7cxhc64szn8\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Even if you already have a research background in another field, mechanistic interpretability is sufficiently different that you should expect to need to relearn at least some of your instincts. This stage remains very relevant to you, though you can hopefully learn faster.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"13\" data-footnote-id=\"9wj0u0qz3q\" role=\"doc-endnote\" id=\"fn9wj0u0qz3q\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9wj0u0qz3q\"><sup><strong><a href=\"#fnref9wj0u0qz3q\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;The rest of this piece will be framed around approaching learning research like this and why I think it is a reasonable process. Obviously, there is not one true correct way to learn research! When I e.g. critique something as a “mistake”, interpret this as “I often see people do this and think it’s suboptimal for them”, not “there does not exist a way of learning research where this is a good idea<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"14\" data-footnote-id=\"xw1ra5pqnd\" role=\"doc-endnote\" id=\"fnxw1ra5pqnd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"xw1ra5pqnd\"><sup><strong><a href=\"#fnrefxw1ra5pqnd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;My term for associated knowledge, understanding, intuition, etc.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"15\" data-footnote-id=\"tq4gws0zq69\" role=\"doc-endnote\" id=\"fntq4gws0zq69\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tq4gws0zq69\"><sup><strong><a href=\"#fnreftq4gws0zq69\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Read <a href=\"https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft\">my thoughts on SAEs here<\/a>. There’s still useful work to be done, but it’s an oversubscribed area, and our bar should be higher. They are a useful tool, but not as promising as I once hoped.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"16\" data-footnote-id=\"cdmsagzbqkp\" role=\"doc-endnote\" id=\"fncdmsagzbqkp\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"cdmsagzbqkp\"><sup><strong><a href=\"#fnrefcdmsagzbqkp\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This was using a technique called synthetic document fine-tuning (and some other creativity on top), which basically lets you insert false beliefs into a model by generating a bunch of fictional documents where those beliefs are true and fine-tuning the model on them.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"17\" data-footnote-id=\"p0f0m03b55r\" role=\"doc-endnote\" id=\"fnp0f0m03b55r\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"p0f0m03b55r\"><sup><strong><a href=\"#fnrefp0f0m03b55r\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>We chose problems we’re excited to see worked on, while trying to avoid fad-like dynamics<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"18\" data-footnote-id=\"g12d8d1lqu\" role=\"doc-endnote\" id=\"fng12d8d1lqu\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"g12d8d1lqu\"><sup><strong><a href=\"#fnrefg12d8d1lqu\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;Latents refer to the hidden units of the SAE. These were originally termed “features”, but that term is also used to mean “the interpretable concept the latent refers to”, so I use a different term to minimise confusion.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"19\" data-footnote-id=\"0td6a2gxwht\" role=\"doc-endnote\" id=\"fn0td6a2gxwht\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"0td6a2gxwht\"><sup><strong><a href=\"#fnref0td6a2gxwht\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;One of my MATS scholars make a working GPT-5 model diffing agent in a day<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"20\" data-footnote-id=\"5bdglmkdzr\" role=\"doc-endnote\" id=\"fn5bdglmkdzr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"5bdglmkdzr\"><sup><strong><a href=\"#fnref5bdglmkdzr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This is the one line in the post <i>without <\/i>a “as of early Sept 2025” disclaimer, this feels pretty evergreen<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"21\" data-footnote-id=\"wuxdh4f7kh\" role=\"doc-endnote\" id=\"fnwuxdh4f7kh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wuxdh4f7kh\"><sup><strong><a href=\"#fnrefwuxdh4f7kh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note: \"think\" or \"chain of thought\" are terrible terms. It's far more useful to think of the chain of thought as a scratchpad that a model with very limited short-term memory can choose to use or ignore.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"22\" data-footnote-id=\"3qxoen8tddk\" role=\"doc-endnote\" id=\"fn3qxoen8tddk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3qxoen8tddk\"><sup><strong><a href=\"#fnref3qxoen8tddk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Reasoning models break a lot of standard interpretability techniques because now the computational graph goes through the discrete, non-differentiable, and random operation of sampling thousands of times. Most interpretability techniques focus on studying a single forward pass.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"23\" data-footnote-id=\"lm5ixkfuzk\" role=\"doc-endnote\" id=\"fnlm5ixkfuzk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"lm5ixkfuzk\"><sup><strong><a href=\"#fnreflm5ixkfuzk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Not just, e.g., ones you can publish on.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"24\" data-footnote-id=\"idab8074tka\" role=\"doc-endnote\" id=\"fnidab8074tka\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"idab8074tka\"><sup><strong><a href=\"#fnrefidab8074tka\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>I called this moving fast in the blog post, but I think that may have confused some people.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"25\" data-footnote-id=\"wpekmwudkpd\" role=\"doc-endnote\" id=\"fnwpekmwudkpd\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wpekmwudkpd\"><sup><strong><a href=\"#fnrefwpekmwudkpd\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Though often this is done well with just a good introduction<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"26\" data-footnote-id=\"1bau7vsh9tk\" role=\"doc-endnote\" id=\"fn1bau7vsh9tk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1bau7vsh9tk\"><sup><strong><a href=\"#fnref1bau7vsh9tk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And having a well-known researcher as co-author is not sufficient evidence to avoid this, alas. I’m sure at least one paper I’ve co-authored in the past year or two is substantially false<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"27\" data-footnote-id=\"adytzr5d7y\" role=\"doc-endnote\" id=\"fnadytzr5d7y\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"adytzr5d7y\"><sup><strong><a href=\"#fnrefadytzr5d7y\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>It's strongly in your interests for people to build on your work because that makes your original work look better, in addition to being just pretty cool to see people engage deeply with your stuff.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"28\" data-footnote-id=\"va7mhfkrhm\" role=\"doc-endnote\" id=\"fnva7mhfkrhm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"va7mhfkrhm\"><sup><strong><a href=\"#fnrefva7mhfkrhm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that deliberately reproducing work, or trying to demonstrate the past work is shoddy, is completely reasonable. You just need to not <i>accidentally<\/i>&nbsp;reinvent the wheel.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"29\" data-footnote-id=\"tt0owz8koks\" role=\"doc-endnote\" id=\"fntt0owz8koks\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"tt0owz8koks\"><sup><strong><a href=\"#fnreftt0owz8koks\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>This is generally a good thing to do regardless of whether you’re focused on research taste or not!<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"30\" data-footnote-id=\"e3252d8idmr\" role=\"doc-endnote\" id=\"fne3252d8idmr\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"e3252d8idmr\"><sup><strong><a href=\"#fnrefe3252d8idmr\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>And, nowadays, LLM knowledge too I guess?<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"31\" data-footnote-id=\"8354hd0flji\" role=\"doc-endnote\" id=\"fn8354hd0flji\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"8354hd0flji\"><sup><strong><a href=\"#fnref8354hd0flji\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you’ll need someone who’s written several Arxiv papers to endorse you. cs.LG is the typical category for ML papers.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"32\" data-footnote-id=\"9oppcf0ftrh\" role=\"doc-endnote\" id=\"fn9oppcf0ftrh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"9oppcf0ftrh\"><sup><strong><a href=\"#fnref9oppcf0ftrh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Note that you can submit something to a workshop <i>and <\/i>to a conference, so long as the workshop is “non-archival”<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"33\" data-footnote-id=\"fmh579omuc6\" role=\"doc-endnote\" id=\"fnfmh579omuc6\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"fmh579omuc6\"><sup><strong><a href=\"#fnreffmh579omuc6\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>A conference paper is a fair bit more effort, and you generally want to be working with someone who understands the academic conventions and shibboleths and the various hoops you should be jumping through. But I think this can be a nice thing to aim for, especially if you're starting out and need credentials, though mech interp cares less about peer review than most academic subfields.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"34\" data-footnote-id=\"f09vsa4w37e\" role=\"doc-endnote\" id=\"fnf09vsa4w37e\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"f09vsa4w37e\"><sup><strong><a href=\"#fnreff09vsa4w37e\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>See <a href=\"https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/\">this NeurIPS experiment<\/a>&nbsp;showing that half the spotlight papers would be rejected by an independent reviewing council<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"35\" data-footnote-id=\"7ruxx269r2s\" role=\"doc-endnote\" id=\"fn7ruxx269r2s\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7ruxx269r2s\"><sup><strong><a href=\"#fnref7ruxx269r2s\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp;This is one of the most valuable things I do for my MATS scholars, IMO.<\/p><\/div><\/li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"36\" data-footnote-id=\"slnwemz4grq\" role=\"doc-endnote\" id=\"fnslnwemz4grq\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"slnwemz4grq\"><sup><strong><a href=\"#fnrefslnwemz4grq\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\" data-footnote-content=\"\"><p>&nbsp; Unfortunately, standard reference culture, especially in the US, is to basically lie, and the amount of lying varies between contexts, rendering references mostly useless unless from a cultural context the hiring manager understands or ideally from people they know and trust. This is one of the reasons that doing AI safety mentoring programs like MATS can be extremely valuable, because often your mentor will know people who might then go on to hire you, which makes you a lower risk hire from their perspective.<\/p><\/div><\/li><\/ol>",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher"
            },
            "headline": "How To Become A Mechanistic Interpretability Researcher",
            "description": "Note: If you’ll forgive the shameless self-promotion, applications for my MATS stream are open until Sept 12. I help people write a mech interp paper…",
            "datePublished": "2025-09-02T23:38:43.780Z",
            "about": [
                {
                    "@type": "Thing",
                    "name": "Careers",
                    "url": "https://www.alignmentforum.org/w/careers",
                    "description": "<p>Posts relating to jobs, career development, etc.<\/p>"
                },
                {
                    "@type": "Thing",
                    "name": "Interpretability (ML & AI)",
                    "url": "https://www.alignmentforum.org/w/interpretability-ml-and-ai",
                    "description": "<p><strong>Interpretability<\/strong> is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.<\/p><p>Present-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.<\/p><p>A prominent subfield of interpretability of neural networks is mechanistic interpretability, which attempts to <a href=\"https://www.neelnanda.io/mechanistic-interpretability/quickstart\">understand<\/a> <i>how<\/i> neural networks perform the tasks they perform, for example by finding <a href=\"https://www.lesswrong.com/tag/transformer-circuits\">circuits in transformer models<\/a>. This can be contrasted to subfieds of interpretability which seek to attribute some output to a part of a specific input, such as clarifying which pixels in an input image <a href=\"https://christophm.github.io/interpretable-ml-book/pixel-attribution.html\">caused<\/a> a computer vision model to output the classification \"horse\".<\/p><h2>See Also<\/h2><ul><li><a href=\"https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\">Explainable Artificial Intelligence<\/a> on Wikipedia<\/li><li><a href=\"https://www.lesswrong.com/tag/transformer-circuits\">Transformer Circuits<\/a><\/li><li><a href=\"https://christophm.github.io/interpretable-ml-book/\">Interpretable Machine Learning<\/a>, textbook<\/li><\/ul><h3>Research<\/h3><ul><li><a href=\"https://distill.pub/2020/circuits/\">Circuits Thread<\/a><\/li><li><a href=\"https://transformer-circuits.pub/\">Transformer Circuits Thread<\/a><\/li><\/ul>"
                },
                {
                    "@type": "Thing",
                    "name": "Scholarship & Learning",
                    "url": "https://www.alignmentforum.org/w/scholarship-and-learning",
                    "description": "<p><strong>Scholarship &amp; Learning. <\/strong>Here be posts on how to study, research, and learn.<\/p><p>Topics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. – <\/i><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><i>Twelve Virtues of Rationality<\/i><\/a><\/p><\/blockquote><h2>See Also<\/h2><ul><li><a href=\"https://www.lesswrong.com/tag/spaced-repetition\">Spaced Repetition<\/a> is a technique for long-term retention of learned material.<\/li><li><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a> are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".<\/li><li>The other <a href=\"https://www.lesswrong.com/tag/virtues?showPostCount=true&amp;useTagName=true\">Virtues<\/a> of Rationality.<\/li><\/ul><h2>Top Resources<\/h2><ul><li><a href=\"https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently\">Scholarship: How to Do It Efficiently<\/a> is a guide to quickly researching topics and understanding what is known within a field.<\/li><li><a href=\"https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1\">Literature Review For Academic Outsiders: What, How, and Why<\/a> similar to the first resource, contains many links to further resources.<\/li><li><a href=\"https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific\">[Question] How do you assess the quality / reliability of a scientific study?<\/a> A question post with many highly excellent lengthy responses, several which received bounty payouts.<\/li><li><a href=\"https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things\">On learning difficult things<\/a> covers techniques and methods for studying difficult topics.<\/li><li><a href=\"https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears\">Paper-Reading for Gears<\/a> is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.<\/li><li><a href=\"https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll\">The 3 Books Technique for Learning a New Skilll<\/a> is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.<\/li><li><a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">The Best Textbooks on Every Subject<\/a> crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.<\/li><li><a href=\"https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy\">Forum participation as a research strategy<\/a> argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.<\/li><li><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\">Fact Posts: How and Why<\/a> is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.<\/li><li><a href=\"https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review\">How to (not) do a literature review<\/a> which contains a very concrete list of steps for literature reviews, including mistakes to avoid.<\/li><\/ul><p><strong>Ex<\/strong>... <\/p>"
                },
                {
                    "@type": "Thing",
                    "name": "Practical",
                    "url": "https://www.alignmentforum.org/w/practical",
                    "description": "<p><strong>Practical<\/strong> posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.<\/p><p>Material that is directly about <i>how to think better<\/i> can be found at <a href=\"https://www.lessestwrong.com/tag/rationality\">Rationality<\/a>.<\/p><p>&nbsp;<\/p><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Practical Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude<\/a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness<\/a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies<\/a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition<\/a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting<\/a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack<\/a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep<\/a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring<\/a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits<\/a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions<\/a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements<\/a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation<\/a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka<\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making<\/a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation<\/a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building<\/a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools<\/a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition<\/a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia<\/a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations<\/a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization<\/a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination<\/a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity<\/a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower<\/a><\/p><\/td><\/tr><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal<\/strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)<\/a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures<\/a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship<\/u><\/a><\/td><\/tr><\/tbody><\/table><\/figure>"
                },
                {
                    "@type": "Thing",
                    "name": "AI",
                    "url": "https://www.alignmentforum.org/w/ai",
                    "description": "<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory?showPostCount=true&amp;useTagName=true\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding?showPostCount=true&amp;useTagName=true\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk?showPostCount=true&amp;useTagName=true\">Eliciting Latent Knowledge<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/interpretability-ml-and-ai?showPostCount=true&amp;useTagName=true\">Interpretability (ML &amp; AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center?showPostCount=true&amp;useTagName=true\">Alignment Research Center<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropic?showPostCount=true&amp;useTagName=true\">Anthropic<\/a><br><a href=\"https://www.lesswrong.com/tag/apart-research?showPostCount=true&amp;useTagName=true\">Apart Research<\/a><br><a href=\"https://www.lesswrong.com/tag/axrp?showPostCount=true&amp;useTagName=true\">AXRP<\/a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true\">CHAI (UC Berkeley)<\/a><br><a href=\"https://www.lesswrong.com/tag/conjecture-org?showPostCount=true&amp;useTagName=true\">Conjecture (org)<\/a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind<\/a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true\">FHI (Oxf<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "
                }
            ],
            "author": [
                {
                    "@type": "Person",
                    "name": "Neel Nanda",
                    "url": "https://www.alignmentforum.org/users/neel-nanda-1"
                }
            ],
            "interactionStatistic": [
                {
                    "@type": "InteractionCounter",
                    "interactionType": {
                        "@type": "http://schema.org/CommentAction"
                    },
                    "userInteractionCount": 10
                },
                {
                    "@type": "InteractionCounter",
                    "interactionType": {
                        "@type": "http://schema.org/LikeAction"
                    },
                    "userInteractionCount": 75
                }
            ]
        }</script>

