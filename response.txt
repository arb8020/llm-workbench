Short version: you ran out of memory during the forward pass because GPU 1 was already packed almost to its limit (mostly with weights), and your run needed ~1.16 GiB more scratch/activation space for a Linear op. A few things amplified the peak: device_map="auto" placed too much on GPU 1, you save activations from all 48 layers in a single pass, and you reload the 30B model for every batch (which can cause allocator pressure/fragmentation and temporary spikes). The nvidia-smi snapshot shows ~58.6 GiB before the heavy forward allocations; at the OOM point PyTorch reports ~78.3 GiB in use on GPU 1—which is when the allocation failed.

Why it happens
- Model size vs GPU headroom:
  - Qwen3-30B in bf16 is roughly 60+ GiB of weights. With two 80 GiB GPUs, “auto” can still pack one GPU close to full if the split isn’t perfectly balanced. PyTorch’s OOM says GPU 1 had only ~0.84 GiB free when a 1.16 GiB allocation for a GEMM/activation buffer was needed.
- Peak forward memory > steady-state memory:
  - nvidia-smi shows ~58.6 GiB after load (steady state). During forward, extra temporary buffers, attention workspaces, and saved activations push the peak close to the 80 GiB limit. The OOM message reflects that peak, not the earlier snapshot.
- Saving many activations at once:
  - You “save()” two activations per layer for all 48 layers in a single trace. Those tensors stay live on GPU until the context exits, raising the peak.
- Recreating the model every batch:
  - Each batch calls extract_activations(), which reloads the model. Even if Python frees it at function exit, this pattern can cause spikes and allocator pressure; depending on hooks/GC timing, memory might not be reclaimed immediately and fragmentation gets worse.
- Possible KV-cache use:
  - If use_cache is left True, KV caches for 48 layers × 2048 tokens consume extra memory during forward.

What to change (in order of impact)
1) Load the model once, reuse it across batches.
   - Move model construction out of the for-loop and pass the instance in.
   - After each batch, clear transient state:
     - del tracer, del temporary tensors; torch.cuda.empty_cache() if needed.

2) Control device placement explicitly; don’t rely on device_map="auto".
   - Use a balanced/sequential map with caps so one GPU isn’t overfilled:
     - device_map="balanced"
     - max_memory={"cuda:0": "76GiB", "cuda:1": "76GiB"}
   - Verify the final map (print llm.model.hf_device_map) to ensure layers are evenly split.

3) Reduce peak activation footprint by chunking hooks.
   - Don’t save all layers in a single pass. Do in chunks, e.g., 6–8 layers per trace:
     - For each chunk: open trace, register .save() for those layers, run forward, immediately move saved activations to CPU, then release hooks and empty cache. Repeat for next chunk.
   - This keeps only a small subset of layer outputs resident at once.

4) Disable KV cache for analysis forwards.
   - Set llm.model.config.use_cache = False or pass use_cache=False in the trace/forward.
   - This removes a large per-layer, per-token memory component you don’t need for plain activation extraction.

5) Ensure low-precision compute everywhere it’s safe.
   - Load in torch_dtype=torch.bfloat16.
   - If your tracer saves activations in fp32 by default, downcast before transferring to CPU if acceptable (e.g., to float16/bf16) to lower on-GPU footprint.

6) Tweak allocator to reduce fragmentation spikes.
   - Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
   - Call torch.cuda.empty_cache() after each chunk/batch.

7) If still tight: reduce sequence length or layer count per run; or quantize weights (8-bit) for analysis (only if that’s acceptable for your use case).

Sketch of safer structure
- Build model once:
  - llm = LanguageModel(model_name, device_map="balanced", max_memory={"cuda:0": "76GiB", "cuda:1": "76GiB"}, torch_dtype=torch.bfloat16)
  - llm.model.config.use_cache = False
- Chunked activation capture:
  - for layers_chunk in chunk(layers, 8):
      with torch.inference_mode(), llm.trace(texts) as tracer:
          regs = {}
          for i in layers_chunk:
              regs[f"{i}_attn"] = llm.model.layers[i].input_layernorm.output.save()
              regs[f"{i}_mlp"]  = llm.model.layers[i].post_attention_layernorm.output.save()
      for k, proxy in regs.items():
          out = proxy.to(torch.bfloat16).cpu()
          save_to_disk(out)
      del regs; torch.cuda.empty_cache()

Sanity checks to run
- Print the device map (llm.model.hf_device_map) and confirm an even split.
- Print llm.model.config.use_cache; ensure it’s False.
- Track peak allocation with torch.cuda.max_memory_allocated() before/after a batch.
- Try a shorter seq_len (e.g., 1024) once; if OOM disappears, it confirms peak activation/caches were the trigger.

Bottom line: your run is right at the limit for a single 80 GiB GPU once the forward scratch/activations are included. Balance the device map, avoid reloading per batch, disable KV cache, and save activations in chunks to keep peak memory under control.
